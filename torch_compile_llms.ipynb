{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch._dynamo.config\n",
    "import torch._inductor.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.profiler._utils._init_for_cuda_graphs()\n",
    "prof = torch.profiler.profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee756c25a914c0b97e94a2387aebf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f1d769b6e640d4b0c443baa50ffd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fecf9eb69474b1b8729d9fa4e5576bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b5cd98f926459291d8c1344bcf0b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd7cd1dfb9440e0811aa4e1ca435546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e5a55ead7b4f5ebea64e0e9b7b57fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcdd28e572b4f71b3a81b2858bfb7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec1e685b6b64dad9ef4e74078ff762a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba848da9c7849178da1bd848493eb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   primes = []\n",
      "   for num in range(2, n+1):\n",
      "       is_prime = True\n",
      "       for i in range(2, int(math.sqrt(num))+1):\n",
      "           if num % i == 0:\n",
      "               is_prime = False\n",
      "               break\n",
      "       if is_prime:\n",
      "           primes.append(num)\n",
      "   print(primes)\n",
      "   \n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "[2, 3, 5, 7, 11, 13, 17, 19]\n",
      "```\n",
      "\n",
      "Exercise 5:\n",
      "Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "   \"\"\"\n",
      "   \n",
      "Time taken 113.24970636796206\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with prof:\n",
    "  outputs = model.generate(**inputs, max_length=200)\n",
    "  text = tokenizer.batch_decode(outputs)[0]\n",
    "  print(text)\n",
    "t = time.perf_counter() - t0\n",
    "print(f\"Time taken {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(f\"no_compile_prof.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/gaurav/anaconda3/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import GPT2LMHeadModel, \\\n",
    "                            pipeline, \\\n",
    "                            AutoTokenizer, \\\n",
    "                            AutoModel, \\\n",
    "                            LlamaForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "# in seconds. We use CUDA events and synchronization for the most accurate\n",
    "# measurements.\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "gen_model = 'meta-llama/Llama-3.2-1B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=gen_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generates random input and targets data for the model, where `b` is\n",
    "# batch size.\n",
    "def generate_data(b, max_length=16):\n",
    "    # Generate b random strings (or you can provide your own text data)\n",
    "    generator = pipeline('text-generation', model=gen_model, device='cuda')\n",
    "    sentences_generated = [item['generated_text']\n",
    "                    for item in generator(\"Generate something about\", max_length=25, num_return_sequences=b)]\n",
    "\n",
    "    # Tokenize the text\n",
    "    encodings = tokenizer(sentences_generated, return_tensors='pt', padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    # Move the tokenized input data to GPU\n",
    "    input_ids = encodings['input_ids'].to(torch.int64).cuda()  # token ids\n",
    "    attention_mask = encodings['attention_mask'].to(torch.int64).cuda()  # attention mask\n",
    "\n",
    "    # No labels required in this case, but you could return labels if needed\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Usage example\n",
    "batch_size = 4\n",
    "data = generate_data(batch_size)\n",
    "\n",
    "N_ITERS = 10\n",
    "\n",
    "def init_model():\n",
    "    model_name = \"distilgpt2\"  # This model is about 300MB\n",
    "    model = LlamaForCausalLM.from_pretrained(gen_model)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Generate something about your life, and see what your friends and family have to say. Get started now.\\nWhy would you want to do this?\\nDo you have a life story you want to share?\\nDo you want to share something with your friends and family?\\nDo you want to tell the world about your life?\\nWhat would you like to share?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you like to share with your friends and family?\\nWhat would you like to share with the world?\\nWhat would you']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_one_sample = generate_data(1)\n",
    "just_one_output = model.generate(input_ids=just_one_sample[0], attention_mask=just_one_sample[1], max_new_tokens=256)\n",
    "tokenizer.batch_decode(just_one_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 16, 128256])\n",
      "\n",
      "Generated sequences:\n",
      "Sequence 1: Question a new the world  days in a book.\n",
      " The the number of pages\n",
      "Sequence 2: Question a new the world, you. It world is full of things that are\n",
      "Sequence 3: Question a new the world of were in the office. It can want to use\n",
      "Sequence 4: Question a new the person3D object\n",
      " rotating a object around a Z-axis\n"
     ]
    }
   ],
   "source": [
    "# Generate a batch of 4 inputs\n",
    "inp = generate_data(4)\n",
    "\n",
    "# Run inference on the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=inp[0], attention_mask=inp[1])\n",
    "\n",
    "# Print the shape of the output\n",
    "print(f\"Output shape: {output.logits.shape}\")\n",
    "\n",
    "# Optionally, you can decode the output to get the generated text\n",
    "generated_sequences = torch.argmax(output.logits, dim=-1)\n",
    "decoded_output = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated sequences:\")\n",
    "for i, sequence in enumerate(decoded_output):\n",
    "    print(f\"Sequence {i + 1}: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager: 0.023097984313964842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/anaconda3/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 9.2183408203125\n"
     ]
    }
   ],
   "source": [
    "# Reset since we are using a different mode.\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Compile the model\n",
    "model_opt = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "# Generate a batch of 4 inputs\n",
    "inp = generate_data(4)\n",
    "\n",
    "# Run inference on the compiled model\n",
    "with torch.no_grad():\n",
    "    print(\"eager:\", timed(lambda: model(input_ids=inp[0], attention_mask=inp[1]))[1])\n",
    "    print(\"compile:\", timed(lambda: model_opt(input_ids=inp[0], attention_mask=inp[1]))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's run a few more iterations and see how the compile time is higher in the first one or two iterations and it comes down significiantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 0: 0.05784713745117188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 1: 0.054974433898925784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 2: 0.054830974578857425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 3: 0.05603891372680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 4: 0.05689836883544922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 5: 0.055298046112060545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 6: 0.05606326293945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 7: 0.05496435165405274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 8: 0.05618441772460937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager eval time 9: 0.05660406494140625\n",
      "~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 0: 10.87389453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 1: 6.8763974609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 2: 0.05574220657348633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 3: 0.05427142333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 4: 0.054163360595703126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 5: 0.054274337768554684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 6: 0.056618305206298826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 7: 0.055883071899414063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 8: 0.05592272186279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile eval time 9: 0.055807361602783205\n",
      "~~~~~~~~~~\n",
      "(eval) eager median: 0.056051088333129884, compile median: 0.05584521675109863, speedup: 1.003686467597553x\n",
      "~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "eager_times = []\n",
    "batch_size = 16\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(batch_size)\n",
    "    with torch.no_grad():\n",
    "        _, eager_time = timed(lambda: model(input_ids=inp[0], attention_mask=inp[1]))\n",
    "    eager_times.append(eager_time)\n",
    "    print(f\"eager eval time {i}: {eager_time}\")\n",
    "\n",
    "print(\"~\" * 10)\n",
    "\n",
    "compile_times = []\n",
    "for i in range(N_ITERS):\n",
    "    inp = generate_data(batch_size)\n",
    "    with torch.no_grad():\n",
    "        _, compile_time = timed(lambda: model_opt(input_ids=inp[0], attention_mask=inp[1]))\n",
    "    compile_times.append(compile_time)\n",
    "    print(f\"compile eval time {i}: {compile_time}\")\n",
    "print(\"~\" * 10)\n",
    "\n",
    "import numpy as np\n",
    "eager_med = np.median(eager_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = eager_med / compile_med\n",
    "assert(speedup > 1)\n",
    "print(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\n",
    "print(\"~\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 'max-autotune' compile mode\n",
    "\n",
    "We now have the GUARDS also enabled\n",
    "\n",
    "Note: Unfortunately, even my 4060Ti isn't good enough for AutoTune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "V1024 00:17:44.312000 125150921582400 torch/_dynamo/guards.py:2169] [0/0_1] [__guards] GUARDS:\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] \n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] +- RootGuardManager\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125146543924752)                   # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/llama/modeling_llama.py:1182 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/llama/modeling_llama.py:1182 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | +- GuardManager: source=L['self'].model, accessed_by=DictGetItemGuardAccessor(model)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].model, 125138873944912)             # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | | +- GuardManager: source=L['self'].model.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | | | +- GuardManager: source=L['self'].model.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].model.training, 8905664)            # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 150565744)                 # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/llama/modeling_llama.py:1182 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['input_ids'], accessed_by=DictGetItemGuardAccessor(input_ids)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['input_ids'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[4, 16], stride=[16, 1])  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- NO_HASATTR: hasattr(L['input_ids'], '_dynamo_dynamic_indices') == False   # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['attention_mask'])\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['use_cache'], accessed_by=DictGetItemGuardAccessor(use_cache)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['use_cache'], 8820832)                      # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8820832)                    # return_dict = return_dict if return_dict is not None else self.config.use_return_dict  # transformers/models/llama/modeling_llama.py:1186 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['position_ids'], accessed_by=DictGetItemGuardAccessor(position_ids)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['position_ids'], 8820832)                   # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['inputs_embeds'], accessed_by=DictGetItemGuardAccessor(inputs_embeds)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['inputs_embeds'], 8820832)                  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['attention_mask'], accessed_by=DictGetItemGuardAccessor(attention_mask)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['attention_mask'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[4, 16], stride=[16, 1])  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- NO_HASATTR: hasattr(L['attention_mask'], '_dynamo_dynamic_indices') == False  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['attention_mask'])\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['cache_position'], accessed_by=DictGetItemGuardAccessor(cache_position)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['cache_position'], 8820832)                 # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['past_key_values'], accessed_by=DictGetItemGuardAccessor(past_key_values)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['past_key_values'], 8820832)                # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 8820832)              # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/llama/modeling_llama.py:1182 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | +- GuardManager: source=L['output_hidden_states'], accessed_by=DictGetItemGuardAccessor(output_hidden_states)\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_hidden_states'], 8820832)           # output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states  # transformers/models/llama/modeling_llama.py:1184 in forward\n",
      "V1024 00:17:44.313000 125150921582400 torch/_dynamo/guards.py:2148] [0/0_1] [__guards] \n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2169] [1/0_1] [__guards] GUARDS:\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] \n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] +- RootGuardManager\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125138873944912)                   # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/llama/modeling_llama.py:935 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/llama/modeling_llama.py:935 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 150565744)                 # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/llama/modeling_llama.py:935 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- GuardManager: source=L['self'].embed_tokens, accessed_by=DictGetItemGuardAccessor(embed_tokens)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].embed_tokens, 125143441818512)      # inputs_embeds = self.embed_tokens(input_ids)  # transformers/models/llama/modeling_llama.py:950 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | | +- GuardManager: source=L['self'].embed_tokens.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | | | +- GuardManager: source=L['self'].embed_tokens.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].embed_tokens.training, 8905664)     # inputs_embeds = self.embed_tokens(input_ids)  # transformers/models/llama/modeling_llama.py:950 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=L['self'].gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].gradient_checkpointing, 8905664)    # if self.gradient_checkpointing and self.training and use_cache:  # transformers/models/llama/modeling_llama.py:943 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['input_ids'], accessed_by=DictGetItemGuardAccessor(input_ids)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['input_ids'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[4, 16], stride=[16, 1])  # if (input_ids is None) ^ (inputs_embeds is not None):  # transformers/models/llama/modeling_llama.py:938 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- NO_HASATTR: hasattr(L['input_ids'], '_dynamo_dynamic_indices') == False   # if (input_ids is None) ^ (inputs_embeds is not None):  # transformers/models/llama/modeling_llama.py:938 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['use_cache'], accessed_by=DictGetItemGuardAccessor(use_cache)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['use_cache'], 8820832)                      # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/llama/modeling_llama.py:935 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8906112)                    # return_dict = return_dict if return_dict is not None else self.config.use_return_dict  # transformers/models/llama/modeling_llama.py:936 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['inputs_embeds'], accessed_by=DictGetItemGuardAccessor(inputs_embeds)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['inputs_embeds'], 8820832)                  # if (input_ids is None) ^ (inputs_embeds is not None):  # transformers/models/llama/modeling_llama.py:938 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['past_key_values'], accessed_by=DictGetItemGuardAccessor(past_key_values)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['past_key_values'], 8820832)                # use_cache and not isinstance(past_key_values, Cache) and not self.training  # transformers/models/llama/modeling_llama.py:954 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 8905664)              # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/llama/modeling_llama.py:931 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=L['output_hidden_states'], accessed_by=DictGetItemGuardAccessor(output_hidden_states)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_hidden_states'], 8905664)           # output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states  # transformers/models/llama/modeling_llama.py:933 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=G['Cache'], accessed_by=DictGetItemGuardAccessor(Cache)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['Cache'], 151528112)                        # use_cache and not isinstance(past_key_values, Cache) and not self.training  # transformers/models/llama/modeling_llama.py:954 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=G['logger'], accessed_by=DictGetItemGuardAccessor(logger)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['logger'], 125138876517712)                 # logger.warning_once(  # transformers/models/llama/modeling_llama.py:958 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=G['DynamicCache'], accessed_by=DictGetItemGuardAccessor(DynamicCache)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['DynamicCache'], 151626256)                 # past_key_values = DynamicCache.from_legacy_cache(past_key_values)  # transformers/models/llama/modeling_llama.py:957 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=G['__builtins_dict___48'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___48)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___48']['super'], accessed_by=DictGetItemGuardAccessor(super)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___48']['super'], 8802688)  # super().__init__()  # transformers/cache_utils.py:321 in __init__\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___48']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___48']['isinstance'], 125150921101920)  # use_cache and not isinstance(past_key_values, Cache) and not self.training  # transformers/models/llama/modeling_llama.py:954 in forward\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_cache_utils'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_cache_utils)\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_cache_utils'], 125150807801456)  # super().__init__()  # transformers/cache_utils.py:321 in __init__\n",
      "V1024 00:17:44.372000 125150921582400 torch/_dynamo/guards.py:2148] [1/0_1] [__guards] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager: 0.023377119064331056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2169] [2/0] [__guards] GUARDS:\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] \n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] +- RootGuardManager\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125138873944912)                   # causal_mask = self._update_causal_mask(  # transformers/models/llama/modeling_llama.py:971 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # causal_mask = self._update_causal_mask(  # transformers/models/llama/modeling_llama.py:971 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=L['self'].norm, accessed_by=DictGetItemGuardAccessor(norm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].norm, 125138872117776)              # hidden_states = self.norm(hidden_states)  # transformers/models/llama/modeling_llama.py:1020 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].norm.__dict__)      # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].norm.training, 8905664)             # hidden_states = self.norm(hidden_states)  # transformers/models/llama/modeling_llama.py:1020 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].norm.weight, 125138872717904)       # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- EQUALS_MATCH: L['self'].norm.variance_epsilon == 1e-05                      # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=L['self'].layers, accessed_by=DictGetItemGuardAccessor(layers)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers, 125138872115536)            # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers, 106463328)                 # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- LENGTH_CHECK: len(L['self'].layers) == 16                                   # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers.training, 8905664)           # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0], 125146543881424)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp, 125145419122896)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[0].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.act_fn, 125138874808784)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.up_proj, 125138874806480)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.down_proj, 125138874808592)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.gate_proj, 125146848745872)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[0].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn, 125138875754704)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[0].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.k_proj, 125138874806608)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.o_proj, 125145552029264)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.q_proj, 125138874807312)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.v_proj, 125138874799120)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].self_attn.layer_idx == 0                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].input_layernorm, 125138874807440)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[0].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].input_layernorm.weight, 125138872350000)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].post_attention_layernorm, 125138874521488)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[0].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[0].post_attention_layernorm.weight, 125138872350384)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[0].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[0].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1], 125138874529552)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp, 125138871774416)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[1].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.act_fn, 125138871774480)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.up_proj, 125138871776912)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.down_proj, 125138871773392)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.gate_proj, 125138871773328)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[1].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn, 125145419233680)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[1].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.k_proj, 125138871778768)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.o_proj, 125138871773584)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.q_proj, 125145419234192)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.v_proj, 125138871777040)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].self_attn.layer_idx == 1                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].input_layernorm, 125138871777808)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[1].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].input_layernorm.weight, 125138872352112)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].post_attention_layernorm, 125138871776656)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[1].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[1].post_attention_layernorm.weight, 125138872352496)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[1].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[1].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[2], accessed_by=GetItemGuardAccessor(2)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2], 125138871778064)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[2].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[2].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp, 125138871776464)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[2].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.act_fn, 125138871777360)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.up_proj, 125138871776080)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.down_proj, 125138871779088)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.gate_proj, 125138871773776)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[2].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn, 125138871773904)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[2].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.k_proj, 125138871773968)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.o_proj, 125138871777936)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.q_proj, 125138871777232)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.v_proj, 125138871776336)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].self_attn.layer_idx == 2                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].input_layernorm, 125138871776016)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[2].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].input_layernorm.weight, 125138872351440)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].post_attention_layernorm, 125138871777744)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[2].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[2].post_attention_layernorm.weight, 125138872351056)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[2].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[2].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[2]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[3], accessed_by=GetItemGuardAccessor(3)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3], 125138871775312)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[3].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[3].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp, 125138872089232)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[3].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.act_fn, 125138872076304)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.up_proj, 125138872075216)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.down_proj, 125138872090064)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.gate_proj, 125138872075152)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[3].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn, 125138871775184)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[3].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.k_proj, 125138873800720)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.o_proj, 125138873801040)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.q_proj, 125145756859216)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.v_proj, 125138873799696)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].self_attn.layer_idx == 3                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].input_layernorm, 125138872078800)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[3].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].input_layernorm.weight, 125138872349328)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].post_attention_layernorm, 125138872088016)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[3].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[3].post_attention_layernorm.weight, 125138872348752)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[3].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[3].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[3]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[4], accessed_by=GetItemGuardAccessor(4)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4], 125138872080464)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[4].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[4].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp, 125138871921616)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[4].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.act_fn, 125138871923664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.up_proj, 125138871921232)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.down_proj, 125138871924048)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.gate_proj, 125138871919760)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[4].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn, 125138872078544)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[4].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.k_proj, 125138872083024)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.o_proj, 125138872075472)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.q_proj, 125138872083920)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.v_proj, 125138872090448)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].self_attn.layer_idx == 4                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].input_layernorm, 125138871920464)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[4].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].input_layernorm.weight, 125138872347216)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].post_attention_layernorm, 125138871923600)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[4].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[4].post_attention_layernorm.weight, 125138872346640)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[4].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[4].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[4]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[5], accessed_by=GetItemGuardAccessor(5)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5], 125138871920208)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[5].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[5].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp, 125138871910928)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[5].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.act_fn, 125138871915536)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.up_proj, 125138871911760)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.down_proj, 125138871912144)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.gate_proj, 125138871912400)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[5].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn, 125138871924112)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[5].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.k_proj, 125138871911568)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.o_proj, 125138871912784)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.q_proj, 125138871922960)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.v_proj, 125138871911376)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].self_attn.layer_idx == 5                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].input_layernorm, 125138871912976)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[5].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].input_layernorm.weight, 125138872350864)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].post_attention_layernorm, 125138871913872)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[5].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[5].post_attention_layernorm.weight, 125138872348944)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[5].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[5].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[5]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[6], accessed_by=GetItemGuardAccessor(6)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6], 125138871913040)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[6].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[6].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp, 125138871914960)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[6].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.act_fn, 125138871915664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.up_proj, 125138871915216)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.down_proj, 125138871913808)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.gate_proj, 125138871914640)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[6].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn, 125138871914128)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[6].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.k_proj, 125138871913296)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.o_proj, 125138871915280)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.q_proj, 125138871913104)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.v_proj, 125138871913616)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].self_attn.layer_idx == 6                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].input_layernorm, 125138871916560)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[6].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].input_layernorm.weight, 125138872728656)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].post_attention_layernorm, 125138871915600)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[6].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[6].post_attention_layernorm.weight, 125138872728272)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[6].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[6].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[6]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[7], accessed_by=GetItemGuardAccessor(7)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7], 125138871915856)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[7].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[7].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp, 125138871917712)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[7].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.act_fn, 125138871918288)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.up_proj, 125138871917776)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.down_proj, 125138871918032)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.gate_proj, 125138871918352)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[7].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn, 125138871916368)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[7].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.k_proj, 125138871916048)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.o_proj, 125138871917008)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.q_proj, 125138871916816)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.v_proj, 125138871916432)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].self_attn.layer_idx == 7                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].input_layernorm, 125138871918096)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[7].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].input_layernorm.weight, 125138872726544)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].post_attention_layernorm, 125138871919120)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[7].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[7].post_attention_layernorm.weight, 125138872725968)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[7].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[7].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[7]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[8], accessed_by=GetItemGuardAccessor(8)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8], 125138871918544)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[8].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[8].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp, 125138871925776)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[8].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.act_fn, 125138871923536)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.up_proj, 125138871920016)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.down_proj, 125138871910544)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.gate_proj, 125138871921744)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[8].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn, 125138871918608)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[8].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.k_proj, 125138871918864)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.o_proj, 125138871919632)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.q_proj, 125138871918928)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.v_proj, 125138871919504)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].self_attn.layer_idx == 8                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].input_layernorm, 125138871920336)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[8].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].input_layernorm.weight, 125138872723952)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].post_attention_layernorm, 125138871925584)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[8].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[8].post_attention_layernorm.weight, 125138872723760)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[8].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[8].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[8]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[9], accessed_by=GetItemGuardAccessor(9)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9], 125138871920656)         # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[9].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[9].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].training, 8905664)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp, 125138870810064)     # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[9].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.training, 8905664)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.act_fn, 125138870809488)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.up_proj, 125138870809872)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.down_proj, 125138870809616)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.gate_proj, 125138870809936)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[9].mlp.config, 150565744)   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn, 125138870811408)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[9].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.k_proj, 125138870797136)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.o_proj, 125138870810960)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.q_proj, 125138870811024)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.v_proj, 125138870811344)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].self_attn.head_dim == 64                  # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].self_attn.layer_idx == 9                  # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].self_attn.num_heads == 32                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].self_attn.num_key_value_heads == 8        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].self_attn.num_key_value_groups == 4       # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].input_layernorm, 125138870809424)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[9].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].input_layernorm.weight, 125138872723280)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].post_attention_layernorm, 125138870809360)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[9].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[9].post_attention_layernorm.weight, 125138872722992)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[9].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[9].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[9]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[10], accessed_by=GetItemGuardAccessor(10)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10], 125138870809232)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[10].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[10].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp, 125138870807376)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[10].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.act_fn, 125138870806800)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.up_proj, 125138870807184)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.down_proj, 125138870806928)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.gate_proj, 125138870807248)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[10].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn, 125138870809168)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[10].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.k_proj, 125138870808656)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.o_proj, 125138870808080)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.q_proj, 125138870808912)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.v_proj, 125138870808272)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].self_attn.layer_idx == 10                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].input_layernorm, 125138870806736)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[10].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].input_layernorm.weight, 125138872714544)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].post_attention_layernorm, 125138870806672)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[10].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[10].post_attention_layernorm.weight, 125138872714928)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[10].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[10].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[10]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[11], accessed_by=GetItemGuardAccessor(11)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11], 125138870806544)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[11].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[11].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp, 125138870804688)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[11].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.act_fn, 125138870804112)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.up_proj, 125138870804496)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.down_proj, 125138870804240)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.gate_proj, 125138870804560)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[11].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn, 125138870806480)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[11].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.k_proj, 125138870805968)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.o_proj, 125138870805392)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.q_proj, 125138870806224)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.v_proj, 125138870805584)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].self_attn.layer_idx == 11                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].input_layernorm, 125138870804048)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[11].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].input_layernorm.weight, 125138872717040)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].post_attention_layernorm, 125138870803984)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[11].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[11].post_attention_layernorm.weight, 125138872717424)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[11].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[11].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[11]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[12], accessed_by=GetItemGuardAccessor(12)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12], 125138870803856)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[12].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[12].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp, 125138870802000)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[12].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.act_fn, 125138870801424)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.up_proj, 125138870801808)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.down_proj, 125138870801552)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.gate_proj, 125138870801872)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[12].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn, 125138870803792)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[12].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.k_proj, 125138870803280)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.o_proj, 125138870802704)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.q_proj, 125138870803536)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.v_proj, 125138870802896)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].self_attn.layer_idx == 12                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].input_layernorm, 125138870801360)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[12].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].input_layernorm.weight, 125138872719536)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].post_attention_layernorm, 125138870801296)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[12].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[12].post_attention_layernorm.weight, 125138872720016)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[12].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[12].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[12]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[13], accessed_by=GetItemGuardAccessor(13)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13], 125138870801168)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[13].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[13].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp, 125138870799312)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[13].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.act_fn, 125138870798736)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.up_proj, 125138870799120)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.down_proj, 125138870798864)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.gate_proj, 125138870799184)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[13].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn, 125138870801104)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[13].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.k_proj, 125138870800592)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.o_proj, 125138870800016)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.q_proj, 125138870800848)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.v_proj, 125138870800208)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].self_attn.layer_idx == 13                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].input_layernorm, 125138870798672)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[13].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].input_layernorm.weight, 125138872722032)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].post_attention_layernorm, 125138870798608)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[13].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[13].post_attention_layernorm.weight, 125138872722608)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[13].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[13].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[13]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[14], accessed_by=GetItemGuardAccessor(14)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14], 125138870798480)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[14].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[14].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp, 125138870796560)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[14].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.act_fn, 125138872113616)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.up_proj, 125138870797328)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.down_proj, 125138872115984)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.gate_proj, 125138870796368)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[14].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn, 125138870798416)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[14].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.k_proj, 125138870797904)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.o_proj, 125138870796432)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.q_proj, 125138870798160)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.v_proj, 125138870797520)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].self_attn.layer_idx == 14                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].input_layernorm, 125138872113104)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[14].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].input_layernorm.weight, 125138872721360)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].post_attention_layernorm, 125138872116432)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[14].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[14].post_attention_layernorm.weight, 125138872720784)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[14].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[14].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[14]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].layers[15], accessed_by=GetItemGuardAccessor(15)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15], 125138872114640)        # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layers[15].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[15].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].training, 8905664)       # for decoder_layer in self.layers:  # transformers/models/llama/modeling_llama.py:984 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp, accessed_by=DictGetItemGuardAccessor(mlp)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp, 125138872110800)    # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[15].mlp.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.training, 8905664)   # hidden_states = self.mlp(hidden_states)  # transformers/models/llama/modeling_llama.py:750 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.act_fn, accessed_by=DictGetItemGuardAccessor(act_fn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.act_fn, 125138872115728)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.act_fn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.act_fn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.act_fn.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.up_proj, accessed_by=DictGetItemGuardAccessor(up_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.up_proj, 125138872113360)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.up_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.up_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.up_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.down_proj, accessed_by=DictGetItemGuardAccessor(down_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.down_proj, 125138872111696)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.down_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.down_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.down_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.gate_proj, accessed_by=DictGetItemGuardAccessor(gate_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.gate_proj, 125138872110160)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.gate_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.gate_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].mlp.gate_proj.training, 8905664)  # down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))  # transformers/models/llama/modeling_llama.py:309 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].layers[15].mlp.config, 150565744)  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:292 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].mlp._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn, accessed_by=DictGetItemGuardAccessor(self_attn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn, 125138872111440)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[15].self_attn.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.training, 8905664)  # hidden_states, self_attn_weights, present_key_value = self.self_attn(  # transformers/models/llama/modeling_llama.py:734 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.k_proj, accessed_by=DictGetItemGuardAccessor(k_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.k_proj, 125138872107856)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.k_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.k_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.k_proj.training, 8905664)  # key_states = self.k_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:618 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.o_proj, accessed_by=DictGetItemGuardAccessor(o_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.o_proj, 125138872109392)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.o_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.o_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.o_proj.training, 8905664)  # attn_output = self.o_proj(attn_output)  # transformers/models/llama/modeling_llama.py:672 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.q_proj, accessed_by=DictGetItemGuardAccessor(q_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.q_proj, 125138872107536)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.q_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.q_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.q_proj.training, 8905664)  # query_states = self.q_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:617 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.v_proj, accessed_by=DictGetItemGuardAccessor(v_proj)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.v_proj, 125138872108496)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.v_proj.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.v_proj.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].self_attn.v_proj.training, 8905664)  # value_states = self.v_proj(hidden_states)  # transformers/models/llama/modeling_llama.py:619 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.head_dim, accessed_by=DictGetItemGuardAccessor(head_dim)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].self_attn.head_dim == 64                 # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.layer_idx, accessed_by=DictGetItemGuardAccessor(layer_idx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].self_attn.layer_idx == 15                # key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # transformers/models/llama/modeling_llama.py:640 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.num_heads, accessed_by=DictGetItemGuardAccessor(num_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].self_attn.num_heads == 32                # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:621 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.num_key_value_heads, accessed_by=DictGetItemGuardAccessor(num_key_value_heads)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].self_attn.num_key_value_heads == 8       # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # transformers/models/llama/modeling_llama.py:622 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].self_attn.num_key_value_groups, accessed_by=DictGetItemGuardAccessor(num_key_value_groups)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].self_attn.num_key_value_groups == 4      # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm, accessed_by=DictGetItemGuardAccessor(input_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].input_layernorm, 125138872116496)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[15].input_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].input_layernorm.training, 8905664)  # hidden_states = self.input_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:731 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].input_layernorm.weight, 125138872718768)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].input_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].input_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm, accessed_by=DictGetItemGuardAccessor(post_attention_layernorm)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].post_attention_layernorm, 125138872116112)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].layers[15].post_attention_layernorm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].post_attention_layernorm.training, 8905664)  # hidden_states = self.post_attention_layernorm(hidden_states)  # transformers/models/llama/modeling_llama.py:749 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layers[15].post_attention_layernorm.weight, 125138872718192)  # return self.weight * hidden_states.to(input_dtype)  # transformers/models/llama/modeling_llama.py:125 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | | +- EQUALS_MATCH: L['self'].layers[15].post_attention_layernorm.variance_epsilon == 1e-05  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].layers[15].post_attention_layernorm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].layers[15]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=L['self'].rotary_emb, accessed_by=DictGetItemGuardAccessor(rotary_emb)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].rotary_emb, 125138872118096)        # position_embeddings = self.rotary_emb(hidden_states, position_ids)  # transformers/models/llama/modeling_llama.py:977 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].rotary_emb.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].rotary_emb.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].rotary_emb.training, 8905664)       # position_embeddings = self.rotary_emb(hidden_states, position_ids)  # transformers/models/llama/modeling_llama.py:977 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb._buffers, accessed_by=DictGetItemGuardAccessor(_buffers)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].rotary_emb.inv_freq, accessed_by=DictGetItemGuardAccessor(inv_freq)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].rotary_emb.inv_freq, 125138871827536)  # inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)  # transformers/models/llama/modeling_llama.py:203 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb.rope_type, accessed_by=DictGetItemGuardAccessor(rope_type)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- EQUALS_MATCH: L['self'].rotary_emb.rope_type == 'llama3'                    # if \"dynamic\" in self.rope_type:  # transformers/models/llama/modeling_llama.py:199 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb.attention_scaling, accessed_by=DictGetItemGuardAccessor(attention_scaling)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- EQUALS_MATCH: L['self'].rotary_emb.attention_scaling == 1.0                 # cos = cos * self.attention_scaling  # transformers/models/llama/modeling_llama.py:215 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=L['self'].rotary_emb.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__, accessed_by=GetAttrGuardAccessor(__closure__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__[0], accessed_by=TupleGetItemGuardAccessor(0)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__[0].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].rotary_emb.forward.__closure__[0].cell_contents, 8896576)  # return forward_call(*args, **kwargs)  # nn/modules/module.py:1562 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__[1], accessed_by=TupleGetItemGuardAccessor(1)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__[1].cell_contents, accessed_by=GetAttrGuardAccessor(cell_contents)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].rotary_emb.forward.__closure__[1].cell_contents.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].rotary_emb.forward.__closure__[1].cell_contents.__code__, 545317024)  # return forward_call(*args, **kwargs)  # nn/modules/module.py:1562 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 150565744)                 # if self.config._attn_implementation == \"flash_attention_2\":  # transformers/models/llama/modeling_llama.py:1052 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['self'].gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].gradient_checkpointing, 8905664)    # if self.gradient_checkpointing and self.training:  # transformers/models/llama/modeling_llama.py:988 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['use_cache'], accessed_by=DictGetItemGuardAccessor(use_cache)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['use_cache'], 8906112)                      # if use_cache:  # transformers/models/llama/modeling_llama.py:758 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8906112)                    # if not return_dict:  # transformers/models/llama/modeling_llama.py:1030 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['position_ids'], accessed_by=DictGetItemGuardAccessor(position_ids)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['position_ids'], 8820832)                   # if position_ids is None:  # transformers/models/llama/modeling_llama.py:968 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['inputs_embeds'], accessed_by=DictGetItemGuardAccessor(inputs_embeds)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['inputs_embeds'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[4, 16, 2048], stride=[32768, 2048, 1])  # past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device  # transformers/models/llama/modeling_llama.py:966 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['inputs_embeds'], '_dynamo_dynamic_indices') == False  # past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device  # transformers/models/llama/modeling_llama.py:966 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['inputs_embeds'], L['attention_mask'])\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['attention_mask'], accessed_by=DictGetItemGuardAccessor(attention_mask)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['attention_mask'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.int64, device=0, requires_grad=False, size=[4, 16], stride=[16, 1])  # if AttentionMaskConverter._ignore_causal_mask_sdpa(  # transformers/models/llama/modeling_llama.py:1065 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- NO_HASATTR: hasattr(L['attention_mask'], '_dynamo_dynamic_indices') == False  # if AttentionMaskConverter._ignore_causal_mask_sdpa(  # transformers/models/llama/modeling_llama.py:1065 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['inputs_embeds'], L['attention_mask'])\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['cache_position'], accessed_by=DictGetItemGuardAccessor(cache_position)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['cache_position'], 8820832)                 # if cache_position is None:  # transformers/models/llama/modeling_llama.py:963 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['past_key_values'], accessed_by=DictGetItemGuardAccessor(past_key_values)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'], 151626256)             # past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0  # transformers/models/llama/modeling_llama.py:964 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=L['past_key_values'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['past_key_values'].key_cache, accessed_by=DictGetItemGuardAccessor(key_cache)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].key_cache, 8839392)     # if len(self.key_cache) <= layer_idx:  # transformers/cache_utils.py:391 in get_seq_length\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- LENGTH_CHECK: not L['past_key_values'].key_cache                            # if len(self.key_cache) <= layer_idx:  # transformers/cache_utils.py:391 in get_seq_length\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['past_key_values'].value_cache, accessed_by=DictGetItemGuardAccessor(value_cache)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['past_key_values'].value_cache, 8839392)   # self.value_cache.append(value_states)  # transformers/cache_utils.py:381 in update\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- LENGTH_CHECK: not L['past_key_values'].value_cache                          # self.value_cache.append(value_states)  # transformers/cache_utils.py:381 in update\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['past_key_values']._seen_tokens, accessed_by=DictGetItemGuardAccessor(_seen_tokens)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- EQUALS_MATCH: L['past_key_values']._seen_tokens == 0                        # self._seen_tokens += key_states.shape[-2]  # transformers/cache_utils.py:376 in update\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=L['past_key_values'].get_seq_length, accessed_by=GetAttrGuardAccessor(get_seq_length)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=L['past_key_values'].get_seq_length, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=L['past_key_values'].get_seq_length.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- EQUALS_MATCH: L['past_key_values'].get_seq_length.__defaults__[0] == 0      # if len(self.key_cache) <= layer_idx:  # transformers/cache_utils.py:391 in get_seq_length\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 8905664)              # if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:  # transformers/models/llama/modeling_llama.py:1064 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['return_legacy_cache'], accessed_by=DictGetItemGuardAccessor(return_legacy_cache)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_legacy_cache'], 8906112)            # if return_legacy_cache:  # transformers/models/llama/modeling_llama.py:1027 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=L['output_hidden_states'], accessed_by=DictGetItemGuardAccessor(output_hidden_states)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_hidden_states'], 8905664)           # all_hidden_states = () if output_hidden_states else None  # transformers/models/llama/modeling_llama.py:980 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 125150850121008)                  # cache_position = torch.arange(  # transformers/models/llama/modeling_llama.py:965 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].nn, accessed_by=GetAttrGuardAccessor(nn)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn, 125150694903136)               # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/models/llama/modeling_llama.py:660 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['torch'].nn.functional, accessed_by=GetAttrGuardAccessor(functional)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional, 125150684383248)    # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/models/llama/modeling_llama.py:660 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['torch'].nn.functional.scaled_dot_product_attention, accessed_by=GetAttrGuardAccessor(scaled_dot_product_attention)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['torch'].nn.functional.scaled_dot_product_attention, 125150786499776)  # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/models/llama/modeling_llama.py:660 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].cat, accessed_by=GetAttrGuardAccessor(cat)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].cat, 125150839476032)              # emb = torch.cat((freqs, freqs), dim=-1)  # transformers/models/llama/modeling_llama.py:210 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].full, accessed_by=GetAttrGuardAccessor(full)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].full, 125150839352880)             # causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)  # transformers/models/llama/modeling_llama.py:94 in _prepare_4d_causal_attention_mask_with_cache_position\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].triu, accessed_by=GetAttrGuardAccessor(triu)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].triu, 125150839399712)             # causal_mask = torch.triu(causal_mask, diagonal=1)  # transformers/models/llama/modeling_llama.py:96 in _prepare_4d_causal_attention_mask_with_cache_position\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].finfo, accessed_by=GetAttrGuardAccessor(finfo)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].finfo, 125150511418784)            # min_dtype = torch.finfo(dtype).min  # transformers/models/llama/modeling_llama.py:1074 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 125150839422576)            # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/llama/modeling_llama.py:124 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].Tensor, accessed_by=GetAttrGuardAccessor(Tensor)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].Tensor, 101225952)                 # if isinstance(attention_mask, torch.Tensor)  # transformers/models/llama/modeling_llama.py:1081 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].arange, accessed_by=GetAttrGuardAccessor(arange)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].arange, 125150839348320)           # cache_position = torch.arange(  # transformers/models/llama/modeling_llama.py:965 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].float32, accessed_by=GetAttrGuardAccessor(float32)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- EQUALS_MATCH: G['torch'].float32 == torch.float32                           # hidden_states = hidden_states.to(torch.float32)  # transformers/models/llama/modeling_llama.py:122 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['torch'].autocast, accessed_by=GetAttrGuardAccessor(autocast)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].autocast, 101663392)               # with torch.autocast(device_type=device_type, enabled=False):  # transformers/models/llama/modeling_llama.py:208 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['repeat_kv'], accessed_by=DictGetItemGuardAccessor(repeat_kv)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['repeat_kv'].__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['repeat_kv'].__code__, 125138873178640)     # key_states = repeat_kv(key_states, self.num_key_value_groups)  # transformers/models/llama/modeling_llama.py:642 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['StaticCache'], accessed_by=DictGetItemGuardAccessor(StaticCache)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['StaticCache'], 151644320)                  # using_static_cache = isinstance(past_key_values, StaticCache)  # transformers/models/llama/modeling_llama.py:1061 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['rotate_half'], accessed_by=DictGetItemGuardAccessor(rotate_half)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['rotate_half'].__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['rotate_half'].__code__, 125138873436272)   # q_embed = (q * cos) + (rotate_half(q) * sin)  # transformers/models/llama/modeling_llama.py:275 in apply_rotary_pos_emb\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['apply_rotary_pos_emb'], accessed_by=DictGetItemGuardAccessor(apply_rotary_pos_emb)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['apply_rotary_pos_emb'].__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['apply_rotary_pos_emb'].__code__, 125138873110320)  # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # transformers/models/llama/modeling_llama.py:635 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['apply_rotary_pos_emb'], accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['apply_rotary_pos_emb'].__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- EQUALS_MATCH: G['apply_rotary_pos_emb'].__defaults__[1] == 1                # cos = cos.unsqueeze(unsqueeze_dim)  # transformers/models/llama/modeling_llama.py:273 in apply_rotary_pos_emb\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['AttentionMaskConverter'], accessed_by=DictGetItemGuardAccessor(AttentionMaskConverter)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['AttentionMaskConverter'], 151665728)       # if AttentionMaskConverter._ignore_causal_mask_sdpa(  # transformers/models/llama/modeling_llama.py:1065 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['AttentionMaskConverter']._unmask_unattended, accessed_by=GetAttrGuardAccessor(_unmask_unattended)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['AttentionMaskConverter']._unmask_unattended.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['AttentionMaskConverter']._unmask_unattended.__code__, 125148098123312)  # causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)  # transformers/models/llama/modeling_llama.py:1106 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['AttentionMaskConverter']._ignore_causal_mask_sdpa, accessed_by=GetAttrGuardAccessor(_ignore_causal_mask_sdpa)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['AttentionMaskConverter']._ignore_causal_mask_sdpa.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['AttentionMaskConverter']._ignore_causal_mask_sdpa.__code__, 151516128)  # if AttentionMaskConverter._ignore_causal_mask_sdpa(  # transformers/models/llama/modeling_llama.py:1065 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['AttentionMaskConverter']._ignore_causal_mask_sdpa, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['AttentionMaskConverter']._ignore_causal_mask_sdpa.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['AttentionMaskConverter']._ignore_causal_mask_sdpa.__defaults__[0], 8820832)  # elif sliding_window is None or key_value_length < sliding_window:  # transformers/modeling_attn_mask_utils.py:276 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['BaseModelOutputWithPast'], accessed_by=DictGetItemGuardAccessor(BaseModelOutputWithPast)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['BaseModelOutputWithPast'], 151703824)      # return BaseModelOutputWithPast(  # transformers/models/llama/modeling_llama.py:1032 in torch_dynamo_resume_in_forward_at_958\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['__builtins_dict___51'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___51)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___51']['len'], accessed_by=DictGetItemGuardAccessor(len)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___51']['len'], 125150921102240)  # if len(self.key_cache) <= layer_idx:  # transformers/cache_utils.py:391 in get_seq_length\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___51']['str'], accessed_by=DictGetItemGuardAccessor(str)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___51']['str'], 8799936)    # device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"  # transformers/models/llama/modeling_llama.py:207 in forward\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___51']['range'], accessed_by=DictGetItemGuardAccessor(range)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___51']['range'], 8816640)  # for layer_idx in range(len(self)):  # transformers/cache_utils.py:403 in to_legacy_cache\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___51']['hasattr'], accessed_by=DictGetItemGuardAccessor(hasattr)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___51']['hasattr'], 125150921101520)  # or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())  # transformers/modeling_attn_mask_utils.py:259 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___51']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___51']['isinstance'], 125150921101920)  # using_static_cache = isinstance(past_key_values, StaticCache)  # transformers/models/llama/modeling_llama.py:1061 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_cache_utils'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_cache_utils)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_cache_utils'], 125150807801456)  # if len(self.key_cache) <= layer_idx:  # transformers/cache_utils.py:391 in get_seq_length\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 125150694908736)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch, 125150850121008)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, accessed_by=GetAttrGuardAccessor(_C)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, 125150839212848)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, accessed_by=GetAttrGuardAccessor(_get_tracing_state)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, 125150786030384)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_modeling_attn_mask_utils)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'], 125148096833920)  # torch.jit.is_tracing()  # transformers/modeling_attn_mask_utils.py:257 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch, 125150850121008)  # torch.jit.is_tracing()  # transformers/modeling_attn_mask_utils.py:257 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.fx, accessed_by=GetAttrGuardAccessor(fx)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch.fx, 125150664935520)  # or isinstance(inputs_embeds, torch.fx.Proxy)  # transformers/modeling_attn_mask_utils.py:258 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.fx.Proxy, accessed_by=GetAttrGuardAccessor(Proxy)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch.fx.Proxy, 112369696)  # or isinstance(inputs_embeds, torch.fx.Proxy)  # transformers/modeling_attn_mask_utils.py:258 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.all, accessed_by=GetAttrGuardAccessor(all)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch.all, 125150839473952)  # return expanded_mask.mul(~torch.all(expanded_mask == min_dtype, dim=-1, keepdim=True))  # transformers/modeling_attn_mask_utils.py:235 in _unmask_unattended\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.jit, accessed_by=GetAttrGuardAccessor(jit)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch.jit, 125150671542832)  # torch.jit.is_tracing()  # transformers/modeling_attn_mask_utils.py:257 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.jit.is_tracing, accessed_by=GetAttrGuardAccessor(is_tracing)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch.jit.is_tracing, 125150672391680)  # torch.jit.is_tracing()  # transformers/modeling_attn_mask_utils.py:257 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch.bool, accessed_by=GetAttrGuardAccessor(bool)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_attn_mask_utils'].torch.bool == torch.bool  # if expanded_mask.dtype == torch.bool:  # transformers/modeling_attn_mask_utils.py:230 in _unmask_unattended\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch._dynamo, accessed_by=GetAttrGuardAccessor(_dynamo)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch._dynamo, 125150850120768)  # or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())  # transformers/modeling_attn_mask_utils.py:259 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_attn_mask_utils'].torch._dynamo.is_compiling, accessed_by=GetAttrGuardAccessor(is_compiling)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_attn_mask_utils'].torch._dynamo.is_compiling, 125150047744576)  # or (hasattr(torch, \"_dynamo\") and torch._dynamo.is_compiling())  # transformers/modeling_attn_mask_utils.py:259 in _ignore_causal_mask_sdpa\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | +- GuardManager: source=G['_prepare_4d_causal_attention_mask_with_cache_position'], accessed_by=DictGetItemGuardAccessor(_prepare_4d_causal_attention_mask_with_cache_position)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | +- GuardManager: source=G['_prepare_4d_causal_attention_mask_with_cache_position'].__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['_prepare_4d_causal_attention_mask_with_cache_position'].__code__, 631943168)  # causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(  # transformers/models/llama/modeling_llama.py:1086 in _update_causal_mask\n",
      "V1024 00:17:52.752000 125150921582400 torch/_dynamo/guards.py:2148] [2/0] [__guards] \n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2169] [3/0] [__guards] GUARDS:\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] \n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] +- RootGuardManager\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125146543924752)                   # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:1203 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:1203 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 150565744)                 # if self.config.pretraining_tp > 1:  # transformers/models/llama/modeling_llama.py:1203 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | +- GuardManager: source=L['self'].lm_head, accessed_by=DictGetItemGuardAccessor(lm_head)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].lm_head, 125138872118800)           # logits = self.lm_head(hidden_states)  # transformers/models/llama/modeling_llama.py:1208 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | | +- GuardManager: source=L['self'].lm_head.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | | | +- GuardManager: source=L['self'].lm_head.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].lm_head.training, 8905664)          # logits = self.lm_head(hidden_states)  # transformers/models/llama/modeling_llama.py:1208 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GuardManager: source=L['labels'], accessed_by=DictGetItemGuardAccessor(labels)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['labels'], 8820832)                         # if labels is not None:  # transformers/models/llama/modeling_llama.py:1212 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GuardManager: source=L['___stack0'], accessed_by=DictGetItemGuardAccessor(___stack0)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- TYPE_MATCH: ___check_type_id(L['___stack0'], 151703824)                   # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=L['___stack0'].attentions, accessed_by=GetAttrGuardAccessor(attentions)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(L['___stack0'].attentions, 8820832)           # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=L['___stack0'].hidden_states, accessed_by=GetAttrGuardAccessor(hidden_states)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(L['___stack0'].hidden_states, 8820832)        # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=L['___stack0'].past_key_values, accessed_by=GetAttrGuardAccessor(past_key_values)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- TYPE_MATCH: ___check_type_id(L['___stack0'].past_key_values, 8810304)     # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- LENGTH_CHECK: len(L['___stack0'].past_key_values) == 16                     # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=L['___stack0'].last_hidden_state, accessed_by=GetAttrGuardAccessor(last_hidden_state)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- TENSOR_MATCH: check_tensor(L['___stack0'].last_hidden_state, Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[4, 16, 2048], stride=[32768, 2048, 1])  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- NO_HASATTR: hasattr(L['___stack0'].last_hidden_state, '_dynamo_dynamic_indices') == False  # outputs = self.model(  # transformers/models/llama/modeling_llama.py:1189 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8906112)                    # if not return_dict:  # transformers/models/llama/modeling_llama.py:1224 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | +- GuardManager: source=G['CausalLMOutputWithPast'], accessed_by=DictGetItemGuardAccessor(CausalLMOutputWithPast)\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['CausalLMOutputWithPast'], 151727920)       # return CausalLMOutputWithPast(  # transformers/models/llama/modeling_llama.py:1228 in torch_dynamo_resume_in_forward_at_1189\n",
      "V1024 00:17:52.808000 125150921582400 torch/_dynamo/guards.py:2148] [3/0] [__guards] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile: 8.5381982421875\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "# Compile the model\n",
    "torch._logging.set_logs(graph=True, recompiles=True, guards=True)\n",
    "model_opt = torch.compile(model, mode=\"max-autotune\")\n",
    "\n",
    "# Generate a batch of 4 inputs\n",
    "inp = generate_data(4)\n",
    "\n",
    "# Run inference on the compiled model\n",
    "with torch.no_grad():\n",
    "    print(\"eager:\", timed(lambda: model(input_ids=inp[0], attention_mask=inp[1]))[1])\n",
    "    print(\"compile:\", timed(lambda: model_opt(input_ids=inp[0], attention_mask=inp[1]))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes torch.compile() work and fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Capture - TorchDynamo and FX Graphs\n",
    "\n",
    "TorchDynamo is responsible for the JIT compilation of Python Code into FX Graphs (the optimization doesn't stop there though).\n",
    "TorchDynamo extracts FX graphs by analyzing Python bytecode during runtime and detecting calls to PyTorch operations.\n",
    "\n",
    "TorchInductor is another component of torch.compile that ingests the FX graphs and compiles them into optimized kernels. However, TorchDynamo allows for different backends to be used as TorchInductor primarily caters to CUDA GPU kernels. \n",
    "\n",
    "In order to inspect the FX graphs that TorchDynamo outputs, let us create a custom backend that outputs the FX graph and simply returns the graph’s unoptimized forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom backend called with FX graph:\n",
      "opcode       name           target                  args                 kwargs\n",
      "-----------  -------------  ----------------------  -------------------  --------\n",
      "placeholder  l_input_ids_   L_input_ids_            ()                   {}\n",
      "call_module  inputs_embeds  L__self___embed_tokens  (l_input_ids_,)      {}\n",
      "output       output         output                  ((inputs_embeds,),)  {}\n",
      "custom backend called with FX graph:\n",
      "opcode         name                                                 target                                                     args                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     kwargs\n",
      "-------------  ---------------------------------------------------  ---------------------------------------------------------  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  -------------------------------------------------------------------------------------------------------\n",
      "placeholder    l_inputs_embeds_                                     L_inputs_embeds_                                           ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "placeholder    l_attention_mask_                                    L_attention_mask_                                          ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  cache_position                                       <built-in method arange of type object at 0x71d2dfc6e580>  (0, 16)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'device': device(type='cuda', index=0)}\n",
      "call_method    position_ids                                         unsqueeze                                                  (cache_position, 0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  causal_mask                                          <built-in method full of type object at 0x71d2dfc6e580>    ((16, 16),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'fill_value': -3.4028234663852886e+38, 'dtype': torch.float32, 'device': device(type='cuda', index=0)}\n",
      "call_function  causal_mask_1                                        <built-in method triu of type object at 0x71d2dfc6e580>    (causal_mask,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {'diagonal': 1}\n",
      "call_function  arange_1                                             <built-in method arange of type object at 0x71d2dfc6e580>  (16,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {'device': device(type='cuda', index=0)}\n",
      "call_method    reshape                                              reshape                                                    (cache_position, -1, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  gt                                                   <built-in function gt>                                     (arange_1, reshape)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  causal_mask_2                                        <built-in function imul>                                   (causal_mask_1, gt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  getitem                                              <built-in function getitem>                                (causal_mask_2, (None, None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    causal_mask_3                                        expand                                                     (getitem, 1, 1, -1, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    causal_mask_4                                        clone                                                      (causal_mask_3,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_1                                            <built-in function getitem>                                (causal_mask_4, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  getitem_2                                            <built-in function getitem>                                (l_attention_mask_, (slice(None, None, None), None, None, slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  padding_mask                                         <built-in function add>                                    (getitem_1, getitem_2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  padding_mask_1                                       <built-in function eq>                                     (padding_mask, 0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_3                                            <built-in function getitem>                                (causal_mask_4, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    masked_fill                                          masked_fill                                                (getitem_3, padding_mask_1, -3.4028234663852886e+38)                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  setitem                                              <built-in function setitem>                                (causal_mask_4, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)), masked_fill)                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  eq_1                                                 <built-in function eq>                                     (causal_mask_4, -3.4028234663852886e+38)                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  all_1                                                <built-in method all of type object at 0x71d2dfc6e580>     (eq_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'dim': -1, 'keepdim': True}\n",
      "call_function  invert                                               <built-in function invert>                                 (all_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    causal_mask_5                                        mul                                                        (causal_mask_4, invert)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  _set_grad_enabled                                    <built-in function _set_grad_enabled>                      (False,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "get_attr       l__self___rotary_emb_inv_freq                        L__self___rotary_emb_inv_freq                              ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_4                                            <built-in function getitem>                                (l__self___rotary_emb_inv_freq, (None, slice(None, None, None), None))                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    float_1                                              float                                                      (getitem_4,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    inv_freq_expanded                                    expand                                                     (float_1, 1, -1, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  getitem_5                                            <built-in function getitem>                                (position_ids, (slice(None, None, None), None, slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    position_ids_expanded                                float                                                      (getitem_5,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_function  _enter_autocast                                      <function _enter_autocast at 0x71d2f0036980>               ('cuda', None, False, None)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_method    float_3                                              float                                                      (inv_freq_expanded,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    float_4                                              float                                                      (position_ids_expanded,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  matmul                                               <built-in function matmul>                                 (float_3, float_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    freqs                                                transpose                                                  (matmul, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  emb                                                  <built-in method cat of type object at 0x71d2dfc6e580>     ((freqs, freqs),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {'dim': -1}\n",
      "call_method    cos                                                  cos                                                        (emb,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    sin                                                  sin                                                        (emb,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  _exit_autocast                                       <function _exit_autocast at 0x71d2f0036ca0>                (_enter_autocast,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  cos_1                                                <built-in function mul>                                    (cos, 1.0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  sin_1                                                <built-in function mul>                                    (sin, 1.0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    cos_2                                                to                                                         (cos_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {'dtype': torch.float32}\n",
      "call_method    sin_2                                                to                                                         (sin_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {'dtype': torch.float32}\n",
      "call_function  _set_grad_enabled_1                                  <built-in function _set_grad_enabled>                      (True,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states                                        to                                                         (l_inputs_embeds_, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_1                                                pow                                                        (hidden_states, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    variance                                             mean                                                       (pow_1, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_1                                                <built-in function add>                                    (variance, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  rsqrt                                                <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  hidden_states_1                                      <built-in function mul>                                    (hidden_states, rsqrt)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "get_attr       l__self___layers_0_input_layernorm_weight            L__self___layers_0_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_3                                                 to                                                         (hidden_states_1, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  hidden_states_2                                      <built-in function mul>                                    (l__self___layers_0_input_layernorm_weight, to_3)                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    query_states                                         L__self___layers_0_self_attn_q_proj                        (hidden_states_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    key_states                                           L__self___layers_0_self_attn_k_proj                        (hidden_states_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    value_states                                         L__self___layers_0_self_attn_v_proj                        (hidden_states_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    view                                                 view                                                       (query_states, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    query_states_1                                       transpose                                                  (view, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    view_1                                               view                                                       (key_states, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    key_states_1                                         transpose                                                  (view_1, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    view_2                                               view                                                       (value_states, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    value_states_1                                       transpose                                                  (view_2, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    cos_3                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_3                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_5                                                <built-in function mul>                                    (query_states_1, cos_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1                                                   <built-in function getitem>                                (query_states_1, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  x2                                                   <built-in function getitem>                                (query_states_1, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  neg                                                  <built-in function neg>                                    (x2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_function  cat_1                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg, x1),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'dim': -1}\n",
      "call_function  mul_6                                                <built-in function mul>                                    (cat_1, sin_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  q_embed                                              <built-in function add>                                    (mul_5, mul_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  mul_7                                                <built-in function mul>                                    (key_states_1, cos_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_function  x1_1                                                 <built-in function getitem>                                (key_states_1, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  x2_1                                                 <built-in function getitem>                                (key_states_1, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  neg_1                                                <built-in function neg>                                    (x2_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_2                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_1, x1_1),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_8                                                <built-in function mul>                                    (cat_2, sin_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  k_embed                                              <built-in function add>                                    (mul_7, mul_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  getitem_10                                           <built-in function getitem>                                (k_embed, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    hidden_states_3                                      expand                                                     (getitem_10, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_2                                         reshape                                                    (hidden_states_3, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_11                                           <built-in function getitem>                                (value_states_1, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    hidden_states_4                                      expand                                                     (getitem_11, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_2                                       reshape                                                    (hidden_states_4, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  causal_mask_6                                        <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_2                                       contiguous                                                 (q_embed,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    key_states_3                                         contiguous                                                 (key_states_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_3                                       contiguous                                                 (value_states_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  attn_output                                          <built-in function scaled_dot_product_attention>           (query_states_2, key_states_3, value_states_3)                                                                                                                                                                                                                                                                                                                                                                                                                                           {'attn_mask': causal_mask_6, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_4                                          transpose                                                  (attn_output, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    attn_output_1                                        contiguous                                                 (transpose_4,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    attn_output_2                                        view                                                       (attn_output_1, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_module    attn_output_3                                        L__self___layers_0_self_attn_o_proj                        (attn_output_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  hidden_states_5                                      <built-in function add>                                    (l_inputs_embeds_, attn_output_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_6                                      to                                                         (hidden_states_5, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    pow_2                                                pow                                                        (hidden_states_6, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    variance_1                                           mean                                                       (pow_2, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_5                                                <built-in function add>                                    (variance_1, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_1                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_5,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  hidden_states_7                                      <built-in function mul>                                    (hidden_states_6, rsqrt_1)                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "get_attr       l__self___layers_0_post_attention_layernorm_weight   L__self___layers_0_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_5                                                 to                                                         (hidden_states_7, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  hidden_states_8                                      <built-in function mul>                                    (l__self___layers_0_post_attention_layernorm_weight, to_5)                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_module    l__self___layers_0_mlp_gate_proj                     L__self___layers_0_mlp_gate_proj                           (hidden_states_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    l__self___layers_0_mlp_act_fn                        L__self___layers_0_mlp_act_fn                              (l__self___layers_0_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_0_mlp_up_proj                       L__self___layers_0_mlp_up_proj                             (hidden_states_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_11                                               <built-in function mul>                                    (l__self___layers_0_mlp_act_fn, l__self___layers_0_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj                                            L__self___layers_0_mlp_down_proj                           (mul_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_9                                      <built-in function add>                                    (hidden_states_5, down_proj)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    hidden_states_10                                     to                                                         (hidden_states_9, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    pow_3                                                pow                                                        (hidden_states_10, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_2                                           mean                                                       (pow_3, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_7                                                <built-in function add>                                    (variance_2, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_2                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_7,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  hidden_states_11                                     <built-in function mul>                                    (hidden_states_10, rsqrt_2)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_1_input_layernorm_weight            L__self___layers_1_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_7                                                 to                                                         (hidden_states_11, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_12                                     <built-in function mul>                                    (l__self___layers_1_input_layernorm_weight, to_7)                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    query_states_3                                       L__self___layers_1_self_attn_q_proj                        (hidden_states_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_4                                         L__self___layers_1_self_attn_k_proj                        (hidden_states_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_4                                       L__self___layers_1_self_attn_v_proj                        (hidden_states_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_4                                               view                                                       (query_states_3, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    query_states_4                                       transpose                                                  (view_4, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    view_5                                               view                                                       (key_states_4, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_5                                         transpose                                                  (view_5, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    view_6                                               view                                                       (value_states_4, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    value_states_5                                       transpose                                                  (view_6, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    cos_4                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_4                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_14                                               <built-in function mul>                                    (query_states_4, cos_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_2                                                 <built-in function getitem>                                (query_states_4, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  x2_2                                                 <built-in function getitem>                                (query_states_4, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  neg_2                                                <built-in function neg>                                    (x2_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_3                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_2, x1_2),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_15                                               <built-in function mul>                                    (cat_3, sin_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  q_embed_1                                            <built-in function add>                                    (mul_14, mul_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_16                                               <built-in function mul>                                    (key_states_5, cos_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_function  x1_3                                                 <built-in function getitem>                                (key_states_5, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  x2_3                                                 <built-in function getitem>                                (key_states_5, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  neg_3                                                <built-in function neg>                                    (x2_3,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_4                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_3, x1_3),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_17                                               <built-in function mul>                                    (cat_4, sin_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  k_embed_1                                            <built-in function add>                                    (mul_16, mul_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_17                                           <built-in function getitem>                                (k_embed_1, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_13                                     expand                                                     (getitem_17, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_6                                         reshape                                                    (hidden_states_13, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_18                                           <built-in function getitem>                                (value_states_5, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    hidden_states_14                                     expand                                                     (getitem_18, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_6                                       reshape                                                    (hidden_states_14, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_7                                        <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_5                                       contiguous                                                 (q_embed_1,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_7                                         contiguous                                                 (key_states_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_7                                       contiguous                                                 (value_states_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  attn_output_4                                        <built-in function scaled_dot_product_attention>           (query_states_5, key_states_7, value_states_7)                                                                                                                                                                                                                                                                                                                                                                                                                                           {'attn_mask': causal_mask_7, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_8                                          transpose                                                  (attn_output_4, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    attn_output_5                                        contiguous                                                 (transpose_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    attn_output_6                                        view                                                       (attn_output_5, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_module    attn_output_7                                        L__self___layers_1_self_attn_o_proj                        (attn_output_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  hidden_states_15                                     <built-in function add>                                    (hidden_states_9, attn_output_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    hidden_states_16                                     to                                                         (hidden_states_15, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_4                                                pow                                                        (hidden_states_16, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_3                                           mean                                                       (pow_4, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_11                                               <built-in function add>                                    (variance_3, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_3                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_17                                     <built-in function mul>                                    (hidden_states_16, rsqrt_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_1_post_attention_layernorm_weight   L__self___layers_1_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_9                                                 to                                                         (hidden_states_17, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_18                                     <built-in function mul>                                    (l__self___layers_1_post_attention_layernorm_weight, to_9)                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_module    l__self___layers_1_mlp_gate_proj                     L__self___layers_1_mlp_gate_proj                           (hidden_states_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_1_mlp_act_fn                        L__self___layers_1_mlp_act_fn                              (l__self___layers_1_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_1_mlp_up_proj                       L__self___layers_1_mlp_up_proj                             (hidden_states_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_20                                               <built-in function mul>                                    (l__self___layers_1_mlp_act_fn, l__self___layers_1_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_1                                          L__self___layers_1_mlp_down_proj                           (mul_20,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_19                                     <built-in function add>                                    (hidden_states_15, down_proj_1)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_20                                     to                                                         (hidden_states_19, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_5                                                pow                                                        (hidden_states_20, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_4                                           mean                                                       (pow_5, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_13                                               <built-in function add>                                    (variance_4, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_4                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_13,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_21                                     <built-in function mul>                                    (hidden_states_20, rsqrt_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_2_input_layernorm_weight            L__self___layers_2_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_11                                                to                                                         (hidden_states_21, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_22                                     <built-in function mul>                                    (l__self___layers_2_input_layernorm_weight, to_11)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_6                                       L__self___layers_2_self_attn_q_proj                        (hidden_states_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_8                                         L__self___layers_2_self_attn_k_proj                        (hidden_states_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_8                                       L__self___layers_2_self_attn_v_proj                        (hidden_states_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_8                                               view                                                       (query_states_6, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    query_states_7                                       transpose                                                  (view_8, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    view_9                                               view                                                       (key_states_8, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_9                                         transpose                                                  (view_9, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    view_10                                              view                                                       (value_states_8, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    value_states_9                                       transpose                                                  (view_10, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_5                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_5                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_23                                               <built-in function mul>                                    (query_states_7, cos_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_4                                                 <built-in function getitem>                                (query_states_7, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  x2_4                                                 <built-in function getitem>                                (query_states_7, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  neg_4                                                <built-in function neg>                                    (x2_4,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_5                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_4, x1_4),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_24                                               <built-in function mul>                                    (cat_5, sin_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  q_embed_2                                            <built-in function add>                                    (mul_23, mul_24)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_25                                               <built-in function mul>                                    (key_states_9, cos_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_function  x1_5                                                 <built-in function getitem>                                (key_states_9, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  x2_5                                                 <built-in function getitem>                                (key_states_9, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  neg_5                                                <built-in function neg>                                    (x2_5,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_6                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_5, x1_5),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_26                                               <built-in function mul>                                    (cat_6, sin_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  k_embed_2                                            <built-in function add>                                    (mul_25, mul_26)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_24                                           <built-in function getitem>                                (k_embed_2, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_23                                     expand                                                     (getitem_24, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_10                                        reshape                                                    (hidden_states_23, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_25                                           <built-in function getitem>                                (value_states_9, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    hidden_states_24                                     expand                                                     (getitem_25, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_10                                      reshape                                                    (hidden_states_24, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_8                                        <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_8                                       contiguous                                                 (q_embed_2,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_11                                        contiguous                                                 (key_states_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_11                                      contiguous                                                 (value_states_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_8                                        <built-in function scaled_dot_product_attention>           (query_states_8, key_states_11, value_states_11)                                                                                                                                                                                                                                                                                                                                                                                                                                         {'attn_mask': causal_mask_8, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_12                                         transpose                                                  (attn_output_8, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    attn_output_9                                        contiguous                                                 (transpose_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_10                                       view                                                       (attn_output_9, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_module    attn_output_11                                       L__self___layers_2_self_attn_o_proj                        (attn_output_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_25                                     <built-in function add>                                    (hidden_states_19, attn_output_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_26                                     to                                                         (hidden_states_25, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_6                                                pow                                                        (hidden_states_26, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_5                                           mean                                                       (pow_6, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_17                                               <built-in function add>                                    (variance_5, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_5                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_17,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_27                                     <built-in function mul>                                    (hidden_states_26, rsqrt_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_2_post_attention_layernorm_weight   L__self___layers_2_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_13                                                to                                                         (hidden_states_27, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_28                                     <built-in function mul>                                    (l__self___layers_2_post_attention_layernorm_weight, to_13)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_2_mlp_gate_proj                     L__self___layers_2_mlp_gate_proj                           (hidden_states_28,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_2_mlp_act_fn                        L__self___layers_2_mlp_act_fn                              (l__self___layers_2_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_2_mlp_up_proj                       L__self___layers_2_mlp_up_proj                             (hidden_states_28,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_29                                               <built-in function mul>                                    (l__self___layers_2_mlp_act_fn, l__self___layers_2_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_2                                          L__self___layers_2_mlp_down_proj                           (mul_29,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_29                                     <built-in function add>                                    (hidden_states_25, down_proj_2)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_30                                     to                                                         (hidden_states_29, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_7                                                pow                                                        (hidden_states_30, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_6                                           mean                                                       (pow_7, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_19                                               <built-in function add>                                    (variance_6, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_6                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_19,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_31                                     <built-in function mul>                                    (hidden_states_30, rsqrt_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_3_input_layernorm_weight            L__self___layers_3_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_15                                                to                                                         (hidden_states_31, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_32                                     <built-in function mul>                                    (l__self___layers_3_input_layernorm_weight, to_15)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_9                                       L__self___layers_3_self_attn_q_proj                        (hidden_states_32,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_12                                        L__self___layers_3_self_attn_k_proj                        (hidden_states_32,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_12                                      L__self___layers_3_self_attn_v_proj                        (hidden_states_32,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_12                                              view                                                       (query_states_9, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    query_states_10                                      transpose                                                  (view_12, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_13                                              view                                                       (key_states_12, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_13                                        transpose                                                  (view_13, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_14                                              view                                                       (value_states_12, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_13                                      transpose                                                  (view_14, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_6                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_6                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_32                                               <built-in function mul>                                    (query_states_10, cos_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  x1_6                                                 <built-in function getitem>                                (query_states_10, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_6                                                 <built-in function getitem>                                (query_states_10, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_6                                                <built-in function neg>                                    (x2_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_7                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_6, x1_6),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_33                                               <built-in function mul>                                    (cat_7, sin_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  q_embed_3                                            <built-in function add>                                    (mul_32, mul_33)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_34                                               <built-in function mul>                                    (key_states_13, cos_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  x1_7                                                 <built-in function getitem>                                (key_states_13, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_7                                                 <built-in function getitem>                                (key_states_13, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_7                                                <built-in function neg>                                    (x2_7,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_8                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_7, x1_7),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_35                                               <built-in function mul>                                    (cat_8, sin_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  k_embed_3                                            <built-in function add>                                    (mul_34, mul_35)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_31                                           <built-in function getitem>                                (k_embed_3, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_33                                     expand                                                     (getitem_31, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_14                                        reshape                                                    (hidden_states_33, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_32                                           <built-in function getitem>                                (value_states_13, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_34                                     expand                                                     (getitem_32, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_14                                      reshape                                                    (hidden_states_34, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_9                                        <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_11                                      contiguous                                                 (q_embed_3,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_15                                        contiguous                                                 (key_states_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_15                                      contiguous                                                 (value_states_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_12                                       <built-in function scaled_dot_product_attention>           (query_states_11, key_states_15, value_states_15)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_9, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_16                                         transpose                                                  (attn_output_12, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_13                                       contiguous                                                 (transpose_16,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_14                                       view                                                       (attn_output_13, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_15                                       L__self___layers_3_self_attn_o_proj                        (attn_output_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_35                                     <built-in function add>                                    (hidden_states_29, attn_output_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_36                                     to                                                         (hidden_states_35, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_8                                                pow                                                        (hidden_states_36, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_7                                           mean                                                       (pow_8, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_23                                               <built-in function add>                                    (variance_7, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_7                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_23,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_37                                     <built-in function mul>                                    (hidden_states_36, rsqrt_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_3_post_attention_layernorm_weight   L__self___layers_3_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_17                                                to                                                         (hidden_states_37, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_38                                     <built-in function mul>                                    (l__self___layers_3_post_attention_layernorm_weight, to_17)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_3_mlp_gate_proj                     L__self___layers_3_mlp_gate_proj                           (hidden_states_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_3_mlp_act_fn                        L__self___layers_3_mlp_act_fn                              (l__self___layers_3_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_3_mlp_up_proj                       L__self___layers_3_mlp_up_proj                             (hidden_states_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_38                                               <built-in function mul>                                    (l__self___layers_3_mlp_act_fn, l__self___layers_3_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_3                                          L__self___layers_3_mlp_down_proj                           (mul_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_39                                     <built-in function add>                                    (hidden_states_35, down_proj_3)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_40                                     to                                                         (hidden_states_39, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_9                                                pow                                                        (hidden_states_40, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_8                                           mean                                                       (pow_9, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'keepdim': True}\n",
      "call_function  add_25                                               <built-in function add>                                    (variance_8, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_8                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_25,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_41                                     <built-in function mul>                                    (hidden_states_40, rsqrt_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_4_input_layernorm_weight            L__self___layers_4_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_19                                                to                                                         (hidden_states_41, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_42                                     <built-in function mul>                                    (l__self___layers_4_input_layernorm_weight, to_19)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_12                                      L__self___layers_4_self_attn_q_proj                        (hidden_states_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_16                                        L__self___layers_4_self_attn_k_proj                        (hidden_states_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_16                                      L__self___layers_4_self_attn_v_proj                        (hidden_states_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_16                                              view                                                       (query_states_12, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_13                                      transpose                                                  (view_16, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_17                                              view                                                       (key_states_16, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_17                                        transpose                                                  (view_17, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_18                                              view                                                       (value_states_16, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_17                                      transpose                                                  (view_18, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_7                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_7                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_41                                               <built-in function mul>                                    (query_states_13, cos_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  x1_8                                                 <built-in function getitem>                                (query_states_13, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_8                                                 <built-in function getitem>                                (query_states_13, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_8                                                <built-in function neg>                                    (x2_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_9                                                <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_8, x1_8),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_42                                               <built-in function mul>                                    (cat_9, sin_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_function  q_embed_4                                            <built-in function add>                                    (mul_41, mul_42)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_43                                               <built-in function mul>                                    (key_states_17, cos_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  x1_9                                                 <built-in function getitem>                                (key_states_17, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_9                                                 <built-in function getitem>                                (key_states_17, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_9                                                <built-in function neg>                                    (x2_9,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  cat_10                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_9, x1_9),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'dim': -1}\n",
      "call_function  mul_44                                               <built-in function mul>                                    (cat_10, sin_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_function  k_embed_4                                            <built-in function add>                                    (mul_43, mul_44)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_38                                           <built-in function getitem>                                (k_embed_4, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_43                                     expand                                                     (getitem_38, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_18                                        reshape                                                    (hidden_states_43, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_39                                           <built-in function getitem>                                (value_states_17, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_44                                     expand                                                     (getitem_39, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_18                                      reshape                                                    (hidden_states_44, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_10                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_14                                      contiguous                                                 (q_embed_4,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_19                                        contiguous                                                 (key_states_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_19                                      contiguous                                                 (value_states_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_16                                       <built-in function scaled_dot_product_attention>           (query_states_14, key_states_19, value_states_19)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_10, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_20                                         transpose                                                  (attn_output_16, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_17                                       contiguous                                                 (transpose_20,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_18                                       view                                                       (attn_output_17, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_19                                       L__self___layers_4_self_attn_o_proj                        (attn_output_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_45                                     <built-in function add>                                    (hidden_states_39, attn_output_19)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_46                                     to                                                         (hidden_states_45, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_10                                               pow                                                        (hidden_states_46, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_9                                           mean                                                       (pow_10, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_29                                               <built-in function add>                                    (variance_9, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  rsqrt_9                                              <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_29,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_47                                     <built-in function mul>                                    (hidden_states_46, rsqrt_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "get_attr       l__self___layers_4_post_attention_layernorm_weight   L__self___layers_4_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_21                                                to                                                         (hidden_states_47, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_48                                     <built-in function mul>                                    (l__self___layers_4_post_attention_layernorm_weight, to_21)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_4_mlp_gate_proj                     L__self___layers_4_mlp_gate_proj                           (hidden_states_48,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_4_mlp_act_fn                        L__self___layers_4_mlp_act_fn                              (l__self___layers_4_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_4_mlp_up_proj                       L__self___layers_4_mlp_up_proj                             (hidden_states_48,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_47                                               <built-in function mul>                                    (l__self___layers_4_mlp_act_fn, l__self___layers_4_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_4                                          L__self___layers_4_mlp_down_proj                           (mul_47,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_49                                     <built-in function add>                                    (hidden_states_45, down_proj_4)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_50                                     to                                                         (hidden_states_49, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_11                                               pow                                                        (hidden_states_50, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_10                                          mean                                                       (pow_11, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_31                                               <built-in function add>                                    (variance_10, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_10                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_31,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_51                                     <built-in function mul>                                    (hidden_states_50, rsqrt_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_5_input_layernorm_weight            L__self___layers_5_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_23                                                to                                                         (hidden_states_51, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_52                                     <built-in function mul>                                    (l__self___layers_5_input_layernorm_weight, to_23)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_15                                      L__self___layers_5_self_attn_q_proj                        (hidden_states_52,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_20                                        L__self___layers_5_self_attn_k_proj                        (hidden_states_52,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_20                                      L__self___layers_5_self_attn_v_proj                        (hidden_states_52,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_20                                              view                                                       (query_states_15, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_16                                      transpose                                                  (view_20, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_21                                              view                                                       (key_states_20, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_21                                        transpose                                                  (view_21, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_22                                              view                                                       (value_states_20, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_21                                      transpose                                                  (view_22, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_8                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_8                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_50                                               <built-in function mul>                                    (query_states_16, cos_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  x1_10                                                <built-in function getitem>                                (query_states_16, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_10                                                <built-in function getitem>                                (query_states_16, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_10                                               <built-in function neg>                                    (x2_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_11                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_10, x1_10),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_51                                               <built-in function mul>                                    (cat_11, sin_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_function  q_embed_5                                            <built-in function add>                                    (mul_50, mul_51)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_52                                               <built-in function mul>                                    (key_states_21, cos_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  x1_11                                                <built-in function getitem>                                (key_states_21, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_11                                                <built-in function getitem>                                (key_states_21, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_11                                               <built-in function neg>                                    (x2_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_12                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_11, x1_11),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_53                                               <built-in function mul>                                    (cat_12, sin_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_function  k_embed_5                                            <built-in function add>                                    (mul_52, mul_53)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_45                                           <built-in function getitem>                                (k_embed_5, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_53                                     expand                                                     (getitem_45, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_22                                        reshape                                                    (hidden_states_53, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_46                                           <built-in function getitem>                                (value_states_21, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_54                                     expand                                                     (getitem_46, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_22                                      reshape                                                    (hidden_states_54, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_11                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_17                                      contiguous                                                 (q_embed_5,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_23                                        contiguous                                                 (key_states_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_23                                      contiguous                                                 (value_states_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_20                                       <built-in function scaled_dot_product_attention>           (query_states_17, key_states_23, value_states_23)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_11, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_24                                         transpose                                                  (attn_output_20, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_21                                       contiguous                                                 (transpose_24,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_22                                       view                                                       (attn_output_21, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_23                                       L__self___layers_5_self_attn_o_proj                        (attn_output_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_55                                     <built-in function add>                                    (hidden_states_49, attn_output_23)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_56                                     to                                                         (hidden_states_55, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_12                                               pow                                                        (hidden_states_56, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_11                                          mean                                                       (pow_12, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_35                                               <built-in function add>                                    (variance_11, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_11                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_35,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_57                                     <built-in function mul>                                    (hidden_states_56, rsqrt_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_5_post_attention_layernorm_weight   L__self___layers_5_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_25                                                to                                                         (hidden_states_57, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_58                                     <built-in function mul>                                    (l__self___layers_5_post_attention_layernorm_weight, to_25)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_5_mlp_gate_proj                     L__self___layers_5_mlp_gate_proj                           (hidden_states_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_5_mlp_act_fn                        L__self___layers_5_mlp_act_fn                              (l__self___layers_5_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_5_mlp_up_proj                       L__self___layers_5_mlp_up_proj                             (hidden_states_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_56                                               <built-in function mul>                                    (l__self___layers_5_mlp_act_fn, l__self___layers_5_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_5                                          L__self___layers_5_mlp_down_proj                           (mul_56,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_59                                     <built-in function add>                                    (hidden_states_55, down_proj_5)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_60                                     to                                                         (hidden_states_59, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_13                                               pow                                                        (hidden_states_60, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_12                                          mean                                                       (pow_13, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_37                                               <built-in function add>                                    (variance_12, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_12                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_37,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_61                                     <built-in function mul>                                    (hidden_states_60, rsqrt_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_6_input_layernorm_weight            L__self___layers_6_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_27                                                to                                                         (hidden_states_61, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_62                                     <built-in function mul>                                    (l__self___layers_6_input_layernorm_weight, to_27)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_18                                      L__self___layers_6_self_attn_q_proj                        (hidden_states_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_24                                        L__self___layers_6_self_attn_k_proj                        (hidden_states_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_24                                      L__self___layers_6_self_attn_v_proj                        (hidden_states_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_24                                              view                                                       (query_states_18, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_19                                      transpose                                                  (view_24, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_25                                              view                                                       (key_states_24, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_25                                        transpose                                                  (view_25, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_26                                              view                                                       (value_states_24, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_25                                      transpose                                                  (view_26, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_9                                                unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_9                                                unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_59                                               <built-in function mul>                                    (query_states_19, cos_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  x1_12                                                <built-in function getitem>                                (query_states_19, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_12                                                <built-in function getitem>                                (query_states_19, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_12                                               <built-in function neg>                                    (x2_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_13                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_12, x1_12),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_60                                               <built-in function mul>                                    (cat_13, sin_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_function  q_embed_6                                            <built-in function add>                                    (mul_59, mul_60)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_61                                               <built-in function mul>                                    (key_states_25, cos_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_function  x1_13                                                <built-in function getitem>                                (key_states_25, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_13                                                <built-in function getitem>                                (key_states_25, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_13                                               <built-in function neg>                                    (x2_13,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_14                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_13, x1_13),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_62                                               <built-in function mul>                                    (cat_14, sin_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_function  k_embed_6                                            <built-in function add>                                    (mul_61, mul_62)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_52                                           <built-in function getitem>                                (k_embed_6, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_63                                     expand                                                     (getitem_52, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_26                                        reshape                                                    (hidden_states_63, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_53                                           <built-in function getitem>                                (value_states_25, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_64                                     expand                                                     (getitem_53, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_26                                      reshape                                                    (hidden_states_64, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_12                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_20                                      contiguous                                                 (q_embed_6,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_27                                        contiguous                                                 (key_states_26,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_27                                      contiguous                                                 (value_states_26,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_24                                       <built-in function scaled_dot_product_attention>           (query_states_20, key_states_27, value_states_27)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_12, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_28                                         transpose                                                  (attn_output_24, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_25                                       contiguous                                                 (transpose_28,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_26                                       view                                                       (attn_output_25, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_27                                       L__self___layers_6_self_attn_o_proj                        (attn_output_26,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_65                                     <built-in function add>                                    (hidden_states_59, attn_output_27)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_66                                     to                                                         (hidden_states_65, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_14                                               pow                                                        (hidden_states_66, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_13                                          mean                                                       (pow_14, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_41                                               <built-in function add>                                    (variance_13, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_13                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_41,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_67                                     <built-in function mul>                                    (hidden_states_66, rsqrt_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_6_post_attention_layernorm_weight   L__self___layers_6_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_29                                                to                                                         (hidden_states_67, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_68                                     <built-in function mul>                                    (l__self___layers_6_post_attention_layernorm_weight, to_29)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_6_mlp_gate_proj                     L__self___layers_6_mlp_gate_proj                           (hidden_states_68,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_6_mlp_act_fn                        L__self___layers_6_mlp_act_fn                              (l__self___layers_6_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_6_mlp_up_proj                       L__self___layers_6_mlp_up_proj                             (hidden_states_68,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_65                                               <built-in function mul>                                    (l__self___layers_6_mlp_act_fn, l__self___layers_6_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_6                                          L__self___layers_6_mlp_down_proj                           (mul_65,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_69                                     <built-in function add>                                    (hidden_states_65, down_proj_6)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_70                                     to                                                         (hidden_states_69, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_15                                               pow                                                        (hidden_states_70, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_14                                          mean                                                       (pow_15, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_43                                               <built-in function add>                                    (variance_14, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_14                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_43,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_71                                     <built-in function mul>                                    (hidden_states_70, rsqrt_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_7_input_layernorm_weight            L__self___layers_7_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_31                                                to                                                         (hidden_states_71, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_72                                     <built-in function mul>                                    (l__self___layers_7_input_layernorm_weight, to_31)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_21                                      L__self___layers_7_self_attn_q_proj                        (hidden_states_72,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_28                                        L__self___layers_7_self_attn_k_proj                        (hidden_states_72,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_28                                      L__self___layers_7_self_attn_v_proj                        (hidden_states_72,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_28                                              view                                                       (query_states_21, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_22                                      transpose                                                  (view_28, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_29                                              view                                                       (key_states_28, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_29                                        transpose                                                  (view_29, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_30                                              view                                                       (value_states_28, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_29                                      transpose                                                  (view_30, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_10                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_10                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_68                                               <built-in function mul>                                    (query_states_22, cos_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_14                                                <built-in function getitem>                                (query_states_22, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_14                                                <built-in function getitem>                                (query_states_22, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_14                                               <built-in function neg>                                    (x2_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_15                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_14, x1_14),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_69                                               <built-in function mul>                                    (cat_15, sin_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_7                                            <built-in function add>                                    (mul_68, mul_69)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_70                                               <built-in function mul>                                    (key_states_29, cos_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_15                                                <built-in function getitem>                                (key_states_29, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_15                                                <built-in function getitem>                                (key_states_29, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_15                                               <built-in function neg>                                    (x2_15,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_16                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_15, x1_15),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_71                                               <built-in function mul>                                    (cat_16, sin_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_7                                            <built-in function add>                                    (mul_70, mul_71)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_59                                           <built-in function getitem>                                (k_embed_7, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_73                                     expand                                                     (getitem_59, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_30                                        reshape                                                    (hidden_states_73, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_60                                           <built-in function getitem>                                (value_states_29, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_74                                     expand                                                     (getitem_60, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_30                                      reshape                                                    (hidden_states_74, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_13                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_23                                      contiguous                                                 (q_embed_7,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_31                                        contiguous                                                 (key_states_30,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_31                                      contiguous                                                 (value_states_30,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_28                                       <built-in function scaled_dot_product_attention>           (query_states_23, key_states_31, value_states_31)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_13, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_32                                         transpose                                                  (attn_output_28, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_29                                       contiguous                                                 (transpose_32,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_30                                       view                                                       (attn_output_29, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_31                                       L__self___layers_7_self_attn_o_proj                        (attn_output_30,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_75                                     <built-in function add>                                    (hidden_states_69, attn_output_31)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_76                                     to                                                         (hidden_states_75, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_16                                               pow                                                        (hidden_states_76, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_15                                          mean                                                       (pow_16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_47                                               <built-in function add>                                    (variance_15, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_15                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_47,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_77                                     <built-in function mul>                                    (hidden_states_76, rsqrt_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_7_post_attention_layernorm_weight   L__self___layers_7_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_33                                                to                                                         (hidden_states_77, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_78                                     <built-in function mul>                                    (l__self___layers_7_post_attention_layernorm_weight, to_33)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_7_mlp_gate_proj                     L__self___layers_7_mlp_gate_proj                           (hidden_states_78,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_7_mlp_act_fn                        L__self___layers_7_mlp_act_fn                              (l__self___layers_7_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_7_mlp_up_proj                       L__self___layers_7_mlp_up_proj                             (hidden_states_78,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_74                                               <built-in function mul>                                    (l__self___layers_7_mlp_act_fn, l__self___layers_7_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_7                                          L__self___layers_7_mlp_down_proj                           (mul_74,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_79                                     <built-in function add>                                    (hidden_states_75, down_proj_7)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_80                                     to                                                         (hidden_states_79, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_17                                               pow                                                        (hidden_states_80, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_16                                          mean                                                       (pow_17, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_49                                               <built-in function add>                                    (variance_16, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_16                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_49,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_81                                     <built-in function mul>                                    (hidden_states_80, rsqrt_16)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_8_input_layernorm_weight            L__self___layers_8_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_35                                                to                                                         (hidden_states_81, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_82                                     <built-in function mul>                                    (l__self___layers_8_input_layernorm_weight, to_35)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_24                                      L__self___layers_8_self_attn_q_proj                        (hidden_states_82,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_32                                        L__self___layers_8_self_attn_k_proj                        (hidden_states_82,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_32                                      L__self___layers_8_self_attn_v_proj                        (hidden_states_82,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_32                                              view                                                       (query_states_24, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_25                                      transpose                                                  (view_32, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_33                                              view                                                       (key_states_32, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_33                                        transpose                                                  (view_33, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_34                                              view                                                       (value_states_32, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_33                                      transpose                                                  (view_34, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_11                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_11                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_77                                               <built-in function mul>                                    (query_states_25, cos_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_16                                                <built-in function getitem>                                (query_states_25, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_16                                                <built-in function getitem>                                (query_states_25, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_16                                               <built-in function neg>                                    (x2_16,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_17                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_16, x1_16),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_78                                               <built-in function mul>                                    (cat_17, sin_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_8                                            <built-in function add>                                    (mul_77, mul_78)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_79                                               <built-in function mul>                                    (key_states_33, cos_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_17                                                <built-in function getitem>                                (key_states_33, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_17                                                <built-in function getitem>                                (key_states_33, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_17                                               <built-in function neg>                                    (x2_17,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_18                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_17, x1_17),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_80                                               <built-in function mul>                                    (cat_18, sin_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_8                                            <built-in function add>                                    (mul_79, mul_80)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_66                                           <built-in function getitem>                                (k_embed_8, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_83                                     expand                                                     (getitem_66, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_34                                        reshape                                                    (hidden_states_83, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_67                                           <built-in function getitem>                                (value_states_33, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_84                                     expand                                                     (getitem_67, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_34                                      reshape                                                    (hidden_states_84, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_14                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_26                                      contiguous                                                 (q_embed_8,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_35                                        contiguous                                                 (key_states_34,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_35                                      contiguous                                                 (value_states_34,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_32                                       <built-in function scaled_dot_product_attention>           (query_states_26, key_states_35, value_states_35)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_14, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_36                                         transpose                                                  (attn_output_32, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_33                                       contiguous                                                 (transpose_36,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_34                                       view                                                       (attn_output_33, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_35                                       L__self___layers_8_self_attn_o_proj                        (attn_output_34,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_85                                     <built-in function add>                                    (hidden_states_79, attn_output_35)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_86                                     to                                                         (hidden_states_85, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_18                                               pow                                                        (hidden_states_86, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_17                                          mean                                                       (pow_18, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_53                                               <built-in function add>                                    (variance_17, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_17                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_53,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_87                                     <built-in function mul>                                    (hidden_states_86, rsqrt_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_8_post_attention_layernorm_weight   L__self___layers_8_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_37                                                to                                                         (hidden_states_87, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_88                                     <built-in function mul>                                    (l__self___layers_8_post_attention_layernorm_weight, to_37)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_8_mlp_gate_proj                     L__self___layers_8_mlp_gate_proj                           (hidden_states_88,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_8_mlp_act_fn                        L__self___layers_8_mlp_act_fn                              (l__self___layers_8_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_8_mlp_up_proj                       L__self___layers_8_mlp_up_proj                             (hidden_states_88,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_83                                               <built-in function mul>                                    (l__self___layers_8_mlp_act_fn, l__self___layers_8_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_8                                          L__self___layers_8_mlp_down_proj                           (mul_83,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_89                                     <built-in function add>                                    (hidden_states_85, down_proj_8)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_90                                     to                                                         (hidden_states_89, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_19                                               pow                                                        (hidden_states_90, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_18                                          mean                                                       (pow_19, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_55                                               <built-in function add>                                    (variance_18, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_18                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_55,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_91                                     <built-in function mul>                                    (hidden_states_90, rsqrt_18)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_9_input_layernorm_weight            L__self___layers_9_input_layernorm_weight                  ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_39                                                to                                                         (hidden_states_91, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_92                                     <built-in function mul>                                    (l__self___layers_9_input_layernorm_weight, to_39)                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_module    query_states_27                                      L__self___layers_9_self_attn_q_proj                        (hidden_states_92,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    key_states_36                                        L__self___layers_9_self_attn_k_proj                        (hidden_states_92,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    value_states_36                                      L__self___layers_9_self_attn_v_proj                        (hidden_states_92,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    view_36                                              view                                                       (query_states_27, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_28                                      transpose                                                  (view_36, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_37                                              view                                                       (key_states_36, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_37                                        transpose                                                  (view_37, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_38                                              view                                                       (value_states_36, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_37                                      transpose                                                  (view_38, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_12                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_12                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_86                                               <built-in function mul>                                    (query_states_28, cos_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_18                                                <built-in function getitem>                                (query_states_28, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_18                                                <built-in function getitem>                                (query_states_28, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_18                                               <built-in function neg>                                    (x2_18,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_19                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_18, x1_18),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_87                                               <built-in function mul>                                    (cat_19, sin_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_9                                            <built-in function add>                                    (mul_86, mul_87)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_88                                               <built-in function mul>                                    (key_states_37, cos_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_19                                                <built-in function getitem>                                (key_states_37, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_19                                                <built-in function getitem>                                (key_states_37, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_19                                               <built-in function neg>                                    (x2_19,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_20                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_19, x1_19),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_89                                               <built-in function mul>                                    (cat_20, sin_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_9                                            <built-in function add>                                    (mul_88, mul_89)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_73                                           <built-in function getitem>                                (k_embed_9, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_method    hidden_states_93                                     expand                                                     (getitem_73, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_38                                        reshape                                                    (hidden_states_93, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  getitem_74                                           <built-in function getitem>                                (value_states_37, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_94                                     expand                                                     (getitem_74, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_38                                      reshape                                                    (hidden_states_94, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  causal_mask_15                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_29                                      contiguous                                                 (q_embed_9,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_method    key_states_39                                        contiguous                                                 (key_states_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_39                                      contiguous                                                 (value_states_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_36                                       <built-in function scaled_dot_product_attention>           (query_states_29, key_states_39, value_states_39)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_15, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_40                                         transpose                                                  (attn_output_36, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_37                                       contiguous                                                 (transpose_40,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_38                                       view                                                       (attn_output_37, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_39                                       L__self___layers_9_self_attn_o_proj                        (attn_output_38,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_95                                     <built-in function add>                                    (hidden_states_89, attn_output_39)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_96                                     to                                                         (hidden_states_95, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_20                                               pow                                                        (hidden_states_96, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                    {}\n",
      "call_method    variance_19                                          mean                                                       (pow_20, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_59                                               <built-in function add>                                    (variance_19, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_19                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_59,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_97                                     <built-in function mul>                                    (hidden_states_96, rsqrt_19)                                                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "get_attr       l__self___layers_9_post_attention_layernorm_weight   L__self___layers_9_post_attention_layernorm_weight         ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_41                                                to                                                         (hidden_states_97, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_98                                     <built-in function mul>                                    (l__self___layers_9_post_attention_layernorm_weight, to_41)                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    l__self___layers_9_mlp_gate_proj                     L__self___layers_9_mlp_gate_proj                           (hidden_states_98,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_9_mlp_act_fn                        L__self___layers_9_mlp_act_fn                              (l__self___layers_9_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    l__self___layers_9_mlp_up_proj                       L__self___layers_9_mlp_up_proj                             (hidden_states_98,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_function  mul_92                                               <built-in function mul>                                    (l__self___layers_9_mlp_act_fn, l__self___layers_9_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_module    down_proj_9                                          L__self___layers_9_mlp_down_proj                           (mul_92,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_99                                     <built-in function add>                                    (hidden_states_95, down_proj_9)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    hidden_states_100                                    to                                                         (hidden_states_99, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    pow_21                                               pow                                                        (hidden_states_100, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_20                                          mean                                                       (pow_21, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_61                                               <built-in function add>                                    (variance_20, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_20                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_61,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_101                                    <built-in function mul>                                    (hidden_states_100, rsqrt_20)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_10_input_layernorm_weight           L__self___layers_10_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_43                                                to                                                         (hidden_states_101, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_102                                    <built-in function mul>                                    (l__self___layers_10_input_layernorm_weight, to_43)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_30                                      L__self___layers_10_self_attn_q_proj                       (hidden_states_102,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_40                                        L__self___layers_10_self_attn_k_proj                       (hidden_states_102,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_40                                      L__self___layers_10_self_attn_v_proj                       (hidden_states_102,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_40                                              view                                                       (query_states_30, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_31                                      transpose                                                  (view_40, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_41                                              view                                                       (key_states_40, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_41                                        transpose                                                  (view_41, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_42                                              view                                                       (value_states_40, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_41                                      transpose                                                  (view_42, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_13                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_13                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_95                                               <built-in function mul>                                    (query_states_31, cos_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_20                                                <built-in function getitem>                                (query_states_31, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_20                                                <built-in function getitem>                                (query_states_31, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_20                                               <built-in function neg>                                    (x2_20,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_21                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_20, x1_20),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_96                                               <built-in function mul>                                    (cat_21, sin_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_10                                           <built-in function add>                                    (mul_95, mul_96)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  mul_97                                               <built-in function mul>                                    (key_states_41, cos_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_21                                                <built-in function getitem>                                (key_states_41, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_21                                                <built-in function getitem>                                (key_states_41, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_21                                               <built-in function neg>                                    (x2_21,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_22                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_21, x1_21),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_98                                               <built-in function mul>                                    (cat_22, sin_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_10                                           <built-in function add>                                    (mul_97, mul_98)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  getitem_80                                           <built-in function getitem>                                (k_embed_10, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_103                                    expand                                                     (getitem_80, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_42                                        reshape                                                    (hidden_states_103, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_81                                           <built-in function getitem>                                (value_states_41, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_104                                    expand                                                     (getitem_81, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_42                                      reshape                                                    (hidden_states_104, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_16                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_32                                      contiguous                                                 (q_embed_10,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_43                                        contiguous                                                 (key_states_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_43                                      contiguous                                                 (value_states_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_40                                       <built-in function scaled_dot_product_attention>           (query_states_32, key_states_43, value_states_43)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_16, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_44                                         transpose                                                  (attn_output_40, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_41                                       contiguous                                                 (transpose_44,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_42                                       view                                                       (attn_output_41, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_43                                       L__self___layers_10_self_attn_o_proj                       (attn_output_42,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_105                                    <built-in function add>                                    (hidden_states_99, attn_output_43)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    hidden_states_106                                    to                                                         (hidden_states_105, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_22                                               pow                                                        (hidden_states_106, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_21                                          mean                                                       (pow_22, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_65                                               <built-in function add>                                    (variance_21, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_21                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_65,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_107                                    <built-in function mul>                                    (hidden_states_106, rsqrt_21)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_10_post_attention_layernorm_weight  L__self___layers_10_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_45                                                to                                                         (hidden_states_107, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_108                                    <built-in function mul>                                    (l__self___layers_10_post_attention_layernorm_weight, to_45)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_10_mlp_gate_proj                    L__self___layers_10_mlp_gate_proj                          (hidden_states_108,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_10_mlp_act_fn                       L__self___layers_10_mlp_act_fn                             (l__self___layers_10_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_10_mlp_up_proj                      L__self___layers_10_mlp_up_proj                            (hidden_states_108,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_101                                              <built-in function mul>                                    (l__self___layers_10_mlp_act_fn, l__self___layers_10_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_10                                         L__self___layers_10_mlp_down_proj                          (mul_101,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_109                                    <built-in function add>                                    (hidden_states_105, down_proj_10)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_110                                    to                                                         (hidden_states_109, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_23                                               pow                                                        (hidden_states_110, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_22                                          mean                                                       (pow_23, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_67                                               <built-in function add>                                    (variance_22, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_22                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_67,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_111                                    <built-in function mul>                                    (hidden_states_110, rsqrt_22)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_11_input_layernorm_weight           L__self___layers_11_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_47                                                to                                                         (hidden_states_111, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_112                                    <built-in function mul>                                    (l__self___layers_11_input_layernorm_weight, to_47)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_33                                      L__self___layers_11_self_attn_q_proj                       (hidden_states_112,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_44                                        L__self___layers_11_self_attn_k_proj                       (hidden_states_112,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_44                                      L__self___layers_11_self_attn_v_proj                       (hidden_states_112,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_44                                              view                                                       (query_states_33, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_34                                      transpose                                                  (view_44, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_45                                              view                                                       (key_states_44, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_45                                        transpose                                                  (view_45, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_46                                              view                                                       (value_states_44, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_45                                      transpose                                                  (view_46, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_14                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_14                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_104                                              <built-in function mul>                                    (query_states_34, cos_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_22                                                <built-in function getitem>                                (query_states_34, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_22                                                <built-in function getitem>                                (query_states_34, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_22                                               <built-in function neg>                                    (x2_22,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_23                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_22, x1_22),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_105                                              <built-in function mul>                                    (cat_23, sin_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_11                                           <built-in function add>                                    (mul_104, mul_105)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_106                                              <built-in function mul>                                    (key_states_45, cos_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_23                                                <built-in function getitem>                                (key_states_45, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_23                                                <built-in function getitem>                                (key_states_45, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_23                                               <built-in function neg>                                    (x2_23,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_24                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_23, x1_23),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_107                                              <built-in function mul>                                    (cat_24, sin_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_11                                           <built-in function add>                                    (mul_106, mul_107)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_87                                           <built-in function getitem>                                (k_embed_11, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_113                                    expand                                                     (getitem_87, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_46                                        reshape                                                    (hidden_states_113, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_88                                           <built-in function getitem>                                (value_states_45, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_114                                    expand                                                     (getitem_88, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_46                                      reshape                                                    (hidden_states_114, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_17                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_35                                      contiguous                                                 (q_embed_11,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_47                                        contiguous                                                 (key_states_46,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_47                                      contiguous                                                 (value_states_46,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_44                                       <built-in function scaled_dot_product_attention>           (query_states_35, key_states_47, value_states_47)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_17, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_48                                         transpose                                                  (attn_output_44, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_45                                       contiguous                                                 (transpose_48,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_46                                       view                                                       (attn_output_45, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_47                                       L__self___layers_11_self_attn_o_proj                       (attn_output_46,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_115                                    <built-in function add>                                    (hidden_states_109, attn_output_47)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    hidden_states_116                                    to                                                         (hidden_states_115, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_24                                               pow                                                        (hidden_states_116, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_23                                          mean                                                       (pow_24, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_71                                               <built-in function add>                                    (variance_23, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_23                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_71,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_117                                    <built-in function mul>                                    (hidden_states_116, rsqrt_23)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_11_post_attention_layernorm_weight  L__self___layers_11_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_49                                                to                                                         (hidden_states_117, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_118                                    <built-in function mul>                                    (l__self___layers_11_post_attention_layernorm_weight, to_49)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_11_mlp_gate_proj                    L__self___layers_11_mlp_gate_proj                          (hidden_states_118,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_11_mlp_act_fn                       L__self___layers_11_mlp_act_fn                             (l__self___layers_11_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_11_mlp_up_proj                      L__self___layers_11_mlp_up_proj                            (hidden_states_118,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_110                                              <built-in function mul>                                    (l__self___layers_11_mlp_act_fn, l__self___layers_11_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_11                                         L__self___layers_11_mlp_down_proj                          (mul_110,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_119                                    <built-in function add>                                    (hidden_states_115, down_proj_11)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_120                                    to                                                         (hidden_states_119, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_25                                               pow                                                        (hidden_states_120, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_24                                          mean                                                       (pow_25, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_73                                               <built-in function add>                                    (variance_24, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_24                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_73,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_121                                    <built-in function mul>                                    (hidden_states_120, rsqrt_24)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_12_input_layernorm_weight           L__self___layers_12_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_51                                                to                                                         (hidden_states_121, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_122                                    <built-in function mul>                                    (l__self___layers_12_input_layernorm_weight, to_51)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_36                                      L__self___layers_12_self_attn_q_proj                       (hidden_states_122,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_48                                        L__self___layers_12_self_attn_k_proj                       (hidden_states_122,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_48                                      L__self___layers_12_self_attn_v_proj                       (hidden_states_122,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_48                                              view                                                       (query_states_36, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_37                                      transpose                                                  (view_48, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_49                                              view                                                       (key_states_48, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_49                                        transpose                                                  (view_49, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_50                                              view                                                       (value_states_48, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_49                                      transpose                                                  (view_50, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_15                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_15                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_113                                              <built-in function mul>                                    (query_states_37, cos_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_24                                                <built-in function getitem>                                (query_states_37, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_24                                                <built-in function getitem>                                (query_states_37, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_24                                               <built-in function neg>                                    (x2_24,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_25                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_24, x1_24),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_114                                              <built-in function mul>                                    (cat_25, sin_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_12                                           <built-in function add>                                    (mul_113, mul_114)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_115                                              <built-in function mul>                                    (key_states_49, cos_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_25                                                <built-in function getitem>                                (key_states_49, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_25                                                <built-in function getitem>                                (key_states_49, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_25                                               <built-in function neg>                                    (x2_25,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_26                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_25, x1_25),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_116                                              <built-in function mul>                                    (cat_26, sin_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_12                                           <built-in function add>                                    (mul_115, mul_116)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_94                                           <built-in function getitem>                                (k_embed_12, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_123                                    expand                                                     (getitem_94, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_50                                        reshape                                                    (hidden_states_123, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_95                                           <built-in function getitem>                                (value_states_49, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_124                                    expand                                                     (getitem_95, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    value_states_50                                      reshape                                                    (hidden_states_124, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_18                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_38                                      contiguous                                                 (q_embed_12,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_51                                        contiguous                                                 (key_states_50,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_51                                      contiguous                                                 (value_states_50,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_48                                       <built-in function scaled_dot_product_attention>           (query_states_38, key_states_51, value_states_51)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_18, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_52                                         transpose                                                  (attn_output_48, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_49                                       contiguous                                                 (transpose_52,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_50                                       view                                                       (attn_output_49, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_51                                       L__self___layers_12_self_attn_o_proj                       (attn_output_50,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_125                                    <built-in function add>                                    (hidden_states_119, attn_output_51)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    hidden_states_126                                    to                                                         (hidden_states_125, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_26                                               pow                                                        (hidden_states_126, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_25                                          mean                                                       (pow_26, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_77                                               <built-in function add>                                    (variance_25, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_25                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_77,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_127                                    <built-in function mul>                                    (hidden_states_126, rsqrt_25)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_12_post_attention_layernorm_weight  L__self___layers_12_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_53                                                to                                                         (hidden_states_127, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_128                                    <built-in function mul>                                    (l__self___layers_12_post_attention_layernorm_weight, to_53)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_12_mlp_gate_proj                    L__self___layers_12_mlp_gate_proj                          (hidden_states_128,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_12_mlp_act_fn                       L__self___layers_12_mlp_act_fn                             (l__self___layers_12_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_12_mlp_up_proj                      L__self___layers_12_mlp_up_proj                            (hidden_states_128,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_119                                              <built-in function mul>                                    (l__self___layers_12_mlp_act_fn, l__self___layers_12_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_12                                         L__self___layers_12_mlp_down_proj                          (mul_119,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_129                                    <built-in function add>                                    (hidden_states_125, down_proj_12)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_130                                    to                                                         (hidden_states_129, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_27                                               pow                                                        (hidden_states_130, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_26                                          mean                                                       (pow_27, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_79                                               <built-in function add>                                    (variance_26, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_26                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_79,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_131                                    <built-in function mul>                                    (hidden_states_130, rsqrt_26)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_13_input_layernorm_weight           L__self___layers_13_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_55                                                to                                                         (hidden_states_131, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_132                                    <built-in function mul>                                    (l__self___layers_13_input_layernorm_weight, to_55)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_39                                      L__self___layers_13_self_attn_q_proj                       (hidden_states_132,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_52                                        L__self___layers_13_self_attn_k_proj                       (hidden_states_132,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_52                                      L__self___layers_13_self_attn_v_proj                       (hidden_states_132,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_52                                              view                                                       (query_states_39, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_40                                      transpose                                                  (view_52, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_53                                              view                                                       (key_states_52, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_53                                        transpose                                                  (view_53, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_54                                              view                                                       (value_states_52, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_53                                      transpose                                                  (view_54, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_16                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_16                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_122                                              <built-in function mul>                                    (query_states_40, cos_16)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_26                                                <built-in function getitem>                                (query_states_40, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_26                                                <built-in function getitem>                                (query_states_40, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_26                                               <built-in function neg>                                    (x2_26,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_27                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_26, x1_26),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_123                                              <built-in function mul>                                    (cat_27, sin_16)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_13                                           <built-in function add>                                    (mul_122, mul_123)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_124                                              <built-in function mul>                                    (key_states_53, cos_16)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_27                                                <built-in function getitem>                                (key_states_53, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_27                                                <built-in function getitem>                                (key_states_53, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_27                                               <built-in function neg>                                    (x2_27,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_28                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_27, x1_27),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_125                                              <built-in function mul>                                    (cat_28, sin_16)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_13                                           <built-in function add>                                    (mul_124, mul_125)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_101                                          <built-in function getitem>                                (k_embed_13, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_133                                    expand                                                     (getitem_101, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    key_states_54                                        reshape                                                    (hidden_states_133, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_102                                          <built-in function getitem>                                (value_states_53, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_134                                    expand                                                     (getitem_102, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    value_states_54                                      reshape                                                    (hidden_states_134, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_19                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_41                                      contiguous                                                 (q_embed_13,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_55                                        contiguous                                                 (key_states_54,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_55                                      contiguous                                                 (value_states_54,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_52                                       <built-in function scaled_dot_product_attention>           (query_states_41, key_states_55, value_states_55)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_19, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_56                                         transpose                                                  (attn_output_52, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_53                                       contiguous                                                 (transpose_56,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_54                                       view                                                       (attn_output_53, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_55                                       L__self___layers_13_self_attn_o_proj                       (attn_output_54,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_135                                    <built-in function add>                                    (hidden_states_129, attn_output_55)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    hidden_states_136                                    to                                                         (hidden_states_135, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_28                                               pow                                                        (hidden_states_136, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_27                                          mean                                                       (pow_28, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_83                                               <built-in function add>                                    (variance_27, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_27                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_83,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_137                                    <built-in function mul>                                    (hidden_states_136, rsqrt_27)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_13_post_attention_layernorm_weight  L__self___layers_13_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_57                                                to                                                         (hidden_states_137, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_138                                    <built-in function mul>                                    (l__self___layers_13_post_attention_layernorm_weight, to_57)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_13_mlp_gate_proj                    L__self___layers_13_mlp_gate_proj                          (hidden_states_138,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_13_mlp_act_fn                       L__self___layers_13_mlp_act_fn                             (l__self___layers_13_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_13_mlp_up_proj                      L__self___layers_13_mlp_up_proj                            (hidden_states_138,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_128                                              <built-in function mul>                                    (l__self___layers_13_mlp_act_fn, l__self___layers_13_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_13                                         L__self___layers_13_mlp_down_proj                          (mul_128,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_139                                    <built-in function add>                                    (hidden_states_135, down_proj_13)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_140                                    to                                                         (hidden_states_139, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_29                                               pow                                                        (hidden_states_140, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_28                                          mean                                                       (pow_29, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_85                                               <built-in function add>                                    (variance_28, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_28                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_85,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_141                                    <built-in function mul>                                    (hidden_states_140, rsqrt_28)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_14_input_layernorm_weight           L__self___layers_14_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_59                                                to                                                         (hidden_states_141, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_142                                    <built-in function mul>                                    (l__self___layers_14_input_layernorm_weight, to_59)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_42                                      L__self___layers_14_self_attn_q_proj                       (hidden_states_142,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_56                                        L__self___layers_14_self_attn_k_proj                       (hidden_states_142,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_56                                      L__self___layers_14_self_attn_v_proj                       (hidden_states_142,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_56                                              view                                                       (query_states_42, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_43                                      transpose                                                  (view_56, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_57                                              view                                                       (key_states_56, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_57                                        transpose                                                  (view_57, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_58                                              view                                                       (value_states_56, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_57                                      transpose                                                  (view_58, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_17                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_17                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_131                                              <built-in function mul>                                    (query_states_43, cos_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_28                                                <built-in function getitem>                                (query_states_43, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_28                                                <built-in function getitem>                                (query_states_43, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_28                                               <built-in function neg>                                    (x2_28,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_29                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_28, x1_28),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_132                                              <built-in function mul>                                    (cat_29, sin_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_14                                           <built-in function add>                                    (mul_131, mul_132)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_133                                              <built-in function mul>                                    (key_states_57, cos_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_29                                                <built-in function getitem>                                (key_states_57, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_29                                                <built-in function getitem>                                (key_states_57, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_29                                               <built-in function neg>                                    (x2_29,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_30                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_29, x1_29),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_134                                              <built-in function mul>                                    (cat_30, sin_17)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_14                                           <built-in function add>                                    (mul_133, mul_134)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_108                                          <built-in function getitem>                                (k_embed_14, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_143                                    expand                                                     (getitem_108, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    key_states_58                                        reshape                                                    (hidden_states_143, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_109                                          <built-in function getitem>                                (value_states_57, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_144                                    expand                                                     (getitem_109, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    value_states_58                                      reshape                                                    (hidden_states_144, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_20                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_44                                      contiguous                                                 (q_embed_14,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_59                                        contiguous                                                 (key_states_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_59                                      contiguous                                                 (value_states_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_56                                       <built-in function scaled_dot_product_attention>           (query_states_44, key_states_59, value_states_59)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_20, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_60                                         transpose                                                  (attn_output_56, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_57                                       contiguous                                                 (transpose_60,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_58                                       view                                                       (attn_output_57, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_59                                       L__self___layers_14_self_attn_o_proj                       (attn_output_58,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_145                                    <built-in function add>                                    (hidden_states_139, attn_output_59)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    hidden_states_146                                    to                                                         (hidden_states_145, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_30                                               pow                                                        (hidden_states_146, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_29                                          mean                                                       (pow_30, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_89                                               <built-in function add>                                    (variance_29, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_29                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_89,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_147                                    <built-in function mul>                                    (hidden_states_146, rsqrt_29)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_14_post_attention_layernorm_weight  L__self___layers_14_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_61                                                to                                                         (hidden_states_147, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_148                                    <built-in function mul>                                    (l__self___layers_14_post_attention_layernorm_weight, to_61)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_14_mlp_gate_proj                    L__self___layers_14_mlp_gate_proj                          (hidden_states_148,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_14_mlp_act_fn                       L__self___layers_14_mlp_act_fn                             (l__self___layers_14_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_14_mlp_up_proj                      L__self___layers_14_mlp_up_proj                            (hidden_states_148,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_137                                              <built-in function mul>                                    (l__self___layers_14_mlp_act_fn, l__self___layers_14_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_14                                         L__self___layers_14_mlp_down_proj                          (mul_137,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_149                                    <built-in function add>                                    (hidden_states_145, down_proj_14)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_150                                    to                                                         (hidden_states_149, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_31                                               pow                                                        (hidden_states_150, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_30                                          mean                                                       (pow_31, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_91                                               <built-in function add>                                    (variance_30, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_30                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_91,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_151                                    <built-in function mul>                                    (hidden_states_150, rsqrt_30)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_15_input_layernorm_weight           L__self___layers_15_input_layernorm_weight                 ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_63                                                to                                                         (hidden_states_151, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_152                                    <built-in function mul>                                    (l__self___layers_15_input_layernorm_weight, to_63)                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_module    query_states_45                                      L__self___layers_15_self_attn_q_proj                       (hidden_states_152,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    key_states_60                                        L__self___layers_15_self_attn_k_proj                       (hidden_states_152,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    value_states_60                                      L__self___layers_15_self_attn_v_proj                       (hidden_states_152,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_method    view_60                                              view                                                       (query_states_45, 1, 16, 32, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    query_states_46                                      transpose                                                  (view_60, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_61                                              view                                                       (key_states_60, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_61                                        transpose                                                  (view_61, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    view_62                                              view                                                       (value_states_60, 1, 16, 8, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    value_states_61                                      transpose                                                  (view_62, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    cos_18                                               unsqueeze                                                  (cos_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_method    sin_18                                               unsqueeze                                                  (sin_2, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  mul_140                                              <built-in function mul>                                    (query_states_46, cos_18)                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  x1_30                                                <built-in function getitem>                                (query_states_46, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  x2_30                                                <built-in function getitem>                                (query_states_46, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  neg_30                                               <built-in function neg>                                    (x2_30,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_31                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_30, x1_30),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_141                                              <built-in function mul>                                    (cat_31, sin_18)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  q_embed_15                                           <built-in function add>                                    (mul_140, mul_141)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  mul_142                                              <built-in function mul>                                    (key_states_61, cos_18)                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {}\n",
      "call_function  x1_31                                                <built-in function getitem>                                (key_states_61, (Ellipsis, slice(None, 32, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  x2_31                                                <built-in function getitem>                                (key_states_61, (Ellipsis, slice(32, None, None)))                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  neg_31                                               <built-in function neg>                                    (x2_31,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_function  cat_32                                               <built-in method cat of type object at 0x71d2dfc6e580>     ((neg_31, x1_31),)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {'dim': -1}\n",
      "call_function  mul_143                                              <built-in function mul>                                    (cat_32, sin_18)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_function  k_embed_15                                           <built-in function add>                                    (mul_142, mul_143)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_115                                          <built-in function getitem>                                (k_embed_15, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                                 {}\n",
      "call_method    hidden_states_153                                    expand                                                     (getitem_115, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    key_states_62                                        reshape                                                    (hidden_states_153, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  getitem_116                                          <built-in function getitem>                                (value_states_61, (slice(None, None, None), slice(None, None, None), None, slice(None, None, None), slice(None, None, None)))                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    hidden_states_154                                    expand                                                     (getitem_116, 1, 8, 4, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "call_method    value_states_62                                      reshape                                                    (hidden_states_154, 1, 32, 16, 64)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  causal_mask_21                                       <built-in function getitem>                                (causal_mask_5, (slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 16, None)))                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    query_states_47                                      contiguous                                                 (q_embed_15,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "call_method    key_states_63                                        contiguous                                                 (key_states_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {}\n",
      "call_method    value_states_63                                      contiguous                                                 (value_states_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  attn_output_60                                       <built-in function scaled_dot_product_attention>           (query_states_47, key_states_63, value_states_63)                                                                                                                                                                                                                                                                                                                                                                                                                                        {'attn_mask': causal_mask_21, 'dropout_p': 0.0, 'is_causal': False}\n",
      "call_method    transpose_64                                         transpose                                                  (attn_output_60, 1, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    attn_output_61                                       contiguous                                                 (transpose_64,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {}\n",
      "call_method    attn_output_62                                       view                                                       (attn_output_61, 1, 16, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                              {}\n",
      "call_module    attn_output_63                                       L__self___layers_15_self_attn_o_proj                       (attn_output_62,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_function  hidden_states_155                                    <built-in function add>                                    (hidden_states_149, attn_output_63)                                                                                                                                                                                                                                                                                                                                                                                                                                                      {}\n",
      "call_method    hidden_states_156                                    to                                                         (hidden_states_155, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_32                                               pow                                                        (hidden_states_156, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_31                                          mean                                                       (pow_32, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_95                                               <built-in function add>                                    (variance_31, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_31                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_95,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_157                                    <built-in function mul>                                    (hidden_states_156, rsqrt_31)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___layers_15_post_attention_layernorm_weight  L__self___layers_15_post_attention_layernorm_weight        ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_65                                                to                                                         (hidden_states_157, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_158                                    <built-in function mul>                                    (l__self___layers_15_post_attention_layernorm_weight, to_65)                                                                                                                                                                                                                                                                                                                                                                                                                             {}\n",
      "call_module    l__self___layers_15_mlp_gate_proj                    L__self___layers_15_mlp_gate_proj                          (hidden_states_158,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_15_mlp_act_fn                       L__self___layers_15_mlp_act_fn                             (l__self___layers_15_mlp_gate_proj,)                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_module    l__self___layers_15_mlp_up_proj                      L__self___layers_15_mlp_up_proj                            (hidden_states_158,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  mul_146                                              <built-in function mul>                                    (l__self___layers_15_mlp_act_fn, l__self___layers_15_mlp_up_proj)                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_module    down_proj_15                                         L__self___layers_15_mlp_down_proj                          (mul_146,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               {}\n",
      "call_function  hidden_states_159                                    <built-in function add>                                    (hidden_states_155, down_proj_15)                                                                                                                                                                                                                                                                                                                                                                                                                                                        {}\n",
      "call_method    hidden_states_160                                    to                                                         (hidden_states_159, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    pow_33                                               pow                                                        (hidden_states_160, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {}\n",
      "call_method    variance_32                                          mean                                                       (pow_33, -1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'keepdim': True}\n",
      "call_function  add_97                                               <built-in function add>                                    (variance_32, 1e-05)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {}\n",
      "call_function  rsqrt_32                                             <built-in method rsqrt of type object at 0x71d2dfc6e580>   (add_97,)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {}\n",
      "call_function  hidden_states_161                                    <built-in function mul>                                    (hidden_states_160, rsqrt_32)                                                                                                                                                                                                                                                                                                                                                                                                                                                            {}\n",
      "get_attr       l__self___norm_weight                                L__self___norm_weight                                      ()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_method    to_67                                                to                                                         (hidden_states_161, torch.float32)                                                                                                                                                                                                                                                                                                                                                                                                                                                       {}\n",
      "call_function  hidden_states_162                                    <built-in function mul>                                    (l__self___norm_weight, to_67)                                                                                                                                                                                                                                                                                                                                                                                                                                                           {}\n",
      "output         output                                               output                                                     ((hidden_states_162, k_embed, value_states_1, k_embed_1, value_states_5, k_embed_2, value_states_9, k_embed_3, value_states_13, k_embed_4, value_states_17, k_embed_5, value_states_21, k_embed_6, value_states_25, k_embed_7, value_states_29, k_embed_8, value_states_33, k_embed_9, value_states_37, k_embed_10, value_states_41, k_embed_11, value_states_45, k_embed_12, value_states_49, k_embed_13, value_states_53, k_embed_14, value_states_57, k_embed_15, value_states_61),)  {}\n",
      "custom backend called with FX graph:\n",
      "opcode       name                        target                      args                           kwargs\n",
      "-----------  --------------------------  --------------------------  -----------------------------  --------\n",
      "placeholder  l_stack0_last_hidden_state  L_stack0_last_hidden_state  ()                             {}\n",
      "call_module  logits                      L__self___lm_head           (l_stack0_last_hidden_state,)  {}\n",
      "call_method  logits_1                    float                       (logits,)                      {}\n",
      "output       output                      output                      ((logits_1,),)                 {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3232,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [11.0843,  8.6790,  8.2186,  ..., -0.7705, -0.7701, -0.7702],\n",
       "         [14.8126, 10.1446,  9.3261,  ...,  0.3494,  0.3493,  0.3494],\n",
       "         ...,\n",
       "         [14.8741, 10.8963,  8.3688,  ...,  1.5774,  1.5776,  1.5778],\n",
       "         [ 9.7592,  9.4394,  5.6869,  ...,  1.3620,  1.3622,  1.3621],\n",
       "         [ 7.6835,  9.0022,  5.7623,  ...,  0.6373,  0.6378,  0.6378]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[ 9.5610e-02,  1.7827e-01,  3.6755e-02,  ..., -1.7652e+00,\n",
       "            1.7963e+00,  7.9892e-03],\n",
       "          [-3.1883e-02,  1.8593e-01, -1.0885e+00,  ...,  1.0211e+00,\n",
       "           -1.0414e+00, -1.9638e+00],\n",
       "          [-5.4410e+00, -1.2253e+00, -2.4758e+00,  ...,  2.4060e+00,\n",
       "           -2.4556e+00, -1.9858e+00],\n",
       "          ...,\n",
       "          [ 3.0366e+00, -3.1037e+00,  1.0182e+00,  ...,  1.7399e+00,\n",
       "           -1.6562e+00, -1.9943e+00],\n",
       "          [-1.8761e+00, -3.8648e+00, -1.5842e+00,  ...,  4.5803e-01,\n",
       "           -6.1348e-01, -3.6059e+00],\n",
       "          [-4.4316e+00, -2.4726e+00, -2.3310e+00,  ...,  2.9462e+00,\n",
       "           -1.9467e+00, -1.4365e+00]],\n",
       "\n",
       "         [[ 1.3574e-03, -6.1648e-03,  1.7492e-02,  ...,  1.3465e+00,\n",
       "           -3.6406e-01,  2.4415e-01],\n",
       "          [ 2.6255e+00, -2.5852e+00,  1.4561e+00,  ...,  6.2996e-01,\n",
       "            1.6505e+00, -1.1878e+00],\n",
       "          [ 2.9944e+00, -9.9862e-01,  3.4223e+00,  ..., -3.2510e-01,\n",
       "            1.2011e+00, -1.7021e+00],\n",
       "          ...,\n",
       "          [ 1.0396e+00,  1.2398e+00, -1.0999e+00,  ...,  6.6977e-01,\n",
       "            2.3695e+00, -1.1897e+00],\n",
       "          [ 4.2716e-01,  1.1204e-01, -2.0438e-01,  ..., -1.5007e-01,\n",
       "            1.3266e+00, -8.7804e-01],\n",
       "          [ 6.9940e-01,  8.3258e-01,  8.2928e-01,  ..., -4.5498e-01,\n",
       "            1.8527e+00, -9.6844e-01]],\n",
       "\n",
       "         [[ 9.4519e-03, -1.5944e-02, -2.4548e-02,  ...,  1.3063e+00,\n",
       "            2.7019e+00, -1.7912e+00],\n",
       "          [ 8.3109e-01, -7.1204e-01, -1.3431e+00,  ...,  2.4244e-01,\n",
       "           -1.4434e+00, -3.8265e-01],\n",
       "          [-3.0082e-01, -4.7927e-01, -1.3707e+00,  ..., -1.5047e+00,\n",
       "           -5.1735e-01,  1.9666e+00],\n",
       "          ...,\n",
       "          [ 1.0897e+00,  1.2307e+00, -5.2339e-01,  ...,  1.1045e-01,\n",
       "           -4.0511e-01, -2.1643e+00],\n",
       "          [ 7.9507e-02,  2.1735e-01,  1.0142e-01,  ..., -3.3522e-01,\n",
       "           -8.8154e-01,  5.9336e-02],\n",
       "          [-7.4346e-02,  1.6017e-01,  2.8630e-01,  ..., -7.4479e-01,\n",
       "           -1.2930e+00,  5.2776e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.8422e-01,  1.7773e-01,  2.1953e-02,  ...,  1.0377e+00,\n",
       "           -1.6470e+00,  1.0830e+00],\n",
       "          [-1.8428e+00,  7.7269e-01, -1.1385e+00,  ..., -2.4576e-01,\n",
       "            2.4269e-01, -4.4287e-01],\n",
       "          [-4.2811e-02, -1.1627e+00, -5.1321e-01,  ..., -9.5391e-01,\n",
       "           -2.7066e-01, -4.6039e-01],\n",
       "          ...,\n",
       "          [-1.0836e-01, -3.6720e-01, -3.0547e-01,  ..., -1.0557e+00,\n",
       "            5.0298e-02, -1.6083e+00],\n",
       "          [ 1.1542e-01, -2.9125e-01, -2.5436e-01,  ..., -1.3413e+00,\n",
       "            9.7223e-01, -3.8520e+00],\n",
       "          [ 2.3901e-01,  6.5231e-01, -2.3495e-01,  ..., -7.5376e-01,\n",
       "            1.0487e+00, -1.6851e+00]],\n",
       "\n",
       "         [[-2.6274e-03,  2.6224e-03, -3.0507e-02,  ...,  1.1223e+00,\n",
       "            1.1003e+00,  1.8951e-01],\n",
       "          [ 4.4489e+00,  3.8938e+00,  9.7504e-01,  ..., -4.6690e-01,\n",
       "            3.1727e-01, -3.3090e+00],\n",
       "          [ 3.5502e+00,  2.4087e+00,  7.4491e-01,  ..., -2.6790e+00,\n",
       "           -3.7838e-01, -2.1990e+00],\n",
       "          ...,\n",
       "          [ 3.6887e+00, -2.9487e-01,  2.7712e+00,  ..., -3.6303e+00,\n",
       "            1.5922e-01, -3.0475e+00],\n",
       "          [ 3.0434e-01, -4.7322e-01,  1.1890e+00,  ..., -2.1143e+00,\n",
       "            2.1861e-01,  4.2016e-01],\n",
       "          [ 4.6971e-01, -9.4080e-01,  1.1356e+00,  ..., -2.0729e+00,\n",
       "           -1.5513e-03, -6.8869e-02]],\n",
       "\n",
       "         [[-1.6762e-02,  9.5977e-02, -5.1725e-02,  ...,  1.3930e+00,\n",
       "            7.2637e-01, -1.7956e+00],\n",
       "          [-8.4035e-01,  2.7317e+00, -7.3138e-01,  ..., -3.2571e-01,\n",
       "           -1.1558e+00,  8.1857e-01],\n",
       "          [-2.0530e+00,  1.7472e+00, -9.1370e-01,  ..., -4.9639e-01,\n",
       "           -1.1968e-01,  1.0050e+00],\n",
       "          ...,\n",
       "          [ 5.5708e-01, -3.2559e-01, -1.1873e+00,  ..., -1.1326e+00,\n",
       "           -1.3731e+00,  8.0547e-01],\n",
       "          [ 8.2300e-02, -8.5662e-01,  6.5478e-01,  ...,  7.0430e-01,\n",
       "            4.0017e-01, -4.9915e-01],\n",
       "          [ 2.1401e-01, -4.1069e-01, -1.6572e+00,  ..., -3.6739e-01,\n",
       "            1.8396e+00,  1.4492e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.5795e-03,  1.3599e-02, -4.5031e-02,  ...,  3.4115e-04,\n",
       "           -2.8627e-03,  1.1434e-01],\n",
       "          [ 1.7313e-02, -2.3015e-02, -1.5094e-02,  ..., -1.2581e-01,\n",
       "            6.6844e-02, -4.7114e-02],\n",
       "          [ 3.6041e-02, -1.1734e-02, -4.2703e-03,  ..., -3.8731e-02,\n",
       "           -4.5584e-03,  1.3311e-02],\n",
       "          ...,\n",
       "          [ 1.4457e-02,  3.9454e-02,  5.2808e-02,  ..., -8.6758e-02,\n",
       "           -1.8254e-03,  7.0413e-02],\n",
       "          [-8.0746e-03,  2.8792e-02,  4.5648e-03,  ...,  1.7872e-02,\n",
       "           -4.4121e-02, -3.3829e-02],\n",
       "          [-2.6389e-02, -5.1847e-03,  3.4546e-02,  ...,  3.2550e-02,\n",
       "           -9.0703e-03,  1.0126e-03]],\n",
       "\n",
       "         [[ 4.5735e-04,  2.5293e-04, -4.7075e-05,  ..., -4.1791e-04,\n",
       "            5.8180e-05, -1.2571e-03],\n",
       "          [-8.4663e-02,  4.8954e-02, -3.1418e-02,  ..., -3.6163e-02,\n",
       "            6.0981e-02, -1.3284e-01],\n",
       "          [ 1.6796e-01,  5.0527e-02,  1.6985e-01,  ..., -1.6803e-02,\n",
       "           -1.6558e-02, -6.1578e-02],\n",
       "          ...,\n",
       "          [ 3.7561e-02,  1.8250e-01,  2.0727e-01,  ...,  1.6089e-01,\n",
       "           -5.3278e-02,  1.8658e-02],\n",
       "          [-5.8125e-04,  5.5240e-03, -9.5604e-03,  ...,  2.7582e-03,\n",
       "            7.9767e-03,  1.9451e-03],\n",
       "          [-3.6058e-03,  7.6267e-03,  3.5627e-03,  ..., -6.6536e-04,\n",
       "           -6.2435e-03,  9.2349e-03]],\n",
       "\n",
       "         [[-4.7917e-03, -3.6565e-03,  3.6445e-04,  ..., -3.5329e-04,\n",
       "           -6.5907e-04, -9.0835e-04],\n",
       "          [ 6.2352e-02,  7.7328e-02,  6.2370e-03,  ..., -7.5959e-02,\n",
       "           -4.6137e-03,  8.8519e-03],\n",
       "          [ 5.4777e-02,  8.3328e-02, -1.4637e-02,  ..., -3.3003e-02,\n",
       "           -2.7027e-02,  3.4971e-02],\n",
       "          ...,\n",
       "          [ 1.7605e-01, -6.7235e-02,  1.7233e-02,  ..., -6.1664e-02,\n",
       "           -1.7647e-02, -5.4523e-05],\n",
       "          [ 3.0211e-02,  3.2573e-02, -2.6589e-02,  ...,  1.2482e-02,\n",
       "            1.4175e-02,  2.8620e-03],\n",
       "          [-3.1806e-02,  4.5658e-02, -1.4811e-02,  ...,  9.7301e-03,\n",
       "            2.2862e-02,  5.3875e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.7790e-03,  6.0383e-04, -3.2364e-03,  ..., -5.6062e-04,\n",
       "           -1.4381e-04,  4.3967e-04],\n",
       "          [-3.7158e-03, -7.1261e-02, -3.9633e-02,  ...,  4.7294e-03,\n",
       "            6.5301e-03,  5.3481e-02],\n",
       "          [ 1.1382e-02, -8.6632e-03,  1.6766e-02,  ...,  5.7594e-02,\n",
       "            4.8978e-02, -3.5241e-02],\n",
       "          ...,\n",
       "          [ 2.5676e-02, -3.9249e-02,  2.9148e-02,  ...,  4.7297e-02,\n",
       "            3.1383e-03, -5.3304e-04],\n",
       "          [ 1.1637e-02,  1.8644e-02,  8.6058e-03,  ...,  3.4090e-02,\n",
       "           -1.0730e-02,  2.6018e-02],\n",
       "          [-2.0807e-02, -1.4515e-02,  7.2262e-02,  ..., -4.9397e-02,\n",
       "            3.2500e-02, -2.1658e-02]],\n",
       "\n",
       "         [[-5.6311e-04, -1.6512e-04,  4.0350e-03,  ...,  8.4027e-05,\n",
       "           -1.4644e-03, -8.6667e-04],\n",
       "          [ 1.7919e-01, -1.0852e-01, -1.6430e-01,  ...,  5.9286e-02,\n",
       "            8.9477e-02, -5.5855e-02],\n",
       "          [ 2.5675e-01,  1.0869e-03,  1.1531e-01,  ..., -3.6530e-01,\n",
       "            5.3507e-02, -5.6823e-02],\n",
       "          ...,\n",
       "          [-1.0139e-01, -1.2222e-01, -5.7939e-02,  ...,  6.2737e-02,\n",
       "            5.3934e-02, -1.1663e-01],\n",
       "          [ 6.6833e-03,  4.3920e-03, -1.6561e-02,  ..., -2.5399e-03,\n",
       "            5.3611e-03, -1.7816e-03],\n",
       "          [ 2.2379e-03,  5.6286e-03, -1.7198e-03,  ..., -2.7832e-03,\n",
       "            5.2145e-03,  9.7924e-03]],\n",
       "\n",
       "         [[ 5.6741e-03,  2.7614e-03,  1.6247e-03,  ...,  2.1563e-05,\n",
       "            5.5974e-04, -1.0744e-03],\n",
       "          [-5.8041e-02, -9.0072e-02,  1.1306e-01,  ...,  1.4870e-01,\n",
       "            1.0556e-01,  1.3944e-02],\n",
       "          [ 9.2414e-03,  1.5498e-01, -7.3742e-03,  ..., -2.2271e-01,\n",
       "           -3.9576e-02, -7.7866e-02],\n",
       "          ...,\n",
       "          [-7.0840e-02,  5.4475e-02,  1.0716e-03,  ...,  1.3027e-02,\n",
       "           -5.8265e-02,  4.1278e-02],\n",
       "          [ 7.2511e-02, -2.6733e-02, -1.2601e-02,  ...,  1.1854e-02,\n",
       "            9.6759e-02, -1.2748e-03],\n",
       "          [-2.5886e-02,  2.8877e-03, -3.7385e-02,  ..., -3.0737e-03,\n",
       "            1.4496e-02,  3.1701e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.2139e-03, -1.4249e-03,  1.5600e-03,  ..., -4.3569e-01,\n",
       "           -8.6812e-02,  1.1762e+00],\n",
       "          [ 1.9350e+00, -1.2447e+00, -1.7002e+00,  ..., -9.1995e-01,\n",
       "            8.1568e-01, -4.1878e+00],\n",
       "          [-5.7446e-01, -3.2907e-03, -2.9184e+00,  ..., -2.9764e+00,\n",
       "            1.1214e+00, -3.9793e+00],\n",
       "          ...,\n",
       "          [ 2.0956e+00,  2.2433e+00,  4.7005e-01,  ..., -5.1452e-01,\n",
       "            8.2334e-01, -3.2136e+00],\n",
       "          [ 1.1162e+00,  2.2306e+00, -7.0226e-01,  ..., -1.5233e+00,\n",
       "           -4.1618e-01, -3.8196e+00],\n",
       "          [-6.1993e-02,  2.5093e+00, -2.4052e+00,  ...,  2.5527e-01,\n",
       "            6.0980e-01, -4.8354e+00]],\n",
       "\n",
       "         [[ 8.0973e-04,  2.0151e-03, -1.4747e-03,  ...,  1.0658e+00,\n",
       "           -4.0840e-01, -1.1342e+00],\n",
       "          [ 2.4678e+00, -2.9559e+00, -1.2180e+00,  ..., -4.2290e+00,\n",
       "           -2.1023e+00,  1.6700e+00],\n",
       "          [-4.7544e+00, -4.9827e+00, -2.5075e+00,  ..., -2.6604e+00,\n",
       "           -1.6540e-01,  2.0637e+00],\n",
       "          ...,\n",
       "          [ 3.6940e+00, -4.2165e-01,  2.6126e+00,  ..., -4.7390e+00,\n",
       "            1.5746e-01,  2.2879e+00],\n",
       "          [-2.7123e-02, -3.4669e-01,  3.9702e-01,  ..., -2.9228e+00,\n",
       "           -1.4913e+00,  2.7002e+00],\n",
       "          [-2.3217e+00,  2.4137e+00, -5.5131e-01,  ..., -3.1998e+00,\n",
       "            4.8547e-01,  3.4051e+00]],\n",
       "\n",
       "         [[ 6.2403e-04,  2.8625e-04, -2.4305e-03,  ..., -3.4117e-01,\n",
       "           -2.1266e-01, -2.6674e-01],\n",
       "          [-1.4682e+00, -1.3298e+00, -1.1349e+00,  ...,  2.6811e-01,\n",
       "            1.6897e-01, -7.0562e-01],\n",
       "          [-1.1982e+00, -2.5901e+00, -2.1049e+00,  ..., -3.1137e-01,\n",
       "           -2.1811e-02,  4.1958e-01],\n",
       "          ...,\n",
       "          [-2.1265e+00, -1.2600e+00,  7.5228e-01,  ..., -7.1454e-01,\n",
       "           -4.6744e-02,  8.2946e-01],\n",
       "          [-7.3289e-01, -3.4448e-01, -8.7069e-01,  ..., -4.7391e-01,\n",
       "            4.3473e-01, -3.6716e-01],\n",
       "          [ 4.1637e-01,  1.3982e+00, -1.1761e+00,  ...,  6.3663e-01,\n",
       "           -6.0138e-04, -1.4146e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8379e-04, -1.4670e-04, -4.4258e-04,  ...,  7.4709e-01,\n",
       "            9.1630e-01,  5.2057e-01],\n",
       "          [ 1.3211e+00, -8.4074e-01,  2.6874e+00,  ...,  1.2986e+00,\n",
       "           -2.5056e+00,  1.2020e+00],\n",
       "          [-3.6381e+00,  2.1480e+00,  1.6759e+00,  ...,  1.2310e-01,\n",
       "           -5.2295e+00, -5.6533e+00],\n",
       "          ...,\n",
       "          [ 4.1685e+00,  4.6282e+00,  3.0609e+00,  ..., -6.1119e+00,\n",
       "           -9.2052e-01, -7.5015e-01],\n",
       "          [-8.7005e-01,  2.9821e+00,  3.1555e+00,  ..., -3.9852e+00,\n",
       "           -6.0769e+00, -1.5274e+00],\n",
       "          [-5.0009e+00,  1.1370e+00,  2.8016e+00,  ..., -2.5126e+00,\n",
       "           -7.7030e+00, -3.0900e+00]],\n",
       "\n",
       "         [[ 1.6152e-03, -3.5719e-03, -1.5815e-03,  ...,  1.8492e-02,\n",
       "            2.9929e-01, -7.6359e-01],\n",
       "          [ 3.9231e+00, -1.6081e+00,  3.0374e+00,  ...,  5.6990e-01,\n",
       "           -1.3954e+00,  3.4999e+00],\n",
       "          [-1.9194e+00, -2.8549e+00,  1.9067e+00,  ...,  3.8452e+00,\n",
       "           -1.2770e+00,  3.9601e+00],\n",
       "          ...,\n",
       "          [ 5.8686e+00, -1.8047e+00,  4.0314e+00,  ...,  8.0247e-01,\n",
       "            3.4043e-01,  4.4729e+00],\n",
       "          [ 2.0219e+00,  4.7770e-01,  3.4756e+00,  ...,  2.8171e+00,\n",
       "           -3.1843e-01,  4.0727e+00],\n",
       "          [-3.8745e+00,  1.6361e+00,  3.2627e+00,  ..., -1.5351e+00,\n",
       "           -1.9945e+00,  3.8323e+00]],\n",
       "\n",
       "         [[ 2.7436e-03, -1.3692e-04,  1.4191e-03,  ..., -2.7850e-01,\n",
       "            1.6209e-01, -1.1259e+00],\n",
       "          [ 1.5395e+00,  2.0338e+00,  5.4659e-01,  ..., -2.3014e-01,\n",
       "            2.9305e+00,  9.5570e-01],\n",
       "          [-8.9758e-01,  2.0579e+00,  6.3161e-01,  ...,  2.4794e-01,\n",
       "           -5.5953e-01,  1.2619e+00],\n",
       "          ...,\n",
       "          [ 1.5697e+00,  7.7222e-01,  1.1486e+00,  ..., -8.9094e-03,\n",
       "            2.3134e-01,  1.7078e+00],\n",
       "          [ 1.5785e-02,  2.7254e-01,  7.0113e-01,  ...,  6.4434e-02,\n",
       "            1.0584e+00,  1.9006e+00],\n",
       "          [ 7.5776e-02,  1.4098e-01,  1.6435e-01,  ...,  8.9771e-02,\n",
       "           -8.7711e-01,  2.8379e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 2.7196e-03,  7.0658e-04,  1.8582e-05,  ...,  1.2759e-03,\n",
       "           -1.0338e-03,  3.3517e-03],\n",
       "          [-1.4822e-01,  2.4638e-02, -1.1422e-02,  ..., -5.9676e-02,\n",
       "            1.0737e-01, -1.0539e-01],\n",
       "          [ 7.6334e-02,  1.1730e-01, -5.1219e-01,  ..., -1.1641e-01,\n",
       "           -2.1806e-02, -4.9997e-01],\n",
       "          ...,\n",
       "          [-2.9883e-02,  1.4177e-01,  1.7958e-01,  ..., -6.8812e-02,\n",
       "            5.2222e-02, -5.1628e-02],\n",
       "          [ 2.4902e-03, -1.5411e-01,  1.1618e-02,  ..., -1.8947e-01,\n",
       "            1.6407e-02, -9.2789e-02],\n",
       "          [ 7.3616e-02,  2.1354e-01, -1.5872e-01,  ..., -5.5358e-02,\n",
       "           -1.3317e-02,  5.8355e-02]],\n",
       "\n",
       "         [[ 1.6073e-03, -5.0960e-04,  2.6100e-04,  ...,  2.9433e-04,\n",
       "            1.0156e-03, -5.5970e-04],\n",
       "          [ 3.5075e-01, -2.2089e-01, -2.5835e-01,  ..., -2.2467e-02,\n",
       "            5.5238e-01, -1.8986e-01],\n",
       "          [ 4.0335e-02, -1.8493e-01,  1.6060e-01,  ..., -1.1311e-01,\n",
       "           -2.3526e-01,  7.8048e-02],\n",
       "          ...,\n",
       "          [ 2.4256e-02,  2.6725e-01,  2.5535e-01,  ...,  7.4325e-02,\n",
       "            2.0750e-01, -3.2845e-01],\n",
       "          [ 9.7183e-02, -1.7321e-02, -2.3598e-01,  ..., -1.4275e-01,\n",
       "            2.8268e-01, -3.0310e-01],\n",
       "          [-1.5848e-01,  4.2338e-02, -6.1075e-02,  ...,  2.4659e-02,\n",
       "           -5.1381e-02,  5.1545e-02]],\n",
       "\n",
       "         [[-2.5855e-03, -7.9806e-04, -1.4063e-04,  ...,  8.1533e-03,\n",
       "           -4.7586e-03, -1.7220e-01],\n",
       "          [-1.3528e-01,  1.6023e-01,  7.3915e-02,  ...,  3.9369e-02,\n",
       "           -1.9629e-01,  6.1934e-01],\n",
       "          [-1.6900e-01,  3.1524e-01,  3.4075e-01,  ...,  1.5213e-01,\n",
       "            2.3393e-01,  5.7784e-01],\n",
       "          ...,\n",
       "          [ 3.1979e-02,  1.4093e-01, -1.1385e-01,  ...,  4.2492e-01,\n",
       "           -1.5165e-02,  4.2743e-01],\n",
       "          [ 1.3022e-01, -1.8602e-01,  2.1446e-01,  ..., -1.1995e-01,\n",
       "           -1.9694e-01,  4.5872e-01],\n",
       "          [-1.4854e-01, -4.4869e-02,  2.0757e-01,  ..., -1.6127e-01,\n",
       "            9.9396e-02,  3.7362e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.9384e-04,  1.2597e-03, -2.2476e-02,  ...,  1.7617e-02,\n",
       "            4.6215e-03,  2.4348e-03],\n",
       "          [ 1.5204e-01,  5.0423e-03, -1.4772e-01,  ...,  1.6594e-01,\n",
       "            1.8725e-01,  1.1687e-02],\n",
       "          [-2.6947e-01, -1.1627e-01,  1.0855e-02,  ...,  6.6156e-03,\n",
       "            1.6410e-01, -8.3480e-02],\n",
       "          ...,\n",
       "          [-1.1989e-01,  2.1385e-01,  1.1867e-01,  ...,  7.8365e-02,\n",
       "            2.2902e-01,  3.5646e-02],\n",
       "          [ 1.7789e-01,  1.1039e-01,  2.5468e-01,  ...,  1.4081e-01,\n",
       "           -2.1657e-02,  2.3325e-02],\n",
       "          [ 1.1498e-02, -2.0019e-01,  1.6311e-01,  ..., -2.3550e-02,\n",
       "           -3.1248e-01,  3.4882e-02]],\n",
       "\n",
       "         [[-3.8862e-05, -2.6520e-04, -2.1426e-03,  ..., -1.2366e-05,\n",
       "           -4.6408e-06, -4.6186e-04],\n",
       "          [ 2.4942e-01,  8.8266e-02,  1.1127e-02,  ...,  2.0302e-02,\n",
       "           -6.1464e-03,  2.4295e-01],\n",
       "          [-2.1996e-01, -3.0095e-01,  9.1828e-02,  ...,  6.0150e-03,\n",
       "            1.4591e-01, -1.2568e-01],\n",
       "          ...,\n",
       "          [-2.6859e-02,  2.6737e-02, -1.4075e-01,  ...,  1.9062e-01,\n",
       "            3.0630e-02,  9.3282e-02],\n",
       "          [-2.8070e-02, -1.3150e-02,  1.3500e-01,  ...,  9.8959e-02,\n",
       "           -1.5513e-01, -1.3010e-01],\n",
       "          [-1.4448e-01,  6.6265e-02,  9.8009e-02,  ...,  2.4157e-02,\n",
       "           -4.9157e-02, -8.6480e-02]],\n",
       "\n",
       "         [[-4.7393e-03, -5.8355e-03, -5.3201e-03,  ..., -2.0790e-02,\n",
       "            5.7086e-03,  3.6834e-03],\n",
       "          [-1.4395e-02,  1.1665e-01,  2.1126e-01,  ..., -2.6482e-01,\n",
       "            1.4834e-01, -1.3069e-01],\n",
       "          [ 3.2205e-02,  5.8531e-03, -5.9879e-02,  ...,  2.1715e-01,\n",
       "            3.4708e-01,  9.0813e-02],\n",
       "          ...,\n",
       "          [ 1.3524e-01, -1.5185e-01, -1.1747e-01,  ...,  2.0198e-01,\n",
       "            2.5163e-02, -6.5386e-02],\n",
       "          [-1.0437e-01,  1.4891e-01, -2.8061e-01,  ...,  2.8690e-01,\n",
       "            1.1627e-01,  1.6215e-02],\n",
       "          [ 9.3323e-03,  1.0385e-01, -1.3858e-02,  ..., -1.1115e-01,\n",
       "            4.0250e-02,  4.9819e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-4.1174e-03,  4.1503e-05,  1.4348e-03,  ...,  5.3206e-02,\n",
       "           -1.8270e+00, -6.9581e-01],\n",
       "          [-1.6898e-01,  1.2190e+00, -1.6985e+00,  ...,  1.7298e+00,\n",
       "            5.2735e+00, -2.7237e-01],\n",
       "          [ 2.1927e+00,  1.6487e+00, -1.8605e+00,  ...,  5.2497e-01,\n",
       "            4.7107e+00, -2.5228e-02],\n",
       "          ...,\n",
       "          [-3.8536e-01,  1.6437e-01,  6.5969e-01,  ..., -3.2644e+00,\n",
       "            5.5233e+00, -9.7142e-01],\n",
       "          [-2.8361e-01, -2.2095e+00,  2.6057e+00,  ...,  8.0357e-01,\n",
       "            5.9478e+00, -2.2717e+00],\n",
       "          [ 2.4026e+00, -2.1392e+00,  6.7999e-01,  ...,  7.3199e-01,\n",
       "            5.1874e+00, -1.4208e+00]],\n",
       "\n",
       "         [[ 7.0349e-04,  2.0776e-03, -3.0347e-03,  ...,  4.8449e-01,\n",
       "           -3.7840e-01, -2.3032e-01],\n",
       "          [ 9.1438e-01,  2.2023e-01, -7.0767e-01,  ...,  8.4095e-01,\n",
       "            1.7913e-02, -8.3284e-01],\n",
       "          [ 2.5243e+00,  1.6882e+00, -1.2880e+00,  ...,  1.0455e+00,\n",
       "           -4.3630e-01, -1.5089e+00],\n",
       "          ...,\n",
       "          [ 1.5841e-01,  2.1439e+00,  1.1022e+00,  ...,  1.3461e-01,\n",
       "            1.1303e+00, -9.4240e-01],\n",
       "          [ 1.1981e+00,  1.9118e-01,  1.0794e+00,  ..., -3.0398e-01,\n",
       "            5.6740e-01,  1.2171e-01],\n",
       "          [ 1.8560e+00, -1.2038e+00, -6.1371e-02,  ...,  3.7729e-01,\n",
       "           -2.8157e-01, -6.9938e-01]],\n",
       "\n",
       "         [[ 2.6750e-04,  2.0595e-03,  1.3705e-03,  ...,  2.1106e-01,\n",
       "           -1.6021e+00, -1.3887e+00],\n",
       "          [-6.1763e-01, -2.3037e+00, -1.0033e-01,  ...,  1.5255e+00,\n",
       "            2.6712e+00,  5.6820e+00],\n",
       "          [-4.6111e+00, -3.5855e+00, -1.7061e+00,  ..., -9.5943e-01,\n",
       "            1.8547e+00,  4.4509e+00],\n",
       "          ...,\n",
       "          [-7.2984e-03, -1.7061e+00,  1.3670e+00,  ..., -4.2427e-01,\n",
       "            3.6208e+00,  3.1324e+00],\n",
       "          [-1.2064e+00,  1.3491e+00,  3.6971e+00,  ...,  1.8968e+00,\n",
       "            3.0823e+00,  2.9499e+00],\n",
       "          [-3.3547e+00,  1.9572e+00,  1.3832e+00,  ...,  7.2230e-01,\n",
       "            3.4159e+00,  4.0136e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2378e-03,  3.7466e-04, -1.6616e-03,  ..., -1.1842e-01,\n",
       "           -4.7553e-01,  1.2336e+00],\n",
       "          [-2.4192e+00, -2.2660e+00,  3.2593e+00,  ...,  4.6178e-01,\n",
       "           -1.1090e+00, -4.1385e+00],\n",
       "          [ 6.5790e-01, -1.0711e+00,  2.2892e+00,  ...,  5.2442e-02,\n",
       "            3.7685e+00, -3.6615e+00],\n",
       "          ...,\n",
       "          [-2.5692e+00,  8.3881e-02,  2.8556e+00,  ..., -1.6767e+00,\n",
       "            8.7459e-01, -3.9755e+00],\n",
       "          [ 1.4653e+00,  1.3157e+00,  1.6962e+00,  ..., -2.3464e+00,\n",
       "            2.8953e+00, -4.5117e+00],\n",
       "          [ 3.3371e+00,  1.8177e+00,  2.6315e+00,  ...,  5.9933e-01,\n",
       "            3.2509e+00, -5.4460e+00]],\n",
       "\n",
       "         [[-2.9876e-04,  1.2478e-03,  1.8342e-03,  ..., -6.2026e-01,\n",
       "            1.8709e-01, -1.1013e-01],\n",
       "          [ 2.4638e-01, -2.1251e+00, -1.4692e+00,  ...,  4.7925e-01,\n",
       "            2.8608e-02, -1.1502e-01],\n",
       "          [ 1.1728e+00, -1.4827e+00, -5.5713e-01,  ...,  2.8622e+00,\n",
       "            1.1224e+00, -1.6787e-02],\n",
       "          ...,\n",
       "          [ 5.7701e-01,  4.4340e-01, -2.7902e+00,  ...,  1.3348e+00,\n",
       "            1.9549e+00,  1.3491e+00],\n",
       "          [ 1.9225e+00,  1.9225e+00, -1.1285e+00,  ...,  2.1314e+00,\n",
       "            1.8610e+00,  1.0189e+00],\n",
       "          [ 8.5265e-01,  1.2903e+00, -1.5050e+00,  ...,  3.0361e+00,\n",
       "            1.7882e+00, -5.6405e-01]],\n",
       "\n",
       "         [[ 1.0576e-04,  5.4620e-04, -6.8324e-04,  ...,  2.0276e-01,\n",
       "           -1.9648e-01,  2.1197e-01],\n",
       "          [ 1.8654e+00,  2.6155e+00,  1.9676e+00,  ...,  7.4825e-01,\n",
       "           -8.3580e-01,  8.0264e-01],\n",
       "          [ 2.9937e+00,  3.2986e+00,  3.2237e+00,  ...,  6.6964e-02,\n",
       "            3.9378e-01,  9.0476e-02],\n",
       "          ...,\n",
       "          [ 1.2848e+00,  1.0904e+00, -1.2059e+00,  ..., -7.1829e-01,\n",
       "            1.6191e+00, -3.4342e+00],\n",
       "          [ 1.0090e+00,  8.0775e-01,  1.1006e+00,  ..., -7.3169e-01,\n",
       "            2.1744e-01, -2.4110e+00],\n",
       "          [ 1.3514e+00, -9.7394e-01,  2.1294e+00,  ..., -2.7201e+00,\n",
       "           -3.9257e-01, -3.7681e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.1196e-03, -1.8662e-03,  2.8597e-03,  ...,  2.8360e-03,\n",
       "           -2.3091e-03, -1.4661e-03],\n",
       "          [ 3.2262e-01, -2.2603e-01, -9.4245e-02,  ..., -2.8917e-02,\n",
       "           -1.9163e-01,  1.3795e-01],\n",
       "          [ 2.6296e-01, -1.5872e-01, -2.3494e-02,  ..., -1.8963e-01,\n",
       "           -2.0513e-02, -1.1839e-02],\n",
       "          ...,\n",
       "          [-5.7209e-02, -1.3115e-01,  2.7145e-01,  ...,  1.8429e-01,\n",
       "           -5.8320e-02, -3.6054e-01],\n",
       "          [-1.9315e-01, -3.4344e-02, -2.2140e-01,  ...,  1.6748e-01,\n",
       "           -2.4809e-01, -2.5304e-01],\n",
       "          [ 1.1066e-01, -2.8414e-01, -1.3110e-01,  ...,  4.8236e-01,\n",
       "           -1.0611e-01,  1.0280e-01]],\n",
       "\n",
       "         [[-3.3957e-02,  1.2836e-01, -3.0450e-03,  ...,  1.3015e-02,\n",
       "            5.5600e-03, -3.5589e-04],\n",
       "          [ 5.8394e-01,  1.4470e-01,  3.1701e-02,  ..., -9.1642e-02,\n",
       "           -8.9573e-02,  1.0450e-01],\n",
       "          [ 8.1863e-02,  1.1457e-01, -1.3582e-01,  ..., -3.2781e-01,\n",
       "           -2.7944e-02,  2.2985e-01],\n",
       "          ...,\n",
       "          [-1.9902e-01, -1.5077e-01,  1.0673e-01,  ..., -8.8718e-02,\n",
       "           -3.9603e-02, -8.2434e-02],\n",
       "          [-3.4212e-01, -1.4630e-01, -1.1417e-01,  ..., -3.1293e-01,\n",
       "            1.6145e-01,  1.4101e-01],\n",
       "          [-4.3969e-01,  4.4336e-02, -2.1096e-01,  ..., -1.8586e-01,\n",
       "            1.9330e-01,  4.7110e-01]],\n",
       "\n",
       "         [[ 1.5269e-03,  1.2616e-03, -1.1200e-03,  ..., -2.5942e-04,\n",
       "            6.7010e-04, -2.5245e-03],\n",
       "          [-1.7091e-01, -3.7150e-01,  6.7583e-02,  ...,  3.0606e-01,\n",
       "            1.1082e-01, -1.6427e-02],\n",
       "          [ 6.6383e-03, -2.9611e-01,  2.9256e-01,  ..., -1.1365e-01,\n",
       "            2.3392e-01, -3.1319e-03],\n",
       "          ...,\n",
       "          [ 2.8579e-02, -3.2650e-01,  2.3644e-01,  ...,  2.9950e-01,\n",
       "           -1.2733e-01,  1.1690e-01],\n",
       "          [-1.4211e-01, -4.2242e-01, -2.8164e-03,  ...,  5.8962e-01,\n",
       "           -3.0515e-02, -1.5637e-02],\n",
       "          [-1.1026e-01, -1.2336e-02,  1.1528e-03,  ..., -4.2415e-02,\n",
       "            2.6984e-02, -1.0472e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.3784e-03,  3.2488e-03,  2.2171e-03,  ..., -2.7324e-01,\n",
       "            2.4593e-03, -1.6719e-03],\n",
       "          [-8.6568e-02, -6.0661e-01,  2.6522e-02,  ...,  8.8723e-01,\n",
       "            2.7798e-01,  1.9581e-01],\n",
       "          [-3.1756e-01, -4.3440e-01, -2.4315e-01,  ...,  1.1719e+00,\n",
       "            2.9577e-01,  5.2982e-02],\n",
       "          ...,\n",
       "          [-4.2080e-01, -2.2729e-01,  2.0962e-01,  ...,  7.6506e-01,\n",
       "           -4.4889e-01,  2.7277e-01],\n",
       "          [-8.1783e-02, -2.2788e-02, -9.5408e-02,  ...,  8.2125e-01,\n",
       "            2.6837e-02,  3.9482e-01],\n",
       "          [ 4.1958e-02,  3.8166e-04,  8.0668e-02,  ...,  5.4045e-01,\n",
       "           -1.2770e-01,  3.8646e-01]],\n",
       "\n",
       "         [[ 1.2656e-03,  1.2224e-03, -7.9985e-04,  ..., -1.9534e-03,\n",
       "            6.8800e-04,  8.1042e-04],\n",
       "          [ 9.1415e-02, -1.4297e-01,  2.6563e-01,  ..., -5.5488e-02,\n",
       "           -8.5673e-02,  1.3150e-01],\n",
       "          [-1.1199e-01, -3.1006e-02, -2.1747e-02,  ..., -3.3355e-02,\n",
       "           -3.1546e-01, -2.8080e-01],\n",
       "          ...,\n",
       "          [ 1.0735e-01,  1.0785e-01,  2.4058e-01,  ...,  1.3142e-01,\n",
       "            2.8496e-02,  1.3313e-01],\n",
       "          [-5.2454e-01, -4.0436e-02, -1.2627e-02,  ..., -2.6360e-01,\n",
       "           -1.7450e-01, -1.3110e-01],\n",
       "          [-1.5916e-01, -1.8747e-01, -2.6672e-02,  ..., -1.9696e-01,\n",
       "           -3.2432e-01, -1.5890e-01]],\n",
       "\n",
       "         [[-4.3257e-03,  1.4322e-02, -1.4786e-03,  ...,  8.7591e-04,\n",
       "           -1.4720e-03,  4.1253e-04],\n",
       "          [ 1.4322e-01, -5.4519e-01, -1.8442e-01,  ..., -1.8586e-01,\n",
       "           -3.5733e-01, -1.8659e-01],\n",
       "          [-3.1124e-02, -2.5233e-02, -6.1809e-02,  ..., -1.2340e-01,\n",
       "           -1.8219e-01, -6.9674e-02],\n",
       "          ...,\n",
       "          [-1.9649e-01, -3.9642e-01, -1.0305e-01,  ...,  2.1873e-02,\n",
       "            1.7903e-01, -3.5318e-02],\n",
       "          [-4.9445e-01,  1.4739e-01, -1.7839e-01,  ..., -2.6993e-01,\n",
       "           -7.1830e-02, -2.0136e-01],\n",
       "          [-2.0593e-01,  2.2762e-01, -3.8844e-02,  ..., -3.1392e-01,\n",
       "            1.7693e-01, -4.2432e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.9453e-03, -3.8040e-03,  1.4691e-03,  ..., -1.1004e+00,\n",
       "            1.2234e+00,  9.8771e-01],\n",
       "          [ 1.6318e+00, -6.8961e-01, -2.1278e+00,  ..., -1.1181e-01,\n",
       "            2.3670e+00, -1.0803e-01],\n",
       "          [ 1.2202e+00,  2.7760e-01, -1.5307e+00,  ...,  8.8788e-01,\n",
       "            2.0161e+00, -9.0944e-01],\n",
       "          ...,\n",
       "          [ 1.2310e+00,  1.5444e+00, -6.4527e-01,  ...,  1.0866e+00,\n",
       "            8.6503e-01, -1.2665e+00],\n",
       "          [ 7.5285e-01,  7.0079e-01, -8.6680e-02,  ...,  2.0569e+00,\n",
       "            2.7102e+00, -2.6819e+00],\n",
       "          [ 5.2154e-01,  1.0610e+00, -7.9744e-01,  ..., -5.3963e-03,\n",
       "            2.4472e+00, -1.6667e+00]],\n",
       "\n",
       "         [[ 1.5889e-03,  3.9554e-04,  2.8663e-03,  ..., -1.4775e-01,\n",
       "            6.3642e-01,  2.1692e-01],\n",
       "          [ 4.1342e+00,  2.8232e+00,  1.9378e+00,  ..., -1.7626e+00,\n",
       "           -4.2689e-03, -7.1162e-01],\n",
       "          [ 1.8407e+00,  1.1100e+00,  2.5713e+00,  ...,  3.6619e-02,\n",
       "           -6.6862e-01,  1.2570e+00],\n",
       "          ...,\n",
       "          [ 3.3960e+00, -3.5151e+00, -4.2308e-01,  ...,  1.1345e+00,\n",
       "            7.4708e-01,  2.9205e-02],\n",
       "          [ 1.8216e+00, -3.9968e+00,  1.9169e+00,  ...,  2.1235e+00,\n",
       "           -6.7755e-01,  2.1441e-01],\n",
       "          [ 9.7563e-01, -4.0367e+00,  3.2101e+00,  ...,  1.3520e-01,\n",
       "           -7.4815e-01, -4.8175e-02]],\n",
       "\n",
       "         [[ 3.9505e-03,  1.0595e-03, -1.0237e-03,  ..., -1.6442e-01,\n",
       "            2.0776e+00, -1.9068e+00],\n",
       "          [ 1.3268e+00,  1.8524e+00, -1.5398e+00,  ..., -3.7230e-01,\n",
       "           -4.7488e+00,  5.8246e+00],\n",
       "          [ 4.0053e+00,  2.2532e+00, -2.6253e+00,  ..., -6.7418e-01,\n",
       "           -4.9539e+00,  5.2955e+00],\n",
       "          ...,\n",
       "          [ 6.0000e-01,  5.9818e-01,  2.5092e+00,  ...,  1.0158e+00,\n",
       "           -5.6264e+00,  5.4800e+00],\n",
       "          [ 3.1782e+00,  6.0597e-01, -1.6147e+00,  ...,  5.4982e-01,\n",
       "           -5.4283e+00,  5.2760e+00],\n",
       "          [ 1.4721e+00, -9.4009e-01, -2.2713e+00,  ..., -1.8931e+00,\n",
       "           -5.7266e+00,  5.4748e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.1509e-03, -1.9047e-04,  1.7231e-03,  ..., -4.8710e-01,\n",
       "           -1.2689e+00,  2.9183e-01],\n",
       "          [ 4.9067e-01, -1.3598e+00,  1.8340e+00,  ..., -1.6097e+00,\n",
       "            1.3552e-01,  5.1112e-01],\n",
       "          [-1.4252e+00,  1.4761e-01,  1.2360e+00,  ..., -5.0897e-01,\n",
       "            2.2680e-02,  7.5436e-01],\n",
       "          ...,\n",
       "          [ 7.3535e-01,  1.0994e+00,  1.4287e+00,  ..., -8.2084e-01,\n",
       "           -9.6603e-02, -9.1192e-01],\n",
       "          [-1.1681e+00, -2.0528e-01,  1.4678e+00,  ...,  1.0819e-01,\n",
       "           -6.3144e-01, -7.1238e-01],\n",
       "          [-2.6062e-01,  3.6380e-01,  7.7597e-01,  ...,  8.0581e-01,\n",
       "            3.0968e-01, -1.8892e-01]],\n",
       "\n",
       "         [[-9.2308e-03, -1.4914e-03,  1.9837e-03,  ...,  1.3524e+00,\n",
       "           -1.2361e+00,  1.8967e+00],\n",
       "          [-1.7693e+00, -9.7247e-01, -2.2790e+00,  ..., -3.1131e+00,\n",
       "           -1.6311e+00, -3.6196e+00],\n",
       "          [ 9.8519e-01, -3.6455e-02, -1.1450e+00,  ..., -1.4824e+00,\n",
       "            6.4963e-01, -3.1894e+00],\n",
       "          ...,\n",
       "          [-2.0549e+00,  7.0196e-01, -2.4885e+00,  ..., -1.2717e+00,\n",
       "           -5.4227e-01, -3.6287e+00],\n",
       "          [ 1.9945e-01,  1.4207e+00, -1.6043e+00,  ..., -2.5608e-01,\n",
       "           -1.3801e+00, -2.9348e+00],\n",
       "          [ 1.0101e+00,  9.6288e-01, -1.3074e+00,  ..., -1.4542e+00,\n",
       "           -1.1000e+00, -3.2821e+00]],\n",
       "\n",
       "         [[-4.6712e-04,  3.6345e-03,  1.6733e-04,  ..., -7.2209e-01,\n",
       "           -2.5810e+00,  1.2878e+00],\n",
       "          [ 2.6760e-01,  1.2302e+00,  2.2325e+00,  ..., -6.6344e-01,\n",
       "            4.6323e+00,  3.9167e-01],\n",
       "          [-9.5119e-01,  2.0008e+00,  1.3108e+00,  ..., -7.9088e-01,\n",
       "            4.8834e+00,  2.9518e-01],\n",
       "          ...,\n",
       "          [ 1.0811e+00,  7.7164e-01, -3.2066e-01,  ..., -7.3684e-01,\n",
       "            4.3968e+00, -2.6178e-01],\n",
       "          [-7.6490e-01, -2.9009e-01,  1.0811e+00,  ..., -1.9990e-01,\n",
       "            5.2265e+00, -1.8956e+00],\n",
       "          [ 2.6880e-01, -5.6668e-01,  2.3295e+00,  ..., -8.3603e-01,\n",
       "            6.2926e+00, -1.3043e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.7562e-03, -5.1271e-03, -3.6867e-03,  ..., -1.3612e-03,\n",
       "           -1.8755e-03, -1.1279e-03],\n",
       "          [-7.9866e-02,  3.5973e-01,  1.0530e-01,  ..., -1.8539e-01,\n",
       "            1.8003e-01, -6.2178e-02],\n",
       "          [ 2.0673e-01,  4.3161e-02,  1.1069e-01,  ..., -2.5179e-01,\n",
       "           -2.9151e-01, -2.5007e-02],\n",
       "          ...,\n",
       "          [ 2.8165e-01,  3.9006e-01,  2.8758e-01,  ...,  3.6667e-01,\n",
       "            1.5508e-01, -6.3560e-02],\n",
       "          [-7.8271e-02,  5.8674e-01,  2.3394e-01,  ..., -3.1146e-01,\n",
       "           -5.4926e-01, -1.1890e-01],\n",
       "          [-5.7317e-02,  5.4877e-02, -4.4915e-02,  ..., -2.3327e-01,\n",
       "           -1.7791e-01,  3.9392e-01]],\n",
       "\n",
       "         [[ 2.0795e-03,  4.5254e-04,  1.4042e-04,  ...,  1.4382e-04,\n",
       "            3.7619e-04,  4.2393e-03],\n",
       "          [-1.6333e-01, -2.7382e-02, -7.8788e-02,  ..., -3.3833e-02,\n",
       "           -5.9220e-02, -1.6222e-01],\n",
       "          [-8.2058e-03,  4.2344e-01, -1.7369e-01,  ...,  3.1581e-01,\n",
       "           -6.2313e-02,  2.4205e-01],\n",
       "          ...,\n",
       "          [-5.7915e-02, -1.1096e-01,  2.4418e-03,  ..., -7.8849e-03,\n",
       "            1.0390e-01, -6.8880e-01],\n",
       "          [-3.6437e-01, -2.8349e-01,  1.2994e-01,  ..., -3.0461e-02,\n",
       "           -7.9728e-02, -5.7546e-01],\n",
       "          [-3.9898e-01, -4.8312e-02, -3.6076e-01,  ...,  2.9813e-01,\n",
       "           -8.5960e-01,  2.2225e-02]],\n",
       "\n",
       "         [[ 6.1266e-03, -1.9293e-03,  2.4691e-03,  ...,  1.2258e-03,\n",
       "           -8.2259e-03, -3.4531e-02],\n",
       "          [-9.6650e-02, -2.6994e-01,  8.1638e-03,  ..., -3.8557e-03,\n",
       "           -1.0273e-01,  5.2252e-01],\n",
       "          [-2.4010e-01, -2.1363e-01,  6.2668e-02,  ...,  1.8683e-01,\n",
       "            1.3869e-01,  3.4089e-01],\n",
       "          ...,\n",
       "          [ 3.1074e-02,  4.4355e-01, -9.1642e-02,  ...,  1.7924e-01,\n",
       "            7.6542e-02,  4.4310e-01],\n",
       "          [-3.1301e-03,  2.9274e-01,  3.0913e-02,  ..., -9.1923e-02,\n",
       "            2.1046e-01,  4.1656e-01],\n",
       "          [ 2.3875e-01,  3.2895e-01,  9.0669e-03,  ...,  1.6669e-01,\n",
       "            4.2056e-02,  1.9116e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.5370e-03,  4.4117e-03,  1.2383e-03,  ...,  5.3733e-04,\n",
       "            4.4476e-03,  7.7095e-03],\n",
       "          [-1.9039e-01,  3.8238e-02,  3.1399e-01,  ..., -3.7655e-02,\n",
       "           -5.1129e-02, -3.7690e-01],\n",
       "          [ 1.1005e-01,  3.9932e-01, -9.0883e-02,  ...,  2.5508e-01,\n",
       "           -2.9192e-01,  4.0088e-01],\n",
       "          ...,\n",
       "          [ 1.1542e-01,  5.7335e-01, -7.3958e-02,  ..., -1.6883e-01,\n",
       "           -2.0223e-01,  1.3579e-01],\n",
       "          [ 3.8652e-01,  3.2822e-01,  1.7755e-02,  ..., -5.5397e-01,\n",
       "            8.4716e-02,  1.1115e-01],\n",
       "          [-9.7240e-03, -2.4569e-01,  3.0256e-02,  ..., -2.2018e-01,\n",
       "           -1.6377e-01,  1.7013e-01]],\n",
       "\n",
       "         [[-4.1426e-03, -2.0476e-03, -3.3510e-03,  ..., -8.1401e-04,\n",
       "           -3.0524e-04, -2.3701e-03],\n",
       "          [ 7.2717e-02,  1.6357e-01,  1.8396e-01,  ..., -1.3850e-01,\n",
       "            2.4940e-02,  3.2483e-01],\n",
       "          [-6.0747e-02, -4.4769e-02,  1.0447e-01,  ...,  4.6575e-02,\n",
       "            8.0708e-02,  3.8409e-01],\n",
       "          ...,\n",
       "          [-3.9456e-02, -4.1144e-01, -1.2163e-01,  ...,  5.5659e-02,\n",
       "           -2.1899e-01, -1.7347e-01],\n",
       "          [-2.2774e-01, -3.1413e-01, -1.8718e-01,  ...,  3.2784e-02,\n",
       "           -2.8987e-01, -2.2710e-01],\n",
       "          [-3.3812e-01, -9.9666e-02, -3.4564e-02,  ..., -4.0334e-01,\n",
       "           -1.9173e-01, -1.1624e-01]],\n",
       "\n",
       "         [[ 1.1125e-03,  4.4045e-04, -1.5629e-03,  ..., -6.2340e-03,\n",
       "           -7.9219e-03, -3.2362e-03],\n",
       "          [ 3.1448e-01,  1.1712e-01,  8.2244e-02,  ...,  3.0254e-01,\n",
       "            3.0205e-01,  8.1697e-02],\n",
       "          [ 1.4452e-01, -3.0471e-01, -2.0692e-01,  ..., -4.1187e-01,\n",
       "            3.4181e-01,  3.4247e-01],\n",
       "          ...,\n",
       "          [-2.6594e-01, -3.4904e-01, -2.4928e-01,  ...,  1.4539e-01,\n",
       "           -2.1022e-01,  2.2571e-01],\n",
       "          [-2.3774e-01, -1.8471e-01, -3.7346e-01,  ..., -1.6864e-01,\n",
       "           -1.3781e-01, -3.0406e-01],\n",
       "          [ 1.3748e-01,  1.5909e-01, -3.0046e-01,  ...,  5.0859e-02,\n",
       "           -2.5475e-01, -7.2511e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.8096e-03, -5.9408e-04,  8.7066e-04,  ..., -2.3239e-01,\n",
       "            1.2847e+00, -4.9964e-01],\n",
       "          [-1.9681e+00, -1.0940e+00,  9.2597e-01,  ..., -5.8304e-01,\n",
       "            1.5861e-01, -3.4597e+00],\n",
       "          [ 9.8407e-01, -8.8456e-01,  4.9839e-01,  ...,  9.3391e-01,\n",
       "            7.2835e-01, -2.5982e+00],\n",
       "          ...,\n",
       "          [-7.0275e-01, -1.1211e+00,  1.9235e+00,  ...,  5.9392e-01,\n",
       "           -5.7387e-01, -2.7428e+00],\n",
       "          [ 2.4060e-01, -2.0430e-01, -1.0704e-01,  ...,  1.4806e+00,\n",
       "            1.8305e-01, -2.4935e+00],\n",
       "          [ 7.8031e-01,  9.3572e-01,  5.8861e-01,  ...,  1.1990e+00,\n",
       "            1.4091e-01, -3.1236e+00]],\n",
       "\n",
       "         [[-1.0908e-03, -3.8025e-04, -1.0915e-03,  ..., -1.0192e+00,\n",
       "           -1.0701e+00,  7.8021e-01],\n",
       "          [-1.8542e+00,  1.9979e+00, -1.5720e+00,  ..., -4.4806e-01,\n",
       "           -1.9013e+00, -3.7652e-01],\n",
       "          [ 1.0315e+00,  1.8540e-01, -1.3258e+00,  ..., -1.0771e+00,\n",
       "           -2.2555e+00,  7.6230e-01],\n",
       "          ...,\n",
       "          [-1.5094e+00, -1.0542e+00, -2.9726e+00,  ..., -1.5467e+00,\n",
       "           -5.7304e-01, -1.8861e+00],\n",
       "          [ 1.5543e+00, -1.5173e+00, -1.0370e-01,  ..., -2.2736e+00,\n",
       "            6.8727e-01, -1.1515e+00],\n",
       "          [ 1.5659e-01, -1.6302e+00, -1.1878e+00,  ..., -2.0102e+00,\n",
       "           -2.2700e-01, -1.8899e+00]],\n",
       "\n",
       "         [[ 1.5384e-03, -1.2727e-04, -2.5947e-04,  ...,  1.2032e+00,\n",
       "           -1.8105e+00,  8.7924e-01],\n",
       "          [ 1.8520e+00, -6.5754e-01, -3.1404e+00,  ...,  1.1420e+00,\n",
       "            3.7438e+00,  6.4151e-01],\n",
       "          [-2.0628e+00,  3.4772e-01, -2.3704e+00,  ...,  9.7781e-01,\n",
       "            1.6516e+00,  1.2433e+00],\n",
       "          ...,\n",
       "          [ 8.0852e-01,  1.6230e+00, -2.0363e+00,  ...,  6.2013e-01,\n",
       "            1.6745e+00,  3.9909e-01],\n",
       "          [ 3.1150e-01,  1.2801e+00, -1.6455e+00,  ...,  1.4911e+00,\n",
       "            2.1259e+00, -6.8806e-02],\n",
       "          [-2.5123e+00,  2.6884e-01, -1.9976e+00,  ...,  1.9745e-01,\n",
       "            2.6011e+00,  5.6338e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.6746e-03, -4.9224e-03,  1.4135e-04,  ..., -1.3594e-01,\n",
       "            4.5485e-01,  1.5846e+00],\n",
       "          [ 1.9195e+00, -1.2204e+00,  2.5724e+00,  ...,  2.2241e-01,\n",
       "            2.8363e-01, -1.7222e+00],\n",
       "          [-6.8182e-02,  1.5134e-01,  1.0746e+00,  ...,  8.5486e-01,\n",
       "            1.3369e+00, -2.2500e+00],\n",
       "          ...,\n",
       "          [ 1.0728e+00,  7.9856e-02,  1.1710e+00,  ..., -5.8481e-01,\n",
       "            1.6084e+00, -1.2569e+00],\n",
       "          [ 4.1704e-01,  2.1101e+00,  1.1805e+00,  ..., -2.5634e+00,\n",
       "            3.7581e+00, -2.0141e+00],\n",
       "          [ 5.7194e-01,  1.0191e+00,  2.1476e+00,  ..., -1.0825e+00,\n",
       "            1.6082e+00, -4.4963e-01]],\n",
       "\n",
       "         [[ 1.4500e-03, -5.6532e-04,  3.3049e-04,  ...,  2.8327e-01,\n",
       "            9.9080e-02, -3.0731e-01],\n",
       "          [ 1.6726e+00,  1.3004e+00, -1.7066e-01,  ...,  6.4653e-01,\n",
       "            9.9902e-01,  2.8224e-01],\n",
       "          [-7.9869e-01,  1.1799e+00,  1.4184e+00,  ..., -7.7405e-01,\n",
       "           -1.2405e-01,  5.9166e-02],\n",
       "          ...,\n",
       "          [ 2.2383e+00,  1.2868e-01, -1.1441e+00,  ...,  1.4381e+00,\n",
       "            1.4693e+00, -4.2023e-02],\n",
       "          [-4.8397e-01, -1.9858e+00, -7.9835e-01,  ...,  2.4954e+00,\n",
       "            8.8546e-01, -1.4753e+00],\n",
       "          [-1.7271e+00, -7.2482e-01,  1.8807e-01,  ...,  7.0038e-01,\n",
       "           -6.8273e-02, -1.3770e+00]],\n",
       "\n",
       "         [[-3.8714e-04,  1.0699e-03,  8.3987e-04,  ..., -8.0484e-01,\n",
       "            1.1465e+00,  2.1343e-01],\n",
       "          [-1.1565e+00,  2.0879e+00, -1.3423e+00,  ...,  2.2590e-01,\n",
       "            1.4840e+00,  3.9817e+00],\n",
       "          [ 2.0894e+00,  1.9383e+00, -1.8287e+00,  ..., -5.1532e-01,\n",
       "           -3.4085e-02,  2.8218e+00],\n",
       "          ...,\n",
       "          [-3.7550e-01,  4.8641e-01,  6.4713e-03,  ..., -7.9177e-01,\n",
       "            7.4438e-02,  7.2024e-01],\n",
       "          [ 1.1181e+00, -1.6471e-01, -1.6672e-01,  ..., -1.1397e+00,\n",
       "            7.6198e-01,  2.4575e+00],\n",
       "          [ 2.1025e+00, -4.6311e-01, -1.0505e+00,  ..., -4.4995e-01,\n",
       "            1.3498e+00,  2.0556e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-2.3341e-03, -2.9577e-01, -1.3293e-03,  ...,  4.2696e-04,\n",
       "            2.3120e-03,  3.7684e-04],\n",
       "          [-3.2071e-01,  1.0692e+00,  2.1968e-01,  ...,  3.0257e-01,\n",
       "            1.6443e-01, -1.8645e-02],\n",
       "          [ 1.4895e-01,  8.8629e-01, -2.1787e-01,  ...,  7.5691e-01,\n",
       "            1.2188e-01,  1.0903e-01],\n",
       "          ...,\n",
       "          [ 1.0691e-01,  3.6319e-01, -6.0664e-02,  ...,  1.7007e-02,\n",
       "            1.4335e-01, -5.8612e-02],\n",
       "          [ 2.1113e-01,  3.4163e-01,  1.2688e-01,  ..., -3.8087e-01,\n",
       "           -2.3089e-01,  1.1490e-01],\n",
       "          [-1.0503e-01,  5.7488e-01,  2.3695e-02,  ...,  1.1602e-01,\n",
       "            1.8937e-01,  2.8972e-01]],\n",
       "\n",
       "         [[ 2.4586e-03, -1.4497e-03, -6.1702e-03,  ..., -2.4595e-03,\n",
       "            4.0798e-03, -5.4955e-03],\n",
       "          [ 2.0171e-01, -2.2790e-01, -7.2871e-01,  ...,  2.5459e-01,\n",
       "           -4.9538e-02,  1.2849e-01],\n",
       "          [ 5.8069e-01, -1.2718e-01, -4.5036e-01,  ...,  2.6492e-01,\n",
       "           -2.0099e-01, -2.9507e-01],\n",
       "          ...,\n",
       "          [-3.6818e-01,  6.6487e-01,  3.4864e-02,  ...,  2.5830e-01,\n",
       "           -1.1592e+00,  2.9964e-01],\n",
       "          [ 3.7546e-03,  2.4208e-01, -1.0185e-01,  ...,  2.1218e-01,\n",
       "           -6.8113e-01,  9.7969e-02],\n",
       "          [ 3.7971e-02,  3.2882e-01, -7.2274e-02,  ...,  1.9961e-01,\n",
       "           -6.5500e-01,  2.6436e-01]],\n",
       "\n",
       "         [[-1.9281e-02, -9.1025e-04,  7.8413e-03,  ...,  6.6094e-03,\n",
       "            1.2888e-02,  1.1800e-02],\n",
       "          [-8.9897e-02, -1.9973e-01,  4.1673e-01,  ..., -2.5773e-01,\n",
       "            6.4918e-02, -1.2348e-01],\n",
       "          [-9.0975e-03, -2.6906e-01,  1.9360e-01,  ...,  1.5475e-01,\n",
       "            1.5209e-01, -3.2767e-01],\n",
       "          ...,\n",
       "          [ 1.2910e-01, -3.2312e-02, -9.7975e-02,  ...,  6.0764e-02,\n",
       "            3.2871e-01, -2.7853e-01],\n",
       "          [ 2.6608e-01, -1.4169e-03, -5.3183e-01,  ..., -1.3598e-01,\n",
       "           -1.7948e-01,  6.3431e-02],\n",
       "          [ 6.7582e-02, -5.0048e-02, -1.4306e-01,  ..., -9.6634e-02,\n",
       "            9.9039e-02,  3.7872e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5533e-03,  7.8972e-03, -7.6895e-03,  ..., -2.8458e-03,\n",
       "           -2.1615e-03,  5.4001e-03],\n",
       "          [ 2.2085e-02,  8.9535e-02,  7.9079e-02,  ..., -6.4043e-02,\n",
       "            2.8854e-01,  1.7998e-02],\n",
       "          [ 1.1775e-01,  7.2789e-02, -1.0767e-01,  ...,  8.1835e-02,\n",
       "           -1.1427e-01,  3.3876e-01],\n",
       "          ...,\n",
       "          [-3.7196e-02,  2.6695e-01,  2.9207e-02,  ..., -1.1319e-01,\n",
       "            1.9986e-02, -2.0873e-02],\n",
       "          [ 2.7925e-01, -5.1737e-02,  5.7108e-03,  ..., -1.4646e-01,\n",
       "           -6.1214e-02, -4.7263e-02],\n",
       "          [ 7.3097e-02, -6.2099e-02, -1.2069e-01,  ...,  7.6353e-02,\n",
       "           -3.1553e-02,  6.2219e-02]],\n",
       "\n",
       "         [[ 2.0122e-03, -1.3375e-02,  3.9696e-03,  ...,  4.2177e-02,\n",
       "            2.2304e-03, -4.5831e-03],\n",
       "          [ 1.8146e-01,  8.8693e-01, -2.0851e-01,  ...,  8.4912e-04,\n",
       "           -5.7241e-02,  2.4886e-01],\n",
       "          [-1.9064e-02,  8.3139e-02, -1.5169e-01,  ..., -1.3037e-01,\n",
       "           -4.0631e-02,  1.3742e-01],\n",
       "          ...,\n",
       "          [ 9.1801e-02,  4.0042e-02, -1.8814e-01,  ..., -1.4894e-01,\n",
       "           -2.9985e-01,  1.5323e-02],\n",
       "          [-1.3511e-01,  2.8914e-01, -2.5122e-01,  ...,  3.7535e-01,\n",
       "           -3.5208e-01,  2.5661e-02],\n",
       "          [ 1.4243e-01, -2.1554e-01, -1.1843e-01,  ...,  1.2565e-01,\n",
       "            2.9573e-02,  2.2348e-01]],\n",
       "\n",
       "         [[-7.4330e-03, -4.2978e-04,  3.1423e-04,  ...,  4.1949e-03,\n",
       "            2.5552e-03,  3.2429e-03],\n",
       "          [ 2.9607e-01, -1.7397e-01,  2.5248e-01,  ...,  9.9317e-02,\n",
       "           -1.7165e-01, -8.3696e-03],\n",
       "          [ 2.9204e-01,  3.2889e-03,  1.8541e-01,  ...,  4.1245e-02,\n",
       "           -4.4797e-02, -1.9319e-01],\n",
       "          ...,\n",
       "          [-1.7888e-02, -2.5779e-01, -2.2084e-02,  ..., -8.2205e-03,\n",
       "            1.4851e-01,  1.5410e-01],\n",
       "          [-1.9139e-01, -4.8902e-01,  1.3766e-01,  ...,  1.2743e-01,\n",
       "            3.8616e-01,  1.9767e-01],\n",
       "          [ 1.3747e-01, -2.8310e-01, -8.8099e-04,  ...,  3.8744e-01,\n",
       "            2.5021e-01,  2.5476e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4967e-03,  2.6604e-03,  1.5039e-03,  ..., -9.6533e-01,\n",
       "           -1.3785e+00,  1.8550e+00],\n",
       "          [ 1.7556e-01,  3.2304e+00,  2.8467e+00,  ..., -7.3293e-01,\n",
       "           -1.3575e-01, -5.8133e+00],\n",
       "          [ 2.9685e+00,  2.5912e+00,  3.0929e+00,  ..., -1.1831e+00,\n",
       "           -7.9500e-01, -7.1177e+00],\n",
       "          ...,\n",
       "          [ 1.9680e-01, -4.9284e-01, -9.4719e-01,  ..., -2.8440e+00,\n",
       "           -1.6851e+00, -6.0134e+00],\n",
       "          [ 9.8932e-01, -1.2586e+00,  2.7406e+00,  ..., -9.9612e-01,\n",
       "           -1.3367e+00, -4.4761e+00],\n",
       "          [ 2.3569e+00, -3.0573e+00,  3.7810e+00,  ..., -6.6523e-01,\n",
       "           -1.3404e+00, -5.8290e+00]],\n",
       "\n",
       "         [[-1.0779e-03,  2.2202e-04, -5.6755e-04,  ..., -1.6232e+00,\n",
       "           -1.6631e+00,  6.4198e-01],\n",
       "          [-3.2340e-01,  5.9184e-01, -1.2316e+00,  ...,  3.2818e+00,\n",
       "            2.0717e+00,  3.4015e-02],\n",
       "          [-6.2296e-01,  1.3594e+00, -2.0198e+00,  ...,  2.3120e+00,\n",
       "            5.8009e-01, -3.3521e-01],\n",
       "          ...,\n",
       "          [-1.7598e+00, -5.1635e-01, -1.2815e+00,  ...,  4.2080e+00,\n",
       "            1.2971e+00,  2.1954e+00],\n",
       "          [ 9.9343e-01, -1.3893e+00, -1.6089e+00,  ...,  5.0258e+00,\n",
       "            3.8049e+00,  2.1365e+00],\n",
       "          [ 2.5339e-03, -6.9582e-01, -1.2407e+00,  ...,  3.8323e+00,\n",
       "            2.9886e+00,  5.2060e-01]],\n",
       "\n",
       "         [[ 4.6021e-03, -3.9749e-03, -1.6506e-03,  ..., -5.8952e-01,\n",
       "            3.6434e-01, -6.7574e-01],\n",
       "          [ 3.5349e-02, -1.9899e-01,  2.0779e-02,  ...,  2.4320e-01,\n",
       "           -1.3661e+00,  2.0124e+00],\n",
       "          [-3.2835e-02, -1.2161e-01, -3.1243e-01,  ..., -2.5174e-01,\n",
       "           -3.3261e+00,  4.7717e+00],\n",
       "          ...,\n",
       "          [ 2.2837e-01,  3.6482e-01, -3.8617e-01,  ...,  2.5217e-01,\n",
       "           -3.0827e+00,  2.5498e+00],\n",
       "          [ 8.2723e-01,  8.4285e-01, -7.8592e-01,  ...,  8.6087e-01,\n",
       "           -6.2740e+00,  5.0480e+00],\n",
       "          [ 4.2575e-02,  4.0629e-01, -6.1178e-01,  ...,  2.2798e+00,\n",
       "           -7.4659e+00,  5.4547e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7316e-03, -4.2716e-03,  4.6714e-04,  ...,  1.9551e+00,\n",
       "           -1.0902e+00, -2.1477e-01],\n",
       "          [ 1.1894e+00, -3.8851e-01,  1.4169e+00,  ..., -3.1111e+00,\n",
       "           -1.1534e+00, -1.7159e-01],\n",
       "          [ 3.5635e-01,  1.6504e+00,  1.4729e+00,  ..., -4.2523e+00,\n",
       "            6.9958e-02, -1.4389e+00],\n",
       "          ...,\n",
       "          [ 2.3261e+00,  2.0816e+00,  2.5022e+00,  ..., -5.8420e+00,\n",
       "            7.6115e-01,  3.9118e-01],\n",
       "          [ 1.1748e+00,  1.5212e+00,  3.2594e+00,  ..., -4.7072e+00,\n",
       "            2.2290e+00, -1.4528e+00],\n",
       "          [-1.0338e+00,  6.1038e-02,  2.3869e+00,  ..., -5.9951e+00,\n",
       "            2.0360e+00, -1.3435e+00]],\n",
       "\n",
       "         [[-5.2920e-03, -4.7434e-03, -3.0269e-03,  ...,  9.8426e-01,\n",
       "            2.1160e+00,  1.5318e+00],\n",
       "          [-1.0342e+00, -1.5279e+00, -7.0976e-01,  ...,  5.9087e-01,\n",
       "           -3.6425e+00, -4.4943e-01],\n",
       "          [ 1.9398e-01, -8.4997e-01, -1.7519e+00,  ..., -4.3647e-01,\n",
       "           -4.0110e+00, -2.2596e-01],\n",
       "          ...,\n",
       "          [-8.2990e-01,  1.0761e+00, -4.1425e-01,  ...,  2.6331e-01,\n",
       "           -3.9363e+00,  2.2877e-02],\n",
       "          [ 1.8983e-01,  4.7992e-01, -1.0595e+00,  ...,  4.0638e-01,\n",
       "           -4.5942e+00, -9.8043e-01],\n",
       "          [ 1.7974e+00,  1.7365e+00, -1.7772e+00,  ..., -2.4443e-01,\n",
       "           -5.1953e+00, -1.7610e+00]],\n",
       "\n",
       "         [[-7.8249e-04,  1.6249e-03, -1.1905e-04,  ..., -1.2706e+00,\n",
       "            2.8295e-01, -9.4425e-01],\n",
       "          [-9.4435e-01,  2.6053e+00,  2.6285e+00,  ..., -6.1585e-01,\n",
       "            9.6541e-01, -1.0266e+00],\n",
       "          [-2.0616e+00,  2.1779e+00,  1.1591e+00,  ..., -9.7196e-01,\n",
       "            5.4730e-01, -1.0527e+00],\n",
       "          ...,\n",
       "          [-1.8548e+00,  1.1954e+00,  3.5759e+00,  ..., -1.4420e+00,\n",
       "            1.8684e+00, -6.9789e-01],\n",
       "          [-2.8019e-01, -8.9294e-01,  1.9252e+00,  ..., -3.2035e+00,\n",
       "            1.9674e+00, -1.2559e+00],\n",
       "          [-1.0034e+00, -1.9837e+00,  2.1215e+00,  ..., -1.6643e+00,\n",
       "            9.3006e-01, -2.8161e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-3.4081e-03,  7.1955e-03,  7.7760e-03,  ...,  7.8142e-04,\n",
       "           -4.2747e-02,  1.2287e-02],\n",
       "          [ 3.4477e-02, -1.3022e-01, -3.6538e-01,  ..., -1.2739e-01,\n",
       "            7.8042e-01, -1.3129e-01],\n",
       "          [ 1.1747e-01, -1.6262e-02, -3.2172e-01,  ..., -1.1783e-01,\n",
       "            5.8463e-01, -2.5087e-01],\n",
       "          ...,\n",
       "          [ 5.8723e-02,  5.8168e-02,  1.7285e-01,  ..., -8.5760e-02,\n",
       "           -1.7490e-02,  4.7924e-02],\n",
       "          [ 2.3562e-02, -7.5857e-02,  1.6970e-01,  ..., -1.8910e-01,\n",
       "            7.3636e-01,  2.5994e-01],\n",
       "          [-1.8493e-01, -1.2981e-01,  3.2114e-01,  ..., -2.0626e-01,\n",
       "            7.6987e-01, -1.7002e-01]],\n",
       "\n",
       "         [[ 1.4446e-04, -4.7184e-03,  1.7314e-02,  ...,  1.6688e-03,\n",
       "           -1.1501e-03,  3.8459e-03],\n",
       "          [ 1.5448e-01, -2.1471e-01,  1.2103e-01,  ...,  1.1633e-02,\n",
       "           -2.4337e-01, -3.5180e-01],\n",
       "          [ 1.9278e-01, -2.0086e-01,  2.3535e-01,  ..., -4.6488e-03,\n",
       "           -3.2147e-01, -1.9530e-01],\n",
       "          ...,\n",
       "          [-1.6761e-01, -2.8501e-01,  3.4945e-01,  ..., -6.6032e-02,\n",
       "            9.2370e-02, -4.3661e-02],\n",
       "          [-1.5571e-01, -3.9843e-01, -1.6677e-01,  ..., -2.8719e-01,\n",
       "           -3.1455e-02,  2.4818e-02],\n",
       "          [ 9.3601e-02, -2.2988e-01, -8.8950e-02,  ..., -1.6114e-01,\n",
       "            4.1833e-02,  2.2222e-01]],\n",
       "\n",
       "         [[ 4.2055e-02,  1.9544e-04, -3.0428e-04,  ...,  3.1511e-03,\n",
       "           -1.6241e-02, -1.5832e-03],\n",
       "          [-9.8719e-01, -1.0693e-02, -4.7581e-01,  ..., -4.6354e-02,\n",
       "           -4.0926e-04, -2.9864e-01],\n",
       "          [-9.2068e-01,  4.1747e-02, -3.1274e-01,  ..., -1.8537e-01,\n",
       "           -1.8961e-01,  7.6256e-02],\n",
       "          ...,\n",
       "          [-6.8297e-01, -6.2368e-02,  2.9590e-01,  ...,  1.2979e-01,\n",
       "            2.2444e-01, -1.8242e-01],\n",
       "          [-7.5384e-01, -1.5829e-03, -4.7041e-02,  ..., -3.9804e-01,\n",
       "            4.1447e-01, -9.5332e-02],\n",
       "          [-7.7588e-01,  5.3410e-02,  8.3695e-02,  ..., -4.1682e-01,\n",
       "            3.8352e-01,  2.5183e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4683e-03,  9.2871e-03,  7.6694e-03,  ..., -8.7673e-03,\n",
       "            7.8994e-03,  3.8223e-03],\n",
       "          [ 1.7835e-03,  5.4847e-02, -1.4752e-01,  ...,  1.9168e-01,\n",
       "           -9.3593e-02, -3.4674e-02],\n",
       "          [ 1.0175e-01, -1.6887e-01, -1.6723e-01,  ...,  2.9115e-02,\n",
       "           -5.9374e-02,  1.7025e-01],\n",
       "          ...,\n",
       "          [ 1.0474e-01,  1.0163e-01, -8.5407e-02,  ...,  3.4193e-02,\n",
       "            8.6935e-03, -3.2597e-01],\n",
       "          [-3.5118e-01,  2.5296e-01,  2.4873e-01,  ..., -3.4207e-02,\n",
       "           -4.3161e-02, -1.3975e-01],\n",
       "          [-1.9226e-02,  1.2181e-02,  4.0379e-02,  ...,  2.5581e-02,\n",
       "           -6.5695e-02,  9.4493e-02]],\n",
       "\n",
       "         [[ 2.2299e-02,  3.5980e-02, -8.7464e-03,  ..., -1.4255e-01,\n",
       "           -1.0861e-02,  3.7663e-03],\n",
       "          [-1.5973e-01, -1.6873e-01,  2.2679e-02,  ...,  4.0368e-01,\n",
       "            8.7374e-02, -9.4101e-02],\n",
       "          [-7.3970e-02, -3.8851e-03, -2.5410e-02,  ...,  2.4930e-01,\n",
       "           -9.8555e-03, -2.1053e-01],\n",
       "          ...,\n",
       "          [-1.5209e-01, -6.7428e-01, -1.8410e-02,  ...,  5.2865e-01,\n",
       "           -1.1345e-01, -5.0709e-02],\n",
       "          [-3.5837e-01, -1.8529e-01,  4.5738e-02,  ...,  5.6779e-01,\n",
       "           -3.3695e-01, -2.3621e-01],\n",
       "          [-3.0344e-01, -3.6950e-01,  6.0974e-02,  ...,  5.1450e-01,\n",
       "           -3.6029e-01, -1.4729e-01]],\n",
       "\n",
       "         [[-2.9350e-03,  8.5827e-03,  5.6684e-03,  ..., -3.7662e-04,\n",
       "            7.6714e-03, -6.7722e-03],\n",
       "          [ 3.2811e-01, -6.8338e-02,  1.3645e+00,  ..., -5.3010e-02,\n",
       "           -2.1744e-01, -3.4017e-02],\n",
       "          [ 2.0567e-01, -3.4409e-01,  7.9592e-01,  ...,  4.2119e-02,\n",
       "            4.0613e-02,  9.4421e-02],\n",
       "          ...,\n",
       "          [-2.3907e-01, -2.4493e-01, -6.0034e-02,  ..., -1.2657e-02,\n",
       "           -2.8575e-01, -2.8569e-01],\n",
       "          [-3.8544e-01, -4.6971e-01,  1.1629e+00,  ...,  4.5241e-01,\n",
       "            4.7655e-02, -1.4546e-01],\n",
       "          [-2.4955e-01,  1.6030e-01,  4.8972e-01,  ...,  1.9342e-01,\n",
       "            5.7150e-02,  1.3776e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.8020e-03,  2.8957e-04, -2.7154e-03,  ..., -1.1323e+00,\n",
       "            8.9573e-01,  9.2335e-01],\n",
       "          [ 2.7646e+00,  3.3090e+00, -3.3095e+00,  ..., -7.3934e-01,\n",
       "            1.7687e-01,  1.8110e+00],\n",
       "          [ 5.2643e+00, -2.4147e-01, -4.6218e+00,  ...,  2.1198e-01,\n",
       "            7.3652e-01,  1.6879e+00],\n",
       "          ...,\n",
       "          [ 4.1346e-01, -4.2777e+00,  1.6433e+00,  ..., -2.1210e+00,\n",
       "            1.3257e+00, -1.1594e+00],\n",
       "          [ 2.6402e+00, -4.1863e+00, -1.6896e+00,  ..., -7.9358e-01,\n",
       "            1.0394e+00, -3.6089e-01],\n",
       "          [ 3.6346e+00, -3.9927e+00, -3.1437e+00,  ...,  2.3748e-02,\n",
       "            2.0424e+00,  4.8124e-01]],\n",
       "\n",
       "         [[ 3.9731e-03,  1.7375e-03,  3.6993e-03,  ..., -4.1669e-01,\n",
       "            1.2984e+00,  3.6516e-01],\n",
       "          [ 2.7316e+00, -1.9664e+00,  8.3513e-01,  ..., -6.3431e-01,\n",
       "           -1.8184e-01, -3.7737e-01],\n",
       "          [-7.1260e-01, -2.7507e+00, -1.9995e-01,  ..., -4.4587e-01,\n",
       "            6.8202e-01,  6.6471e-01],\n",
       "          ...,\n",
       "          [ 1.3256e+00, -2.3021e+00,  5.6892e-03,  ..., -8.6147e-01,\n",
       "           -3.5964e-01,  1.4878e-01],\n",
       "          [ 5.4513e-01,  2.0307e+00,  4.1174e+00,  ...,  1.2149e-01,\n",
       "           -5.0953e-01, -4.5871e-01],\n",
       "          [-9.7785e-01,  2.6199e+00,  2.4671e+00,  ...,  2.2720e-01,\n",
       "           -3.0098e-01, -1.2075e-02]],\n",
       "\n",
       "         [[ 7.1280e-04,  2.5565e-03, -9.5440e-05,  ..., -1.4828e+00,\n",
       "           -1.0299e+00,  1.4630e+00],\n",
       "          [ 3.9888e-01,  1.3683e-02,  3.3159e-01,  ...,  5.9133e-01,\n",
       "            5.1732e+00, -4.2454e+00],\n",
       "          [ 6.1101e-01, -1.0605e-01, -3.8240e-02,  ...,  2.5981e+00,\n",
       "            6.0056e+00, -1.2211e+00],\n",
       "          ...,\n",
       "          [-4.8750e-02,  4.6726e-01,  4.0640e-01,  ...,  1.6191e+00,\n",
       "            5.0422e+00, -5.2529e+00],\n",
       "          [ 6.2128e-01, -6.2457e-02,  4.7819e-01,  ..., -1.6049e+00,\n",
       "            4.0165e+00, -2.4480e+00],\n",
       "          [ 4.9445e-01, -1.7532e-01,  5.4572e-01,  ...,  1.6455e+00,\n",
       "            1.5132e+00, -4.5757e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9031e-03,  4.0463e-03,  2.5861e-03,  ...,  1.1092e+00,\n",
       "           -5.9139e-01, -2.6310e+00],\n",
       "          [-2.4933e-01,  1.6115e+00,  1.0092e+00,  ...,  4.4847e-01,\n",
       "           -9.1727e-01,  3.0975e+00],\n",
       "          [-1.7022e+00,  2.0814e-01,  8.3109e-01,  ...,  1.5475e+00,\n",
       "           -9.8532e-01,  4.2973e+00],\n",
       "          ...,\n",
       "          [-3.0054e-01, -6.3009e-01,  1.4425e+00,  ...,  9.2751e-01,\n",
       "           -5.8861e-01,  6.1462e+00],\n",
       "          [-1.6292e+00, -1.6760e+00,  2.1235e+00,  ...,  2.8127e-01,\n",
       "           -3.1627e-01,  6.4949e+00],\n",
       "          [-1.3433e+00, -2.0047e+00,  2.3682e+00,  ...,  3.8503e-01,\n",
       "            6.5478e-01,  7.3969e+00]],\n",
       "\n",
       "         [[ 4.7719e-03,  5.4091e-03, -2.3318e-03,  ..., -9.4375e-01,\n",
       "           -2.2692e+00, -1.5000e+00],\n",
       "          [-2.7564e-01,  1.7597e+00, -1.2135e+00,  ..., -3.9167e-01,\n",
       "            7.2199e-02, -1.8885e+00],\n",
       "          [-1.6019e+00,  9.4388e-01,  5.3019e-02,  ...,  1.0824e+00,\n",
       "            1.2873e+00, -1.3240e+00],\n",
       "          ...,\n",
       "          [-6.1317e-01,  8.5106e-01, -6.3962e-01,  ...,  1.8222e+00,\n",
       "            2.2636e+00, -2.8167e+00],\n",
       "          [-2.8732e-01, -1.7560e+00, -3.2837e-02,  ...,  1.5904e-01,\n",
       "            3.6975e+00, -1.4821e+00],\n",
       "          [-9.0435e-01, -3.1099e+00, -1.1006e+00,  ...,  1.0604e+00,\n",
       "            3.2614e+00, -2.8274e+00]],\n",
       "\n",
       "         [[-3.3177e-03, -4.9230e-03, -1.1449e-03,  ...,  8.6848e-01,\n",
       "            2.7786e+00, -3.5159e+00],\n",
       "          [ 1.5401e-01, -1.1277e+00,  1.1206e+00,  ..., -4.1960e-01,\n",
       "           -3.2212e+00,  2.0708e+00],\n",
       "          [ 6.2800e-01, -2.4161e-01,  1.7434e+00,  ...,  1.8313e+00,\n",
       "           -5.8055e+00,  5.0296e-01],\n",
       "          ...,\n",
       "          [-1.4437e+00,  1.9526e+00, -6.5334e-01,  ...,  3.5634e+00,\n",
       "           -7.2532e+00,  2.3858e+00],\n",
       "          [ 2.0651e+00,  1.2709e-01,  1.2294e+00,  ...,  3.5311e+00,\n",
       "           -5.1781e+00,  4.1010e+00],\n",
       "          [ 2.2597e-01,  1.5877e+00,  1.8143e+00,  ...,  3.5271e+00,\n",
       "           -2.5639e+00,  4.3891e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 8.7406e-03,  3.0163e-02, -6.0530e-03,  ...,  4.8182e-03,\n",
       "            5.9949e-03, -2.1235e-02],\n",
       "          [ 1.7348e-01, -1.1366e-02, -9.8780e-02,  ...,  1.1221e-01,\n",
       "            9.1445e-02,  2.1847e-01],\n",
       "          [ 3.1518e-01,  1.4310e-01,  3.1110e-01,  ...,  7.1689e-02,\n",
       "           -2.6878e-03, -1.6876e-01],\n",
       "          ...,\n",
       "          [ 2.4488e-02, -8.8831e-02, -2.4452e-02,  ...,  2.2876e-02,\n",
       "           -8.1325e-02,  2.3154e-02],\n",
       "          [ 1.5548e-02, -5.3978e-01,  5.2293e-02,  ..., -9.2763e-02,\n",
       "            1.2424e-02,  4.2792e-01],\n",
       "          [ 5.8490e-03, -1.3314e-01, -3.5847e-01,  ..., -2.3221e-01,\n",
       "           -5.4314e-02,  1.0027e-01]],\n",
       "\n",
       "         [[ 4.8663e-02, -6.1931e-03,  4.5511e-03,  ...,  1.6189e-02,\n",
       "            7.9031e-03, -7.7054e-03],\n",
       "          [ 5.6344e-02,  2.4191e-01,  4.1447e-02,  ..., -4.3143e-02,\n",
       "            4.6331e-01, -9.3458e-02],\n",
       "          [ 2.1344e-01,  1.2448e-01, -3.2936e-02,  ...,  1.4095e-03,\n",
       "            6.1733e-01, -1.5063e-01],\n",
       "          ...,\n",
       "          [ 2.8433e-01,  7.3400e-01,  5.5013e-02,  ...,  1.0034e-01,\n",
       "           -4.7207e-01,  1.7799e-02],\n",
       "          [ 1.1983e-01,  7.9610e-01,  3.2319e-01,  ..., -6.2267e-02,\n",
       "            1.0872e-01,  8.3888e-02],\n",
       "          [ 2.0593e-01,  4.5176e-01,  2.1607e-01,  ...,  1.3725e-03,\n",
       "           -2.5119e-02, -1.0560e-01]],\n",
       "\n",
       "         [[ 1.7050e-03, -1.4518e-02,  7.4567e-04,  ..., -1.1062e-02,\n",
       "            1.0233e-02, -1.1288e-03],\n",
       "          [ 9.8636e-02,  3.3142e-01,  6.6337e-02,  ...,  1.4554e-01,\n",
       "            2.9260e-01, -7.6429e-02],\n",
       "          [-3.2957e-02,  2.4902e-01,  2.0483e-01,  ...,  3.3081e-01,\n",
       "            2.2143e-01,  5.1959e-02],\n",
       "          ...,\n",
       "          [ 2.6225e-01,  7.5572e-01,  2.5813e-01,  ...,  6.1277e-02,\n",
       "           -9.8498e-02,  1.0674e-01],\n",
       "          [ 5.3868e-01,  5.1157e-01,  2.6171e-01,  ...,  4.8047e-01,\n",
       "            1.7645e-01,  2.9737e-01],\n",
       "          [ 2.7119e-01,  2.4362e-01, -2.1750e-03,  ...,  1.1309e-01,\n",
       "           -1.1103e-01, -1.9219e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.3310e-02,  7.7015e-03,  6.6515e-03,  ..., -1.3152e-02,\n",
       "            1.6485e-02, -2.8625e-03],\n",
       "          [ 1.9033e-01,  3.3945e-01, -3.3768e-01,  ...,  1.3984e-01,\n",
       "           -7.9596e-02,  4.5116e-02],\n",
       "          [-8.1460e-02, -1.2046e-02, -7.8516e-02,  ..., -2.0579e-01,\n",
       "            1.3628e-02, -3.0648e-01],\n",
       "          ...,\n",
       "          [ 3.1315e-01,  3.6188e-01, -4.4517e-02,  ...,  1.4904e-01,\n",
       "            3.7742e-01, -2.5088e-01],\n",
       "          [ 8.5729e-02, -1.2982e-01, -3.0609e-02,  ...,  4.3599e-01,\n",
       "            2.3904e-01, -4.2303e-01],\n",
       "          [ 1.7274e-02,  6.3057e-03,  1.5726e-01,  ...,  1.8862e-01,\n",
       "           -1.6499e-02, -2.6820e-01]],\n",
       "\n",
       "         [[ 2.1405e-02,  6.0433e-03,  1.0379e-02,  ..., -2.1364e-02,\n",
       "           -6.0028e-03,  6.9378e-03],\n",
       "          [-2.3556e-01,  2.0520e-01, -5.1933e-01,  ...,  5.4975e-01,\n",
       "            2.5726e-01,  3.5610e-01],\n",
       "          [ 4.2984e-01,  2.5071e-01, -3.7570e-01,  ...,  1.6206e-01,\n",
       "            7.8941e-02,  4.0438e-01],\n",
       "          ...,\n",
       "          [-2.6124e-01,  2.5292e-01, -1.0781e-01,  ...,  1.5793e-01,\n",
       "            4.7749e-01, -2.8305e-01],\n",
       "          [ 2.3375e-01,  2.3815e-01, -6.4842e-02,  ...,  1.7144e-01,\n",
       "            2.1631e-01, -2.2137e-01],\n",
       "          [ 2.0088e-02, -1.0944e-01, -2.0853e-01,  ...,  1.6205e-01,\n",
       "           -9.0374e-02, -4.0356e-01]],\n",
       "\n",
       "         [[ 2.2282e-02, -4.7043e-03, -4.4289e-02,  ...,  6.8746e-03,\n",
       "           -8.1504e-03,  2.8155e-03],\n",
       "          [ 6.1206e-02, -8.6346e-02, -1.1587e-01,  ..., -3.0785e-01,\n",
       "           -1.7082e-02,  7.6766e-03],\n",
       "          [-1.1209e-01, -2.0760e-02, -2.1674e-02,  ..., -1.3198e-01,\n",
       "            1.7352e-02, -1.0700e-01],\n",
       "          ...,\n",
       "          [-3.5045e-01, -1.3695e-01,  9.0724e-02,  ...,  2.7481e-01,\n",
       "           -5.4281e-02,  2.6552e-01],\n",
       "          [-2.0182e-01, -2.2503e-01,  1.6831e-01,  ...,  5.9286e-01,\n",
       "           -4.7392e-01,  1.0215e-01],\n",
       "          [-1.6921e-01,  1.0709e-01,  5.8669e-02,  ..., -6.8701e-02,\n",
       "           -8.8365e-02,  7.3942e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.8842e-03,  2.8870e-04, -2.2700e-03,  ...,  6.9198e-01,\n",
       "           -2.1982e-01, -1.1094e-01],\n",
       "          [-1.6589e+00,  4.3686e-01, -7.9821e-02,  ...,  7.1325e-01,\n",
       "           -8.5704e-01,  1.3543e+00],\n",
       "          [-1.6938e+00,  1.7624e+00,  1.2153e+00,  ...,  2.4806e+00,\n",
       "            1.2211e+00,  1.0223e+00],\n",
       "          ...,\n",
       "          [-1.0404e+00,  8.0264e-01,  3.5652e-01,  ...,  2.2334e+00,\n",
       "           -4.5666e-01, -5.2490e-01],\n",
       "          [-1.4086e+00,  1.9272e+00,  1.2191e+00,  ...,  1.2814e+00,\n",
       "            4.8540e-01, -1.3927e+00],\n",
       "          [-1.4670e+00, -3.6686e-01, -3.8648e-01,  ...,  6.3713e-01,\n",
       "            9.4509e-03, -3.3430e-01]],\n",
       "\n",
       "         [[ 2.4911e-03,  2.2665e-03,  8.2780e-04,  ...,  9.2604e-01,\n",
       "            1.4586e-01, -4.6042e-01],\n",
       "          [-1.2031e+00,  2.1692e+00,  6.0725e-01,  ..., -1.5825e+00,\n",
       "           -8.6241e-01, -1.5859e+00],\n",
       "          [-1.7925e+00,  4.7410e-02,  1.0040e+00,  ..., -3.4516e+00,\n",
       "           -2.1047e+00,  7.4406e-01],\n",
       "          ...,\n",
       "          [-8.1471e-01, -1.1290e+00, -4.7998e-01,  ..., -1.7077e+00,\n",
       "           -2.5787e-01, -1.2539e+00],\n",
       "          [-3.9441e-01, -2.1660e+00,  2.9808e-01,  ..., -1.5771e+00,\n",
       "           -1.0198e+00, -1.5980e+00],\n",
       "          [-9.4212e-01, -1.0816e+00,  1.7743e+00,  ..., -3.8640e+00,\n",
       "           -1.2912e+00,  3.0800e-01]],\n",
       "\n",
       "         [[ 1.4778e-05, -8.8977e-05,  1.1033e-03,  ...,  3.2080e-01,\n",
       "            7.1295e-01, -1.2617e-01],\n",
       "          [-3.8942e-02, -2.9653e-01, -6.2306e-01,  ...,  1.5669e+00,\n",
       "           -5.0122e-01, -5.0307e-01],\n",
       "          [-2.3892e+00, -1.7494e-01, -2.2768e+00,  ...,  8.0789e-01,\n",
       "           -3.0773e-01,  9.9998e-02],\n",
       "          ...,\n",
       "          [ 1.1738e-01,  1.5978e+00,  1.3159e+00,  ..., -8.0101e-01,\n",
       "           -6.2369e-01,  1.3859e-01],\n",
       "          [-1.2979e+00, -4.0448e-01, -1.1537e+00,  ...,  4.0848e-01,\n",
       "            4.1960e-01,  3.8353e-01],\n",
       "          [-1.1644e+00, -4.9826e-01, -3.4420e+00,  ..., -4.5626e-01,\n",
       "            1.5398e+00,  9.2144e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7783e-03, -2.6796e-03,  2.2735e-03,  ..., -4.6754e-01,\n",
       "            1.7642e+00, -1.1619e-01],\n",
       "          [ 8.5041e-01, -1.3178e+00,  1.5032e+00,  ...,  1.2201e+00,\n",
       "           -1.5179e+00, -4.2159e-01],\n",
       "          [ 4.8091e-02, -5.1685e-02,  1.1894e+00,  ...,  1.4587e+00,\n",
       "           -2.5621e+00, -2.2026e-01],\n",
       "          ...,\n",
       "          [ 1.5643e+00,  1.6212e+00,  2.4051e+00,  ..., -2.0555e+00,\n",
       "           -3.3984e+00,  5.7557e-01],\n",
       "          [-3.7054e-01,  8.2159e-01,  1.4192e+00,  ..., -1.1152e+00,\n",
       "           -2.4258e+00,  5.8015e-01],\n",
       "          [ 1.7758e-01,  1.4023e+00,  7.4883e-01,  ...,  1.2877e-01,\n",
       "           -2.9599e+00,  2.4900e-01]],\n",
       "\n",
       "         [[-1.3721e-03,  6.8764e-03,  6.4087e-03,  ..., -4.7223e-03,\n",
       "           -1.4665e+00, -8.0141e-01],\n",
       "          [-7.7652e-01,  1.0260e+00, -4.3150e-02,  ...,  5.1906e-01,\n",
       "            6.6438e-01,  8.8142e-01],\n",
       "          [ 5.0761e-01,  2.1167e-01,  1.1949e+00,  ...,  1.4991e+00,\n",
       "            2.7555e+00,  2.9675e+00],\n",
       "          ...,\n",
       "          [-1.3216e+00, -1.0282e+00,  6.7482e-01,  ...,  1.5953e+00,\n",
       "            3.2732e+00,  3.6036e+00],\n",
       "          [-1.1381e+00, -2.0006e+00,  3.8451e-01,  ...,  4.1455e-01,\n",
       "            5.6365e-01,  2.2694e+00],\n",
       "          [ 1.6896e-01, -1.5592e+00,  6.2570e-01,  ..., -3.0565e-01,\n",
       "           -7.5616e-02,  2.5238e+00]],\n",
       "\n",
       "         [[-2.1064e-04, -2.5912e-03, -9.2435e-04,  ...,  3.9791e-01,\n",
       "            2.1654e+00,  1.9424e+00],\n",
       "          [-1.2358e-02, -1.4878e+00, -8.4383e-01,  ..., -2.3234e-01,\n",
       "           -5.7601e-01, -1.7126e+00],\n",
       "          [-2.1225e+00,  6.8188e-02, -1.0100e+00,  ..., -1.2485e+00,\n",
       "           -1.5212e+00, -3.8251e+00],\n",
       "          ...,\n",
       "          [ 9.1474e-01,  2.1527e+00,  1.0304e+00,  ...,  4.3322e-01,\n",
       "           -2.3358e+00, -2.3402e+00],\n",
       "          [-7.4779e-01,  1.5481e+00, -1.8175e+00,  ...,  3.8272e-01,\n",
       "           -2.6379e+00, -2.3705e+00],\n",
       "          [-2.1229e+00,  1.2956e+00, -2.2619e+00,  ...,  1.3982e+00,\n",
       "           -3.7990e+00, -3.3061e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.5394e-02, -2.8117e-02,  2.7875e-02,  ...,  2.5463e-02,\n",
       "           -1.0761e-02, -4.8010e-02],\n",
       "          [ 1.5787e-01,  3.0034e-01, -1.9432e-01,  ...,  1.6622e-03,\n",
       "            2.2574e-02, -3.1777e-01],\n",
       "          [-1.4012e-01,  8.1010e-02,  4.6619e-02,  ..., -8.1943e-02,\n",
       "            3.9576e-01,  2.5800e-02],\n",
       "          ...,\n",
       "          [ 1.4484e-01,  4.0556e-02, -1.1003e-01,  ..., -8.9958e-05,\n",
       "            2.2543e-01, -2.9489e-02],\n",
       "          [ 4.2426e-02, -1.1739e-01, -2.5825e-01,  ..., -2.3001e-01,\n",
       "            3.6046e-01,  1.5431e-01],\n",
       "          [-2.0226e-02,  2.0161e-01, -2.3531e-01,  ..., -1.2637e-01,\n",
       "            2.6996e-01,  8.0962e-02]],\n",
       "\n",
       "         [[-2.0371e-03,  8.3121e-03, -1.4308e-02,  ...,  4.8029e-04,\n",
       "           -3.2176e-03, -3.3904e-03],\n",
       "          [ 1.2914e-01,  1.7631e-01,  3.3059e-01,  ..., -3.6064e-01,\n",
       "           -2.9120e-01,  1.0633e-01],\n",
       "          [-1.0653e-01,  8.9051e-03,  5.2295e-01,  ..., -2.3191e-02,\n",
       "           -2.7087e-02,  4.8649e-02],\n",
       "          ...,\n",
       "          [-1.5279e-01, -2.1046e-01, -5.8181e-02,  ..., -4.6854e-01,\n",
       "            1.2249e-01, -1.0544e-01],\n",
       "          [-1.2519e-02, -1.3741e-01,  2.7642e-01,  ..., -2.2691e-01,\n",
       "           -5.9751e-03, -6.9808e-01],\n",
       "          [ 2.6919e-02,  1.1854e-01,  8.5040e-04,  ..., -1.2718e-01,\n",
       "            3.1187e-01, -4.9490e-01]],\n",
       "\n",
       "         [[-1.5184e-03,  4.5299e-03,  6.9686e-03,  ...,  3.5608e-04,\n",
       "            3.3572e-02, -5.0618e-04],\n",
       "          [-1.7906e-01, -3.1489e-02,  2.7097e-01,  ...,  1.8682e-01,\n",
       "           -3.9076e-01, -2.8346e-01],\n",
       "          [-4.0521e-01,  2.5233e-01,  2.2367e-01,  ...,  4.3188e-01,\n",
       "            3.0748e-01, -1.8183e-02],\n",
       "          ...,\n",
       "          [-1.6034e-01, -2.4452e-01, -1.7287e-01,  ..., -7.1206e-02,\n",
       "            7.1535e-01, -2.1717e-01],\n",
       "          [ 3.2026e-01,  2.4440e-01,  3.7246e-02,  ..., -1.7123e-01,\n",
       "            2.5534e-01, -5.0231e-01],\n",
       "          [ 4.4991e-02,  2.3054e-01, -2.1583e-01,  ..., -6.1654e-02,\n",
       "            3.5519e-01, -2.3420e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.4179e-03, -5.9483e-03,  5.3399e-03,  ..., -4.7193e-03,\n",
       "            1.0593e-02, -1.3212e-03],\n",
       "          [ 1.3995e-02,  5.4121e-02,  2.5061e-02,  ..., -1.1397e-01,\n",
       "           -1.4111e-01, -3.3438e-01],\n",
       "          [-1.7781e-01, -1.0991e-01,  2.2429e-01,  ..., -1.6204e-01,\n",
       "            1.6844e-01, -2.2892e-01],\n",
       "          ...,\n",
       "          [ 3.6863e-01, -2.0560e-01,  1.5947e-01,  ...,  9.6386e-02,\n",
       "            2.3661e-01,  1.9295e-01],\n",
       "          [ 9.0766e-02, -2.0574e-01,  8.9904e-02,  ..., -3.0196e-01,\n",
       "           -1.0874e-01, -3.0178e-03],\n",
       "          [ 1.7245e-01,  1.1427e-01,  2.9139e-01,  ..., -4.8046e-01,\n",
       "            3.7619e-02,  2.4017e-03]],\n",
       "\n",
       "         [[ 2.3157e-02, -1.9252e-02,  2.1539e-03,  ...,  9.5771e-03,\n",
       "            4.6277e-03, -8.5696e-03],\n",
       "          [ 2.4720e-01,  4.2502e-01,  1.1898e-01,  ..., -2.2160e-01,\n",
       "            2.5483e-02,  1.6637e-01],\n",
       "          [ 8.7029e-02,  1.9816e-01, -2.0123e-01,  ...,  3.3793e-01,\n",
       "            1.7903e-01,  1.3231e-02],\n",
       "          ...,\n",
       "          [-3.4032e-02,  2.5328e-01,  7.5739e-02,  ..., -4.1444e-02,\n",
       "            1.5716e-01,  2.9637e-02],\n",
       "          [ 3.7249e-01,  2.3183e-01,  1.2941e-02,  ..., -5.1410e-01,\n",
       "           -1.9044e-01,  2.6002e-01],\n",
       "          [-7.8192e-02, -1.7072e-01,  4.0881e-02,  ..., -2.1982e-01,\n",
       "            1.0650e-01,  1.8385e-01]],\n",
       "\n",
       "         [[ 1.0774e-02,  1.1938e-02, -1.0584e-02,  ...,  6.0791e-03,\n",
       "           -2.2247e-03, -1.8920e-03],\n",
       "          [ 1.4648e-01,  3.1927e-02,  3.5249e-02,  ..., -1.4989e-02,\n",
       "           -7.9144e-02, -7.8204e-02],\n",
       "          [-3.0942e-01,  7.2578e-02, -1.0058e-01,  ..., -1.2579e-01,\n",
       "           -1.8548e-01, -2.6629e-01],\n",
       "          ...,\n",
       "          [-8.1413e-01, -2.1037e-01,  3.9049e-01,  ..., -5.6263e-01,\n",
       "           -2.7721e-02,  1.3229e-01],\n",
       "          [-2.3756e-01, -1.6584e-01,  5.5857e-02,  ..., -5.1961e-01,\n",
       "           -6.9528e-01, -2.7120e-01],\n",
       "          [-1.3873e-01,  2.2440e-01,  6.4024e-02,  ..., -4.0200e-01,\n",
       "           -3.2610e-01, -1.5493e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.4323e-03, -5.7274e-03, -2.9220e-04,  ...,  3.3036e-01,\n",
       "           -7.9079e-01,  3.3854e-01],\n",
       "          [ 3.6411e-01,  1.3740e+00, -6.3967e-02,  ..., -1.0718e+00,\n",
       "           -8.5835e-01,  5.6304e-02],\n",
       "          [-9.4560e-01,  7.8591e-01, -8.4343e-01,  ...,  3.9854e-01,\n",
       "           -3.0680e-01,  1.4594e-01],\n",
       "          ...,\n",
       "          [ 3.5723e-01,  8.0600e-02, -6.5834e-01,  ..., -1.0518e-01,\n",
       "           -1.5344e+00,  4.8736e-01],\n",
       "          [-8.9217e-02, -2.8736e-01, -4.0792e-02,  ...,  1.4046e+00,\n",
       "            1.2873e+00, -1.4888e-01],\n",
       "          [-7.7915e-01, -2.1138e+00,  1.1416e+00,  ...,  2.3852e+00,\n",
       "           -4.9182e-01, -2.5375e-01]],\n",
       "\n",
       "         [[-1.8725e-03,  3.3863e-03, -7.2315e-03,  ..., -5.3815e-01,\n",
       "           -5.5731e-01,  6.1932e-01],\n",
       "          [-9.6148e-01,  1.1661e+00, -9.6985e-01,  ...,  2.5325e+00,\n",
       "           -2.7616e+00,  5.3085e-01],\n",
       "          [-1.3171e-01, -5.9351e-01, -4.6543e-01,  ...,  4.5023e+00,\n",
       "           -1.4749e+00,  2.9828e-02],\n",
       "          ...,\n",
       "          [-7.0140e-01, -1.2250e+00, -3.8320e-02,  ...,  2.8873e+00,\n",
       "           -1.6288e+00, -1.2559e+00],\n",
       "          [-1.4013e+00, -5.2389e-01, -2.5283e+00,  ..., -2.7275e-01,\n",
       "           -2.1399e+00, -7.8448e-01],\n",
       "          [-6.1796e-01,  1.1514e-01, -1.9390e+00,  ...,  3.0036e+00,\n",
       "           -1.1305e+00,  6.5644e-02]],\n",
       "\n",
       "         [[-1.3361e-03, -3.7586e-03,  2.1419e-03,  ..., -5.5360e-02,\n",
       "            1.2167e+00, -2.3505e-01],\n",
       "          [-1.3843e+00, -1.5571e+00,  2.1487e+00,  ...,  3.3779e-01,\n",
       "           -2.1248e-01, -9.9470e-03],\n",
       "          [ 4.7810e-01,  1.5061e-01,  1.0739e+00,  ..., -6.8082e-01,\n",
       "            1.1633e-01,  6.6098e-01],\n",
       "          ...,\n",
       "          [-1.9899e+00,  8.4435e-01,  4.1926e+00,  ...,  2.1809e-01,\n",
       "            1.3043e+00,  3.5934e-02],\n",
       "          [-9.7630e-01,  3.0614e+00,  1.8271e+00,  ..., -9.6578e-02,\n",
       "           -1.0938e+00,  1.1082e+00],\n",
       "          [ 1.2206e+00,  3.6987e-01,  2.6045e+00,  ..., -1.4102e+00,\n",
       "           -1.9960e+00,  1.1921e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.7291e-04,  1.7478e-03,  9.5075e-03,  ...,  1.7828e+00,\n",
       "            1.7678e+00,  4.0869e-01],\n",
       "          [ 5.9703e-01, -2.1566e-01, -6.3027e-02,  ..., -9.9536e-01,\n",
       "           -3.5899e-01, -7.3414e-01],\n",
       "          [-8.2730e-01,  1.2987e+00, -9.7867e-01,  ..., -1.2562e+00,\n",
       "           -1.4319e+00, -7.8253e-01],\n",
       "          ...,\n",
       "          [ 1.0311e+00,  1.0707e+00, -2.0269e+00,  ..., -9.5124e-01,\n",
       "           -3.7951e+00,  1.5509e-01],\n",
       "          [-6.4384e-01,  1.3404e+00, -3.8329e-02,  ..., -1.0559e+00,\n",
       "           -3.2987e+00,  3.5072e-01],\n",
       "          [-1.0937e+00,  8.9584e-01,  4.6955e-01,  ..., -6.4829e-01,\n",
       "           -2.5397e+00,  1.4444e+00]],\n",
       "\n",
       "         [[ 4.5211e-03,  6.2987e-05,  2.5147e-03,  ..., -9.7221e-01,\n",
       "            5.0444e-01,  5.8296e-01],\n",
       "          [ 8.5311e-01,  1.2779e+00,  3.1748e+00,  ..., -1.7663e+00,\n",
       "            2.0872e+00,  4.4725e-01],\n",
       "          [-4.8953e+00, -1.6443e+00,  2.3740e+00,  ..., -8.2807e-03,\n",
       "            3.0793e+00,  3.4702e-01],\n",
       "          ...,\n",
       "          [ 3.9118e+00, -3.6329e+00,  2.3175e+00,  ...,  5.7932e-02,\n",
       "            4.4925e-01,  1.6770e+00],\n",
       "          [-1.8631e+00, -2.9667e+00,  3.1458e+00,  ...,  8.5785e-01,\n",
       "            1.6995e+00,  2.3225e+00],\n",
       "          [-6.7752e+00, -1.1077e+00,  2.7209e+00,  ...,  6.7884e-01,\n",
       "            1.5347e+00,  1.7789e+00]],\n",
       "\n",
       "         [[ 1.5207e-03,  5.4576e-05, -2.2754e-03,  ...,  6.5879e-01,\n",
       "            1.2129e+00,  8.3267e-01],\n",
       "          [ 6.6765e-01,  8.3813e-01, -5.6325e-01,  ..., -1.6620e+00,\n",
       "           -1.9182e+00, -2.4189e+00],\n",
       "          [-1.2258e+00,  6.9629e-02, -1.1622e+00,  ..., -1.0157e+00,\n",
       "           -3.4393e+00, -2.8407e+00],\n",
       "          ...,\n",
       "          [ 5.6226e-01, -8.2268e-01,  1.1648e-01,  ..., -1.3162e+00,\n",
       "            5.2282e-01, -1.1210e-01],\n",
       "          [-1.8654e+00, -1.1362e+00, -1.0341e+00,  ..., -8.4857e-01,\n",
       "           -4.8715e-01, -2.1511e+00],\n",
       "          [-1.6147e+00, -3.0750e+00, -5.4414e-01,  ..., -8.5499e-01,\n",
       "           -1.2860e+00, -1.7254e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 9.3380e-03, -3.0222e-03, -7.7184e-04,  ...,  5.6974e-04,\n",
       "            7.5797e-04,  3.0442e-02],\n",
       "          [-6.2043e-02, -4.0932e-01, -4.6203e-02,  ...,  7.6098e-02,\n",
       "           -4.6574e-02, -1.0123e-01],\n",
       "          [-4.2572e-02, -3.3803e-02, -8.4820e-02,  ...,  3.6044e-02,\n",
       "            4.5334e-02, -3.9798e-02],\n",
       "          ...,\n",
       "          [-5.2908e-02, -2.9647e-01, -2.5809e-01,  ..., -4.3177e-01,\n",
       "            3.7375e-03, -1.9467e-01],\n",
       "          [-1.2694e-01, -4.3009e-02, -2.0600e-01,  ..., -2.6311e-01,\n",
       "            2.4109e-01,  9.3810e-02],\n",
       "          [-1.6216e-01, -2.4130e-02, -3.9062e-01,  ..., -2.0917e-01,\n",
       "            1.9982e-01,  3.3919e-02]],\n",
       "\n",
       "         [[ 8.2246e-03, -2.0338e-03,  5.4085e-03,  ..., -2.2932e-02,\n",
       "            7.8196e-02, -9.9718e-03],\n",
       "          [-8.2830e-02, -4.0103e-02, -8.7571e-02,  ..., -1.6767e-01,\n",
       "            2.2420e-01,  7.5333e-02],\n",
       "          [-1.8313e-01, -4.2604e-03, -7.1007e-02,  ...,  2.4916e-01,\n",
       "            1.8271e-01,  1.9188e-01],\n",
       "          ...,\n",
       "          [-1.0954e-02, -1.5156e-01, -1.4825e-01,  ..., -8.9160e-02,\n",
       "           -7.6236e-01,  5.5438e-01],\n",
       "          [-2.4845e-01,  1.7612e-01, -2.8500e-01,  ...,  5.2619e-02,\n",
       "           -3.0527e-01,  1.2811e-01],\n",
       "          [-9.7568e-02,  1.9528e-01, -3.7692e-02,  ..., -5.6174e-02,\n",
       "            8.7179e-02, -5.5764e-02]],\n",
       "\n",
       "         [[-7.9994e-03, -1.3057e-02,  3.6811e-02,  ...,  1.5338e-02,\n",
       "            9.4707e-03, -1.8848e-01],\n",
       "          [-6.5253e-02, -1.4810e-01, -5.8350e-02,  ..., -1.3061e-01,\n",
       "            2.3678e-01,  2.0033e-01],\n",
       "          [ 1.3327e-01,  6.9006e-02, -1.4503e-01,  ..., -2.3881e-01,\n",
       "           -4.0804e-02, -4.2276e-01],\n",
       "          ...,\n",
       "          [ 3.3489e-01,  7.5879e-02, -2.1367e-01,  ...,  3.8552e-01,\n",
       "            2.0502e-01, -4.8061e-01],\n",
       "          [ 2.4777e-01,  3.0414e-01,  1.0552e-01,  ...,  3.0930e-01,\n",
       "            1.4474e-01,  7.2447e-01],\n",
       "          [ 4.4638e-01,  3.6600e-01, -5.1906e-02,  ...,  5.5578e-02,\n",
       "           -3.7275e-01,  5.6652e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.3424e-03,  4.3738e-01, -1.4696e-02,  ..., -6.7947e-03,\n",
       "            9.0439e-03, -2.3872e-02],\n",
       "          [-2.4250e-02, -2.9417e-01,  2.1234e-01,  ..., -7.6816e-02,\n",
       "            8.8361e-02,  3.2398e-01],\n",
       "          [-2.3211e-02, -1.4916e-01,  3.3098e-02,  ..., -5.8480e-02,\n",
       "           -2.4396e-01,  3.5657e-01],\n",
       "          ...,\n",
       "          [ 3.8169e-02, -2.4529e-01, -4.1598e-01,  ..., -2.3150e-01,\n",
       "           -1.3670e-01,  1.5544e-01],\n",
       "          [-4.3960e-01,  4.4924e-01,  2.4676e-01,  ..., -2.4204e-01,\n",
       "           -3.0062e-02,  2.0774e-01],\n",
       "          [-6.0640e-02, -9.0725e-02,  4.2582e-01,  ..., -5.7788e-02,\n",
       "           -2.1485e-01,  1.7246e-01]],\n",
       "\n",
       "         [[-5.1488e-03,  3.6865e-03, -6.1297e-03,  ..., -7.9216e-03,\n",
       "            1.2789e-02,  2.5968e-01],\n",
       "          [ 3.6350e-02, -1.3955e-01,  9.7820e-02,  ...,  2.8310e-01,\n",
       "           -2.7460e-02, -6.3151e-01],\n",
       "          [ 2.5616e-02,  2.3352e-01,  2.2932e-01,  ..., -2.1658e-01,\n",
       "            9.9478e-02, -5.9835e-01],\n",
       "          ...,\n",
       "          [ 1.7520e-01,  2.6554e-01,  1.8000e-01,  ..., -2.1930e-02,\n",
       "           -2.3471e-03, -6.9624e-01],\n",
       "          [-1.7590e-01,  1.6302e-01, -2.2660e-01,  ..., -6.1486e-02,\n",
       "           -1.7918e-02, -1.8618e-02],\n",
       "          [ 4.2632e-02,  2.9861e-01,  3.1899e-02,  ..., -1.3571e-01,\n",
       "            7.2276e-02,  5.8636e-01]],\n",
       "\n",
       "         [[-5.0414e-03, -6.1499e-03, -6.9996e-03,  ..., -1.1288e-01,\n",
       "            1.1043e-02,  6.2161e-02],\n",
       "          [-2.9550e-01,  6.7077e-02, -4.2941e-02,  ...,  7.4352e-01,\n",
       "            3.4090e-02, -1.1277e-01],\n",
       "          [-3.1428e-02,  2.1154e-01,  1.0428e-02,  ...,  1.5991e-01,\n",
       "           -3.5378e-01, -1.1662e-01],\n",
       "          ...,\n",
       "          [-1.8930e-01, -1.6049e-01,  6.0395e-01,  ...,  5.1400e-01,\n",
       "           -6.8777e-01, -4.7338e-01],\n",
       "          [ 7.5009e-02, -1.4611e-01,  3.4605e-01,  ...,  6.4128e-02,\n",
       "           -2.1899e-01, -1.7446e-01],\n",
       "          [ 5.2583e-02, -3.5921e-02,  3.5840e-01,  ..., -2.4535e-01,\n",
       "            3.8772e-02, -1.5831e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.6947e-03, -2.3719e-03,  5.0183e-04,  ...,  7.3214e-01,\n",
       "            7.6373e-01, -5.8076e-01],\n",
       "          [-8.9594e-01, -1.5126e+00,  1.2044e+00,  ..., -9.8009e-01,\n",
       "           -6.7036e-01,  1.3975e-01],\n",
       "          [ 1.1392e+00,  9.9434e-01,  1.0609e+00,  ...,  2.4461e-01,\n",
       "            2.1935e-01,  9.2423e-02],\n",
       "          ...,\n",
       "          [-6.8440e-01,  4.9820e-01,  2.3753e-01,  ..., -7.0249e-01,\n",
       "            1.0650e+00, -4.3351e-01],\n",
       "          [-2.1118e-01,  2.7372e+00,  1.3354e+00,  ..., -4.0697e-01,\n",
       "           -4.5380e-01, -1.6252e+00],\n",
       "          [ 1.4982e+00,  1.9561e+00,  4.9233e-01,  ..., -6.5164e-01,\n",
       "           -1.1252e+00, -1.2507e+00]],\n",
       "\n",
       "         [[-5.8947e-04,  2.5583e-03, -3.4537e-04,  ..., -1.2813e+00,\n",
       "           -1.5517e+00,  8.6527e-01],\n",
       "          [-1.7735e+00,  1.4242e+00, -2.8773e-01,  ..., -5.2780e-01,\n",
       "           -3.1559e-01,  8.5183e-01],\n",
       "          [-1.6060e+00, -1.2449e-01, -1.6214e+00,  ..., -8.5468e-01,\n",
       "            1.1791e+00,  3.7239e-01],\n",
       "          ...,\n",
       "          [-4.8687e-01,  1.5345e-01,  8.7129e-01,  ..., -1.0436e+00,\n",
       "           -8.4471e-01,  2.4277e+00],\n",
       "          [-2.1471e+00, -4.0419e+00,  1.6182e+00,  ..., -1.2935e+00,\n",
       "            6.0921e-01,  2.4026e+00],\n",
       "          [-1.7263e+00, -1.1479e-01,  1.7988e+00,  ..., -9.1677e-01,\n",
       "            2.0591e+00,  1.0677e+00]],\n",
       "\n",
       "         [[-1.1987e-03,  2.3273e-03,  1.6503e-03,  ...,  5.2768e-01,\n",
       "            4.8684e-01, -1.2439e+00],\n",
       "          [ 1.3882e+00,  1.7134e+00,  8.9802e-01,  ..., -2.1540e+00,\n",
       "           -9.3947e-01,  1.9837e+00],\n",
       "          [ 1.6369e+00,  1.7261e-01,  1.6614e+00,  ..., -1.9611e+00,\n",
       "           -1.1987e+00,  2.5130e+00],\n",
       "          ...,\n",
       "          [ 1.0877e-01, -1.5142e+00, -1.1809e+00,  ..., -1.0813e+00,\n",
       "           -7.8105e-01,  1.4321e+00],\n",
       "          [ 1.1228e+00, -1.3996e+00,  1.2004e+00,  ..., -4.7271e-01,\n",
       "           -3.5189e+00,  2.8862e+00],\n",
       "          [ 1.2480e+00, -1.8155e+00,  2.1865e+00,  ..., -2.7727e+00,\n",
       "           -2.4917e+00,  2.4211e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.5980e-04,  1.0458e-03,  1.4083e-03,  ..., -1.2374e+00,\n",
       "            2.7323e+00, -7.6937e-01],\n",
       "          [ 1.6348e+00,  8.4620e-01,  2.0160e+00,  ..., -2.1993e+00,\n",
       "           -2.6378e+00, -1.6840e+00],\n",
       "          [ 1.8540e+00,  2.6581e-01,  5.2035e-01,  ..., -1.5772e+00,\n",
       "           -2.8238e+00, -1.6508e+00],\n",
       "          ...,\n",
       "          [-9.7886e-02, -5.7824e-01,  1.9495e+00,  ..., -5.0728e-01,\n",
       "           -4.2899e+00, -1.5833e+00],\n",
       "          [ 1.6315e+00, -1.4640e+00,  7.1732e-01,  ...,  8.0692e-01,\n",
       "           -3.7314e+00, -8.1993e-01],\n",
       "          [ 6.9951e-01,  1.4162e+00,  1.5396e+00,  ..., -2.1860e-01,\n",
       "           -4.1457e+00, -7.5709e-01]],\n",
       "\n",
       "         [[-1.1632e-03,  1.0648e-03,  8.9556e-06,  ..., -3.6119e-02,\n",
       "           -1.9077e+00, -6.2221e-01],\n",
       "          [-1.0530e+00, -1.9814e+00,  5.9192e-01,  ...,  6.7828e-01,\n",
       "            2.4505e+00, -5.3767e-01],\n",
       "          [ 2.0216e+00, -2.8767e+00,  9.5186e-01,  ..., -2.6391e-01,\n",
       "            2.9287e+00,  2.2278e-01],\n",
       "          ...,\n",
       "          [-5.6961e-01, -8.4622e-01, -1.2980e-01,  ...,  3.5228e-01,\n",
       "            4.0373e+00, -1.3265e+00],\n",
       "          [-6.9937e-01,  9.2559e-01,  2.6862e+00,  ..., -2.0402e+00,\n",
       "            4.0397e+00, -8.7009e-01],\n",
       "          [ 2.2266e+00,  2.6305e+00,  2.9172e+00,  ..., -2.1506e+00,\n",
       "            3.5148e+00, -3.8180e-01]],\n",
       "\n",
       "         [[-7.8837e-04,  8.1100e-04,  9.3667e-04,  ...,  6.8645e-01,\n",
       "            6.0794e-01,  4.8914e-01],\n",
       "          [ 2.7882e+00,  8.1199e-01,  3.3146e+00,  ...,  4.2637e-01,\n",
       "            1.2805e+00,  1.4633e+00],\n",
       "          [ 3.8152e+00,  2.4507e+00,  1.9064e+00,  ...,  3.8726e-01,\n",
       "            1.8277e+00,  1.0885e+00],\n",
       "          ...,\n",
       "          [ 1.3241e+00,  1.2770e+00,  3.9268e+00,  ..., -2.5768e-01,\n",
       "            1.4694e+00, -5.3993e-02],\n",
       "          [ 3.3154e+00,  8.7792e-01,  3.8230e+00,  ..., -2.9896e-01,\n",
       "            5.9941e-01, -5.7957e-01],\n",
       "          [ 2.1760e+00, -2.7965e-01,  3.3746e+00,  ..., -1.5343e-01,\n",
       "           -1.2562e-01,  1.3457e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 3.6597e-03, -3.1288e-04,  1.7606e-03,  ...,  4.4218e-03,\n",
       "           -8.9574e-03, -4.8680e-03],\n",
       "          [-2.8445e-02,  1.6650e-01,  1.1007e-01,  ...,  1.4139e-01,\n",
       "            2.4756e-01,  3.3708e-02],\n",
       "          [ 3.2231e-01,  1.0824e-01, -1.6637e-01,  ...,  1.0626e-01,\n",
       "           -1.4500e-01, -2.1649e-02],\n",
       "          ...,\n",
       "          [-4.5455e-01, -7.0616e-01, -5.5388e-02,  ...,  3.9905e-02,\n",
       "           -1.7565e-01, -4.3450e-01],\n",
       "          [-1.2633e-01, -1.3070e-01, -8.6620e-02,  ...,  2.1908e-01,\n",
       "           -2.2172e-01, -3.3999e-01],\n",
       "          [-4.4457e-03,  1.1281e-01, -6.3363e-02,  ...,  9.3192e-02,\n",
       "           -1.4104e-01, -3.6420e-02]],\n",
       "\n",
       "         [[-1.5877e-02, -1.6028e-02,  3.2577e-02,  ..., -1.8215e-01,\n",
       "            1.7538e-02, -2.5751e-03],\n",
       "          [ 5.7532e-02, -5.7330e-02, -1.1013e-01,  ...,  1.9255e-01,\n",
       "            1.4236e-03, -2.5454e-01],\n",
       "          [ 2.9940e-02,  2.1644e-02,  2.5523e-02,  ...,  3.8668e-01,\n",
       "            4.9649e-02,  3.1657e-01],\n",
       "          ...,\n",
       "          [ 3.1810e-01,  1.1240e-01,  1.9155e-01,  ...,  3.9107e-01,\n",
       "           -1.5316e-01, -6.3413e-02],\n",
       "          [-2.3600e-01,  5.1179e-01, -8.9812e-02,  ...,  1.1278e-01,\n",
       "            1.6251e-01,  7.8246e-02],\n",
       "          [-3.7477e-01,  3.8386e-01, -1.4457e-01,  ...,  1.8866e-01,\n",
       "            2.1190e-02, -3.2697e-02]],\n",
       "\n",
       "         [[ 4.4139e-02,  2.8367e-03,  2.8165e-03,  ...,  2.9903e-03,\n",
       "            1.0342e-03, -1.4339e-03],\n",
       "          [ 1.7983e-01,  4.3966e-01, -4.1618e-01,  ...,  1.3683e-01,\n",
       "            2.0745e-01, -2.2556e-01],\n",
       "          [ 7.7276e-01, -3.1514e-01, -1.7287e-01,  ..., -1.2378e-01,\n",
       "            3.5291e-01,  2.6739e-01],\n",
       "          ...,\n",
       "          [ 1.8404e-01, -3.3456e-02, -7.7374e-02,  ...,  1.1550e-01,\n",
       "           -2.9871e-01, -1.6503e-01],\n",
       "          [-7.8541e-02,  3.6615e-02, -1.8863e-01,  ...,  2.3140e-01,\n",
       "           -3.7406e-02,  1.3796e-01],\n",
       "          [-4.1126e-02,  3.4490e-01,  9.6546e-02,  ...,  6.0076e-02,\n",
       "           -3.6627e-02, -2.1516e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.9455e-03, -8.9592e-03, -5.3759e-03,  ...,  2.7458e-03,\n",
       "            1.5549e-03, -6.9722e-03],\n",
       "          [ 4.0600e-02,  2.7248e-02, -7.7042e-02,  ..., -1.7908e-01,\n",
       "            2.7178e-01, -2.4078e-02],\n",
       "          [-9.2309e-02,  1.2579e-02, -1.4709e-01,  ...,  5.9034e-02,\n",
       "            9.3968e-02, -7.3825e-02],\n",
       "          ...,\n",
       "          [-2.1100e-01, -3.0617e-01,  1.6130e-01,  ..., -3.2152e-01,\n",
       "           -4.9125e-02,  9.2351e-02],\n",
       "          [ 3.4050e-03,  9.3012e-02,  2.6337e-01,  ..., -2.8879e-01,\n",
       "           -1.3511e-01,  4.2227e-01],\n",
       "          [ 4.1070e-02,  6.5698e-02,  8.4477e-02,  ..., -3.4907e-01,\n",
       "           -1.1736e-01,  1.7156e-01]],\n",
       "\n",
       "         [[ 8.4791e-03,  3.0452e-02, -1.7605e-02,  ...,  5.7797e-02,\n",
       "            3.6964e-03, -3.7360e-01],\n",
       "          [ 2.8631e-01,  2.2230e-01, -4.0421e-01,  ...,  4.9766e-01,\n",
       "            1.0345e-01,  3.6512e-01],\n",
       "          [ 1.3382e-01,  4.5406e-01, -1.6154e-01,  ...,  7.3116e-03,\n",
       "            7.8789e-02,  4.0459e-01],\n",
       "          ...,\n",
       "          [-1.7396e-01,  2.8660e-02, -3.8158e-01,  ..., -1.1243e-01,\n",
       "            1.9788e-01,  3.2308e-01],\n",
       "          [-6.2921e-02,  5.2723e-02,  6.5201e-02,  ...,  1.3693e-01,\n",
       "            1.0502e-01, -9.2162e-02],\n",
       "          [ 2.8366e-01,  9.8358e-02,  4.7173e-01,  ...,  1.2519e-01,\n",
       "            1.8488e-02, -1.9419e-01]],\n",
       "\n",
       "         [[-1.1525e-02,  1.5667e-03,  3.0605e-04,  ...,  5.8588e-03,\n",
       "            7.9420e-04, -5.0355e-03],\n",
       "          [-8.1799e-02,  5.5343e-01,  6.6437e-02,  ...,  7.1668e-02,\n",
       "            2.8461e-01, -1.0545e-01],\n",
       "          [-2.8787e-01,  2.7460e-01, -1.0270e-01,  ..., -5.6739e-01,\n",
       "           -6.3680e-01,  3.5558e-01],\n",
       "          ...,\n",
       "          [ 4.2110e-02, -1.2830e-01,  3.0568e-01,  ..., -2.7821e-01,\n",
       "           -2.0516e-01, -2.4326e-01],\n",
       "          [-1.6827e-01, -2.0713e-01, -9.3006e-02,  ..., -1.9412e-01,\n",
       "           -4.7555e-01, -3.7421e-01],\n",
       "          [-6.4076e-02, -1.9888e-01,  2.4522e-01,  ..., -2.5340e-01,\n",
       "           -2.2848e-01,  2.4277e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.6281e-03,  2.2716e-04,  3.6045e-03,  ...,  4.4801e-01,\n",
       "            1.0434e+00, -2.6322e-01],\n",
       "          [-3.2651e+00, -5.8012e-01,  2.2282e+00,  ...,  2.1717e+00,\n",
       "           -4.2640e+00, -1.0199e+00],\n",
       "          [-1.3524e-01, -1.6688e+00,  2.2640e+00,  ...,  1.8536e+00,\n",
       "           -5.2835e+00, -5.9616e-01],\n",
       "          ...,\n",
       "          [-2.6305e+00, -8.2101e-01,  9.7909e-02,  ..., -3.8802e-01,\n",
       "           -5.9005e+00, -9.7464e-01],\n",
       "          [-2.2737e+00,  1.6504e-01,  2.2803e-01,  ..., -2.9531e+00,\n",
       "           -7.8354e+00, -1.5094e+00],\n",
       "          [ 1.0303e+00,  2.1922e+00,  1.1435e+00,  ..., -4.3701e-01,\n",
       "           -7.8495e+00, -9.7247e-01]],\n",
       "\n",
       "         [[-2.8135e-03, -2.1951e-04, -4.3992e-03,  ...,  5.3548e-01,\n",
       "            1.8182e-01, -2.5445e-01],\n",
       "          [-1.7112e+00, -1.9257e+00, -1.5644e+00,  ..., -3.6136e-01,\n",
       "            1.3912e-01,  2.2159e-01],\n",
       "          [-1.8144e+00, -9.6373e-01, -9.0370e-01,  ..., -1.9035e-01,\n",
       "            5.7561e-01,  6.6647e-01],\n",
       "          ...,\n",
       "          [-4.1901e-01,  2.1064e+00, -1.2768e+00,  ...,  7.5520e-01,\n",
       "           -5.9353e-01, -3.0200e-01],\n",
       "          [-1.4487e+00,  6.6708e-01, -2.9298e+00,  ...,  9.6813e-01,\n",
       "            1.5954e+00,  1.1959e+00],\n",
       "          [-7.5663e-01,  1.8551e+00, -2.8593e+00,  ...,  1.3148e+00,\n",
       "            8.4800e-01,  1.2999e+00]],\n",
       "\n",
       "         [[ 5.7473e-03,  4.4660e-03,  1.7073e-03,  ..., -4.1277e-01,\n",
       "            2.2779e+00, -1.8023e-01],\n",
       "          [ 1.3306e+00,  2.5286e+00,  1.3365e+00,  ..., -3.4286e+00,\n",
       "           -2.1139e+00, -3.1257e-01],\n",
       "          [-7.1766e-01,  2.4069e-01,  4.5694e-01,  ..., -4.5967e+00,\n",
       "           -4.1838e+00, -1.4324e+00],\n",
       "          ...,\n",
       "          [ 5.0383e-01, -5.3587e-01, -1.7113e+00,  ..., -4.1501e+00,\n",
       "           -7.0665e+00,  1.9792e+00],\n",
       "          [ 8.9744e-01, -1.9901e+00,  3.9541e-01,  ..., -2.8680e+00,\n",
       "           -7.6113e+00,  1.7983e-01],\n",
       "          [-2.7430e+00, -1.8496e+00,  1.2374e+00,  ..., -3.2634e+00,\n",
       "           -6.7054e+00, -2.9363e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.7909e-04, -3.9096e-03, -3.0374e-03,  ...,  2.8103e-02,\n",
       "           -2.5997e-01,  2.4376e-01],\n",
       "          [-3.7265e-01,  1.4530e-01, -2.5321e-01,  ...,  1.2626e+00,\n",
       "            3.8383e+00, -4.6857e-01],\n",
       "          [-1.1682e+00,  5.9577e-01,  4.9789e-01,  ...,  2.8676e+00,\n",
       "            5.5475e-01, -7.9241e-01],\n",
       "          ...,\n",
       "          [-1.8524e-01,  3.4616e-01, -8.9942e-01,  ...,  1.0610e+00,\n",
       "            3.9365e+00, -1.5694e+00],\n",
       "          [-2.8107e-01,  1.2977e-01, -1.4268e+00,  ..., -1.1401e+00,\n",
       "            2.7559e+00, -5.1153e+00],\n",
       "          [-5.1696e-01, -4.0829e-01, -8.0996e-01,  ...,  1.3520e+00,\n",
       "            3.0863e+00, -2.9319e+00]],\n",
       "\n",
       "         [[-1.3783e-03, -1.1514e-03, -2.7292e-04,  ..., -9.3934e-01,\n",
       "            9.6075e-01, -9.8795e-01],\n",
       "          [-4.9221e-01,  3.7406e-01,  4.3129e-01,  ...,  2.8819e+00,\n",
       "            4.7065e-02,  3.2826e+00],\n",
       "          [-6.1157e-01,  9.2581e-01, -2.4191e-02,  ...,  2.1374e+00,\n",
       "           -1.7513e+00,  6.3138e+00],\n",
       "          ...,\n",
       "          [-2.3511e-01, -1.3697e-01,  7.7356e-01,  ...,  3.5457e+00,\n",
       "           -3.4299e+00,  6.5187e+00],\n",
       "          [-2.3207e-02, -1.0246e-01,  1.0846e+00,  ...,  2.9958e+00,\n",
       "           -3.2495e+00,  7.8547e+00],\n",
       "          [-2.8555e-01, -5.3480e-01,  4.0422e-01,  ...,  9.3503e-01,\n",
       "           -3.7156e+00,  5.2153e+00]],\n",
       "\n",
       "         [[-2.9349e-03,  1.1602e-03, -6.6653e-03,  ..., -5.5864e-02,\n",
       "            1.9586e+00,  6.6079e-01],\n",
       "          [ 6.0367e-01,  9.5062e-02, -5.8581e-01,  ...,  1.0258e+00,\n",
       "           -1.9140e+00,  2.7114e+00],\n",
       "          [ 1.8866e+00,  2.4644e+00, -5.7484e-01,  ...,  6.3845e-01,\n",
       "           -1.3129e+00,  2.4944e+00],\n",
       "          ...,\n",
       "          [-9.3112e-01,  1.6703e+00,  1.9144e+00,  ..., -7.5418e-01,\n",
       "           -1.2049e+00,  8.0479e-01],\n",
       "          [ 1.2702e+00,  1.2578e+00, -2.6039e-01,  ..., -9.1871e-03,\n",
       "           -1.5224e+00, -1.1894e+00],\n",
       "          [ 1.2903e+00,  2.2398e-01, -2.2255e+00,  ...,  7.2132e-01,\n",
       "           -1.8766e+00,  3.2729e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 6.3097e-03, -1.8348e-04,  5.6857e-03,  ...,  5.8095e-03,\n",
       "            3.6659e-04, -3.9060e-03],\n",
       "          [ 3.1218e-01, -6.6914e-02, -8.0432e-02,  ...,  3.1226e-01,\n",
       "            1.2401e-01, -2.7126e-01],\n",
       "          [-7.9736e-01, -9.3116e-01, -6.8761e-01,  ..., -3.7818e-01,\n",
       "           -2.1613e-01,  8.4663e-02],\n",
       "          ...,\n",
       "          [ 1.5932e-01,  5.2840e-02,  3.2146e-01,  ...,  1.8374e-01,\n",
       "           -1.0557e-01, -1.0472e-01],\n",
       "          [ 2.2546e-01, -1.9295e-02,  1.1814e-02,  ...,  2.2537e-01,\n",
       "           -6.0728e-02, -1.6586e-01],\n",
       "          [ 7.3686e-02,  1.8224e-02,  1.4999e-01,  ...,  1.7765e-02,\n",
       "           -2.7892e-03, -6.4934e-03]],\n",
       "\n",
       "         [[ 3.0925e-01,  1.3482e-02, -3.7614e-03,  ...,  1.0827e-02,\n",
       "           -7.5751e-03,  2.4904e-03],\n",
       "          [-7.6181e-01,  2.4516e-01, -2.5515e-02,  ...,  3.5891e-01,\n",
       "            2.0716e-01,  3.2191e-01],\n",
       "          [-1.0187e+00,  1.5468e-01,  9.8694e-03,  ...,  2.1994e-01,\n",
       "            2.5473e-01,  1.0463e-01],\n",
       "          ...,\n",
       "          [-5.6244e-01, -3.0081e-01, -1.2153e-01,  ..., -2.3745e-01,\n",
       "            3.2327e-01,  4.3092e-01],\n",
       "          [-1.1732e+00, -2.9170e-02, -1.0943e-01,  ..., -2.3750e-01,\n",
       "            2.1477e-01, -9.4553e-02],\n",
       "          [-8.3985e-01, -1.9852e-01,  9.4707e-02,  ...,  9.5510e-04,\n",
       "           -1.2189e-01, -8.1274e-02]],\n",
       "\n",
       "         [[ 1.9334e-02,  5.6107e-03,  9.7229e-03,  ...,  1.2459e-02,\n",
       "           -3.5230e-03, -2.1104e-01],\n",
       "          [-1.1567e-01,  2.0347e-01, -4.4147e-01,  ..., -4.4115e-01,\n",
       "            2.1180e-01,  2.5644e-02],\n",
       "          [-3.6477e-01, -1.6985e-01,  8.2599e-02,  ..., -5.4772e-02,\n",
       "            1.7694e-01,  1.1811e-01],\n",
       "          ...,\n",
       "          [-1.0607e-01, -1.6411e-01, -1.9486e-01,  ...,  8.5386e-02,\n",
       "           -4.6318e-02,  2.7228e-01],\n",
       "          [-4.7892e-03,  2.1248e-01,  4.3710e-01,  ..., -2.7834e-02,\n",
       "            2.1656e-01,  4.5254e-01],\n",
       "          [-7.0519e-02,  8.9305e-02,  5.1862e-01,  ..., -3.4894e-02,\n",
       "            9.3133e-02,  2.1107e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.1064e-03, -2.1467e-04, -2.7597e-03,  ...,  4.4274e-03,\n",
       "           -1.8910e-04,  3.1785e-03],\n",
       "          [-3.0289e-01, -1.8001e-01, -6.7021e-01,  ..., -2.4722e-01,\n",
       "           -2.2209e-01, -3.7650e-01],\n",
       "          [ 3.7524e-01, -3.2295e-01,  2.5968e-01,  ..., -3.5641e-01,\n",
       "            5.6447e-01, -9.1799e-01],\n",
       "          ...,\n",
       "          [ 4.5253e-01,  2.2449e-01, -2.7347e-01,  ..., -4.3618e-01,\n",
       "            1.9388e-01,  2.7735e-01],\n",
       "          [ 3.7987e-01, -3.3551e-01, -2.4776e-01,  ...,  4.3961e-01,\n",
       "            1.2057e-01, -1.7240e-01],\n",
       "          [ 1.8647e-01, -6.0344e-01,  1.2645e-01,  ...,  6.3680e-01,\n",
       "           -4.9271e-01,  3.1158e-01]],\n",
       "\n",
       "         [[-9.6115e-03,  1.9821e-03, -5.8643e-03,  ...,  7.0952e-03,\n",
       "            9.4087e-03,  1.7801e-02],\n",
       "          [ 1.0717e-01, -3.0373e-01,  4.4768e-01,  ..., -1.8178e-01,\n",
       "           -2.7810e-01, -3.4457e-02],\n",
       "          [ 4.0475e-01, -3.2055e-01,  2.4536e-01,  ..., -3.0982e-01,\n",
       "           -1.8991e-01, -1.3435e-01],\n",
       "          ...,\n",
       "          [ 3.8937e-02,  1.5957e-01, -1.3987e-01,  ...,  3.6186e-02,\n",
       "            6.1862e-02, -2.1149e-01],\n",
       "          [ 6.2310e-02, -3.4711e-01, -2.1063e-01,  ...,  1.0641e-01,\n",
       "            1.1848e-01, -3.7162e-01],\n",
       "          [-5.7034e-02, -6.2210e-02,  2.8005e-02,  ...,  6.2642e-02,\n",
       "            1.6585e-01, -2.7096e-01]],\n",
       "\n",
       "         [[ 1.1306e-02, -3.8496e-03,  6.5174e-03,  ..., -1.4817e-03,\n",
       "           -5.1827e-03,  2.2456e-03],\n",
       "          [ 1.9820e-01, -4.5308e-02,  6.3135e-02,  ..., -2.3062e-01,\n",
       "           -1.8817e-01, -3.6178e-03],\n",
       "          [ 3.4628e-02, -8.9774e-02,  2.9032e-01,  ..., -2.3073e-01,\n",
       "            3.8487e-03, -5.8356e-02],\n",
       "          ...,\n",
       "          [ 3.7414e-02, -4.9124e-02,  2.1052e-01,  ...,  2.1316e-01,\n",
       "            2.0631e-01,  2.3755e-01],\n",
       "          [ 1.1549e-02,  2.3287e-02,  1.0899e-01,  ...,  1.0988e-01,\n",
       "           -5.8619e-02,  5.1621e-02],\n",
       "          [-3.2853e-02, -1.5742e-01,  2.2176e-01,  ...,  9.8953e-02,\n",
       "           -4.8233e-02, -6.5791e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.8565e-03, -3.7440e-03, -2.9656e-03,  ...,  6.3688e-01,\n",
       "            1.5142e+00, -7.5226e-01],\n",
       "          [ 5.6220e-01,  2.8783e-02, -1.5322e+00,  ...,  3.6634e-01,\n",
       "           -5.7853e-01, -7.3925e-01],\n",
       "          [ 2.2902e-02,  1.7372e-01,  8.2260e-02,  ...,  9.5466e-01,\n",
       "           -1.0523e+00, -1.4759e+00],\n",
       "          ...,\n",
       "          [ 4.7296e-01, -2.8916e-01, -1.4034e+00,  ...,  9.4559e-01,\n",
       "           -8.8609e-01, -8.8237e-01],\n",
       "          [ 1.2087e+00,  8.7015e-02, -8.4629e-01,  ...,  1.2083e+00,\n",
       "           -2.2528e+00, -1.3022e-01],\n",
       "          [ 1.2150e-01,  7.0593e-01,  3.6678e-01,  ...,  7.6613e-01,\n",
       "           -1.8737e+00, -1.3602e+00]],\n",
       "\n",
       "         [[-1.0625e-02,  3.3912e-03, -3.7628e-03,  ...,  3.9253e-01,\n",
       "            2.0303e-01, -1.8557e+00],\n",
       "          [ 8.3170e-02,  9.9446e-02,  6.8518e-01,  ...,  3.3575e+00,\n",
       "           -3.6018e+00,  1.8853e+00],\n",
       "          [ 5.1984e-01,  4.2740e-01,  1.0457e+00,  ...,  1.6414e+00,\n",
       "           -3.8537e+00,  6.8422e-01],\n",
       "          ...,\n",
       "          [-1.2980e-01, -7.3054e-02,  3.8731e-01,  ..., -3.9518e-01,\n",
       "           -3.1534e-01, -1.0259e+00],\n",
       "          [ 1.8203e-01, -3.1851e-01,  1.1732e+00,  ...,  1.6766e+00,\n",
       "           -1.6643e+00,  4.7939e-01],\n",
       "          [ 5.6546e-01,  3.2425e-01,  1.2384e+00,  ...,  1.0948e+00,\n",
       "           -7.2710e-01,  1.7026e+00]],\n",
       "\n",
       "         [[ 1.4249e-03,  1.9980e-03,  6.0889e-03,  ...,  8.9142e-01,\n",
       "           -3.8012e-01, -5.2819e-01],\n",
       "          [ 4.6367e+00, -3.6086e+00,  3.7885e+00,  ...,  1.6935e+00,\n",
       "            7.1263e-01,  9.9292e-01],\n",
       "          [ 5.9872e+00, -4.3533e+00,  2.9145e+00,  ...,  2.4048e+00,\n",
       "            1.2186e+00,  1.1147e+00],\n",
       "          ...,\n",
       "          [ 4.4878e+00, -2.9455e+00,  3.1977e+00,  ...,  1.6626e+00,\n",
       "            2.6344e+00,  8.1811e-01],\n",
       "          [ 4.9803e+00,  2.1167e+00,  2.5350e+00,  ...,  2.5198e+00,\n",
       "           -6.1119e-01,  4.6944e-01],\n",
       "          [ 3.6673e+00,  2.9868e+00,  3.1649e+00,  ...,  1.6105e+00,\n",
       "            1.3472e-01,  8.8826e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.6322e-03,  2.2711e-03,  1.7356e-04,  ..., -2.9528e-01,\n",
       "            2.0545e-01,  2.6448e+00],\n",
       "          [-5.6588e-01,  9.8525e-01, -9.5894e-01,  ...,  2.8993e+00,\n",
       "           -1.8860e-01, -1.2689e+00],\n",
       "          [-1.8685e+00,  1.8020e+00, -3.4576e-01,  ...,  2.5488e+00,\n",
       "            2.8602e+00, -2.6235e+00],\n",
       "          ...,\n",
       "          [-5.7853e-01,  9.7295e-02, -3.5832e-01,  ..., -1.7277e-01,\n",
       "            1.5481e+00, -4.2226e+00],\n",
       "          [-1.2165e+00, -1.5080e+00, -3.1309e-01,  ..., -1.1045e+00,\n",
       "            2.4722e+00, -4.8735e+00],\n",
       "          [-2.0026e+00, -3.0649e+00,  1.0921e+00,  ...,  7.0143e-02,\n",
       "            1.4503e+00, -4.7091e+00]],\n",
       "\n",
       "         [[-4.8767e-03,  6.4550e-03, -3.2505e-03,  ...,  6.5673e-01,\n",
       "            5.0222e-01,  8.6677e-01],\n",
       "          [-3.0709e-01,  1.0917e+00, -1.6617e+00,  ..., -1.5261e+00,\n",
       "            1.2196e+00,  2.4801e+00],\n",
       "          [ 1.6400e+00, -1.2649e+00, -2.5183e+00,  ..., -1.3221e+00,\n",
       "           -1.4119e+00,  3.1028e+00],\n",
       "          ...,\n",
       "          [-2.2401e-01, -1.0721e-02,  2.8396e-01,  ..., -3.6977e+00,\n",
       "           -1.1857e+00,  2.5173e+00],\n",
       "          [ 5.2239e-01, -1.2734e+00,  1.2346e-01,  ..., -3.0614e+00,\n",
       "           -2.7854e-01, -2.5556e-01],\n",
       "          [ 2.8137e+00, -5.7467e-01,  1.1198e+00,  ..., -1.5655e+00,\n",
       "            2.1236e-01, -4.5634e-01]],\n",
       "\n",
       "         [[ 7.7845e-03, -3.6307e-03,  4.4008e-03,  ..., -3.4767e-01,\n",
       "            1.6494e+00,  5.8524e-01],\n",
       "          [ 2.3518e+00, -3.0290e+00,  1.5578e+00,  ..., -4.3598e-01,\n",
       "           -4.6440e+00, -1.9598e-01],\n",
       "          [-1.8369e+00, -3.5634e+00,  1.1906e+00,  ..., -5.7637e-01,\n",
       "           -6.1582e+00,  1.1808e+00],\n",
       "          ...,\n",
       "          [ 3.4354e+00, -3.7365e-01, -1.5735e+00,  ...,  7.2128e-01,\n",
       "           -5.7117e+00, -1.3123e-01],\n",
       "          [ 1.6679e+00,  7.8410e-01,  2.5828e+00,  ..., -6.4087e-01,\n",
       "           -5.1002e+00, -3.6651e-01],\n",
       "          [-3.6438e+00,  2.8759e+00,  4.1919e+00,  ..., -1.0022e+00,\n",
       "           -4.8840e+00, -7.0992e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[-1.4197e-02,  1.3684e-02,  1.8541e-02,  ...,  5.6899e-03,\n",
       "           -7.0199e-03, -1.4330e-02],\n",
       "          [-1.3855e-01, -3.6371e-01, -4.3483e-01,  ..., -1.5546e-01,\n",
       "            6.3348e-02,  1.6634e-01],\n",
       "          [ 9.5363e-02, -3.6619e-01, -3.9152e-01,  ..., -4.3708e-01,\n",
       "            7.3246e-02,  7.6234e-01],\n",
       "          ...,\n",
       "          [-6.7240e-02,  9.6998e-02, -1.0619e-02,  ..., -5.6690e-04,\n",
       "            3.8386e-01, -1.5111e-01],\n",
       "          [ 2.9808e-01,  2.2555e-01, -1.8925e-01,  ...,  1.0719e-01,\n",
       "            2.2311e-01,  1.4255e-01],\n",
       "          [ 5.6716e-02,  7.3681e-02, -1.4821e-01,  ...,  1.3315e-02,\n",
       "            2.9260e-02,  1.4253e-01]],\n",
       "\n",
       "         [[ 1.0248e-03,  1.9342e-03,  4.5169e-03,  ..., -1.1090e-03,\n",
       "           -6.8087e-04,  7.6106e-04],\n",
       "          [ 2.3023e-01,  4.7861e-02,  1.4015e-01,  ...,  4.1005e-01,\n",
       "            1.6796e-01, -1.4324e-01],\n",
       "          [-2.9930e-01, -3.8351e-01, -3.3708e-01,  ...,  2.3792e-01,\n",
       "            5.1055e-03, -7.0085e-01],\n",
       "          ...,\n",
       "          [-4.7399e-01, -8.2303e-02, -1.7924e-01,  ..., -2.6630e-01,\n",
       "           -3.3005e-01, -5.2768e-02],\n",
       "          [-2.3637e-01,  5.6527e-02, -2.9287e-01,  ..., -1.3409e-01,\n",
       "           -3.3546e-01,  2.7129e-02],\n",
       "          [-1.1570e-01,  1.2755e-01, -1.9740e-01,  ...,  1.1518e-01,\n",
       "           -7.4867e-02,  1.3297e-02]],\n",
       "\n",
       "         [[-1.4074e-01, -2.6273e-02,  4.4732e-02,  ..., -8.0925e-01,\n",
       "            1.0989e-02,  9.3855e-03],\n",
       "          [ 2.8278e-02,  3.3608e-01, -2.3049e-01,  ...,  9.0066e-01,\n",
       "            7.9296e-02, -1.3943e-02],\n",
       "          [ 6.7363e-02,  6.0982e-01,  3.8330e-01,  ...,  4.7183e-01,\n",
       "            1.3169e-02, -1.9645e-01],\n",
       "          ...,\n",
       "          [-4.9262e-01, -3.0508e-01, -1.7747e-01,  ...,  2.9511e-01,\n",
       "           -1.8151e-01,  8.6800e-02],\n",
       "          [-2.8336e-01,  3.1533e-02, -4.2332e-01,  ...,  7.0817e-01,\n",
       "            1.5526e-01, -2.3744e-01],\n",
       "          [-8.0430e-01, -2.5979e-02, -3.1481e-01,  ...,  7.8041e-01,\n",
       "            1.5907e-01, -1.3703e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.0519e-03, -2.3875e-03, -4.5966e-03,  ..., -5.2711e-03,\n",
       "           -6.2076e-03, -1.6003e-03],\n",
       "          [-2.0109e-01,  2.7022e-02,  2.1602e-01,  ...,  3.6519e-01,\n",
       "            1.5476e-01, -1.7836e-01],\n",
       "          [-3.6345e-02,  1.6654e-01,  2.3394e-01,  ..., -3.7109e-02,\n",
       "           -3.0537e-01, -7.2763e-02],\n",
       "          ...,\n",
       "          [ 1.2266e-01,  1.0013e-02, -2.2598e-01,  ..., -2.8328e-03,\n",
       "           -9.4528e-02, -5.2236e-01],\n",
       "          [ 2.6892e-01,  7.4459e-02, -1.6135e-02,  ...,  1.8728e-02,\n",
       "            6.9597e-02, -2.4468e-01],\n",
       "          [ 2.6902e-01,  1.4999e-02, -1.0944e-02,  ..., -2.4565e-02,\n",
       "            1.2343e-02,  7.6667e-03]],\n",
       "\n",
       "         [[ 2.2852e-01, -2.6833e-04,  1.4082e-03,  ...,  1.1648e-03,\n",
       "           -1.2230e-03,  9.2826e-03],\n",
       "          [-3.4411e-01,  3.4783e-01,  3.4699e-01,  ...,  3.8296e-01,\n",
       "           -1.4553e-01,  2.0721e-01],\n",
       "          [ 1.7243e-02,  2.0080e-01,  4.9736e-02,  ...,  1.1598e-01,\n",
       "           -2.1190e-01,  1.0413e-01],\n",
       "          ...,\n",
       "          [-3.4599e-01, -2.9350e-01,  4.5658e-03,  ..., -2.7853e-01,\n",
       "           -7.1330e-02, -3.3428e-01],\n",
       "          [-2.4282e-01, -3.2960e-01, -2.8639e-01,  ...,  2.5125e-01,\n",
       "            1.2257e-01, -2.5463e-01],\n",
       "          [-3.2475e-01, -2.3359e-01, -1.2196e-01,  ..., -2.0782e-02,\n",
       "           -3.5363e-02, -1.2506e-01]],\n",
       "\n",
       "         [[-3.6994e-03, -7.4681e-03, -6.8788e-03,  ...,  4.8175e-04,\n",
       "           -3.3238e-03, -1.5851e-03],\n",
       "          [ 4.1747e-01,  9.5432e-02,  3.4305e-01,  ...,  3.4459e-01,\n",
       "           -5.3171e-02,  6.4776e-02],\n",
       "          [ 2.9232e-01, -2.7465e-01,  1.0257e-01,  ...,  7.1296e-01,\n",
       "            3.6466e-01, -9.6308e-02],\n",
       "          ...,\n",
       "          [-6.0432e-01,  7.3083e-01,  1.8311e-01,  ...,  1.3260e+00,\n",
       "           -2.4257e-01,  5.6598e-01],\n",
       "          [-1.1399e+00,  1.9028e-02,  1.3391e-01,  ..., -5.8613e-01,\n",
       "           -2.1052e-01, -7.0767e-02],\n",
       "          [-4.6491e-01,  2.6160e-03, -1.2186e-01,  ..., -8.2961e-01,\n",
       "           -1.1159e-01, -3.9520e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 5.6230e-03, -3.1609e-03, -1.9298e-03,  ..., -7.9872e-02,\n",
       "           -7.0134e-01, -5.4086e-01],\n",
       "          [ 1.8024e+00, -2.5505e+00, -1.1917e+00,  ...,  1.6192e+00,\n",
       "            2.0211e+00,  1.4266e+00],\n",
       "          [-1.3753e+00, -3.4489e+00, -2.5372e+00,  ..., -1.0801e+00,\n",
       "            3.2852e+00,  1.4309e+00],\n",
       "          ...,\n",
       "          [ 2.5040e+00, -2.6511e+00,  1.4171e-02,  ..., -4.9499e-01,\n",
       "            3.2884e+00,  1.5421e+00],\n",
       "          [ 3.7900e-01, -1.2847e-02, -6.0471e-01,  ..., -2.1749e+00,\n",
       "            3.0753e+00,  1.9097e+00],\n",
       "          [-2.9655e+00,  2.2211e+00, -8.6851e-01,  ..., -1.3804e+00,\n",
       "            2.6879e+00,  1.2866e+00]],\n",
       "\n",
       "         [[-1.5449e-05, -2.7000e-03, -1.3587e-03,  ..., -1.8039e-01,\n",
       "            4.7414e-01, -1.3135e-01],\n",
       "          [ 6.6690e-01,  5.7666e-01,  5.9574e-02,  ...,  5.5745e-03,\n",
       "           -1.8358e+00, -2.0792e+00],\n",
       "          [ 5.6808e-01, -2.7580e-01, -5.9493e-01,  ..., -1.6805e-01,\n",
       "            1.2851e-01, -2.0138e+00],\n",
       "          ...,\n",
       "          [ 1.1056e-01, -6.5539e-01,  5.7459e-01,  ..., -3.3005e-01,\n",
       "           -6.2506e-01, -2.1608e+00],\n",
       "          [ 1.4680e+00, -3.3106e-01,  1.1695e-01,  ...,  2.9459e-01,\n",
       "           -5.9252e-01, -1.6711e+00],\n",
       "          [ 2.0310e+00,  2.1955e-02,  2.5342e-01,  ..., -1.8120e-01,\n",
       "           -1.2987e+00, -2.2728e+00]],\n",
       "\n",
       "         [[-6.0459e-03, -6.1125e-04, -5.8422e-03,  ..., -3.7662e-01,\n",
       "           -2.9969e-01, -3.5939e-02],\n",
       "          [-3.7776e-01,  6.3918e-01, -9.7698e-01,  ..., -6.7920e-01,\n",
       "           -4.6210e-01, -9.5397e-01],\n",
       "          [ 1.2389e+00, -4.6299e-01, -2.6031e+00,  ...,  4.0178e-02,\n",
       "           -1.5195e+00, -3.8548e-01],\n",
       "          ...,\n",
       "          [-6.3825e-01, -1.4816e+00, -1.3403e-01,  ...,  1.0040e-02,\n",
       "            3.1200e-01,  3.8424e-01],\n",
       "          [ 8.9833e-01, -1.0664e+00, -7.3291e-01,  ...,  1.1939e+00,\n",
       "           -8.2049e-01,  3.1419e-01],\n",
       "          [ 2.1549e+00, -2.7186e-02, -1.7539e+00,  ...,  1.6145e+00,\n",
       "            5.0765e-01, -3.0783e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.9396e-04, -1.0700e-02, -3.7353e-03,  ..., -1.9633e-01,\n",
       "            1.3862e+00,  8.3120e-01],\n",
       "          [-1.2683e+00, -2.4584e+00, -2.1370e+00,  ...,  7.8861e-01,\n",
       "           -5.8841e+00, -2.5667e-02],\n",
       "          [-2.5732e+00, -1.3099e+00, -1.6893e+00,  ..., -1.6174e-01,\n",
       "           -5.8177e+00,  4.0506e-01],\n",
       "          ...,\n",
       "          [ 1.5683e-01,  6.5494e-01, -2.2110e+00,  ...,  1.3252e+00,\n",
       "           -4.2659e+00, -1.4054e+00],\n",
       "          [-1.6910e+00,  2.6495e+00, -2.1360e+00,  ...,  8.2281e-01,\n",
       "           -6.5915e+00,  1.1492e+00],\n",
       "          [-2.4191e+00,  3.2241e+00, -2.0478e+00,  ..., -1.6872e+00,\n",
       "           -5.0321e+00,  4.0075e-01]],\n",
       "\n",
       "         [[ 9.0467e-03,  7.3118e-03,  1.8248e-03,  ...,  1.6213e+00,\n",
       "           -4.0108e-01,  1.0857e+00],\n",
       "          [ 1.2594e+00,  6.3079e-02,  5.2532e-01,  ..., -3.6076e+00,\n",
       "           -3.5712e-02, -1.1456e+00],\n",
       "          [ 1.2429e+00,  4.2231e-01,  9.5391e-01,  ..., -3.1768e+00,\n",
       "            5.1087e-01, -2.8757e+00],\n",
       "          ...,\n",
       "          [ 1.4176e-01,  7.0349e-01, -1.0719e+00,  ..., -1.2333e+00,\n",
       "           -1.8016e+00, -4.0905e+00],\n",
       "          [ 2.5991e+00,  1.1890e-01, -4.6159e-01,  ..., -3.4923e+00,\n",
       "            8.2775e-01, -3.2388e+00],\n",
       "          [ 7.4519e-01,  8.1898e-01,  2.4854e+00,  ..., -4.7440e+00,\n",
       "            4.1671e+00, -2.9641e+00]],\n",
       "\n",
       "         [[-9.9970e-05, -2.6867e-03, -7.1347e-03,  ...,  1.0340e+00,\n",
       "            1.3160e+00,  4.4176e-02],\n",
       "          [ 4.6919e-01, -6.5151e-01, -1.9228e+00,  ..., -1.1802e+00,\n",
       "           -1.5890e+00, -3.3428e-01],\n",
       "          [ 1.6108e+00, -1.5255e+00,  2.6549e-02,  ..., -2.1546e+00,\n",
       "           -2.7748e+00,  1.6875e-01],\n",
       "          ...,\n",
       "          [-2.1242e-01,  7.1850e-03, -1.1274e+00,  ..., -3.5151e+00,\n",
       "           -3.2353e+00, -1.7956e+00],\n",
       "          [ 1.7611e+00,  1.8315e-01, -1.8215e+00,  ..., -2.9375e+00,\n",
       "           -3.9537e+00, -1.4219e+00],\n",
       "          [ 9.8242e-01,  5.9695e-01, -1.4158e+00,  ..., -2.7756e+00,\n",
       "           -2.9714e+00, -1.2000e-01]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 0.0167,  0.0132,  0.0205,  ...,  0.0317,  0.0083,  0.0262],\n",
       "          [ 0.1211,  0.0946,  0.1165,  ..., -0.4519,  0.1568,  0.2995],\n",
       "          [-0.2714,  0.3464,  0.0958,  ..., -0.5251, -0.2011, -0.4635],\n",
       "          ...,\n",
       "          [ 0.3313,  0.0370, -0.0122,  ..., -0.0962,  0.0437, -0.5070],\n",
       "          [-0.0539,  0.1401, -0.3776,  ..., -0.2518, -0.1956, -0.0987],\n",
       "          [ 0.0097,  0.0946, -0.4823,  ...,  0.0322, -0.1559, -0.1474]],\n",
       "\n",
       "         [[-0.0279, -0.0109,  0.0020,  ...,  0.0201,  0.0029,  0.0093],\n",
       "          [ 0.2919,  0.1309,  0.1128,  ...,  0.2234, -0.2068, -0.6482],\n",
       "          [ 0.2065,  0.5335, -0.2968,  ..., -0.0741, -0.0060, -0.6225],\n",
       "          ...,\n",
       "          [ 0.5393,  0.1342, -0.0127,  ..., -0.4150, -0.1667, -0.5028],\n",
       "          [-0.0686,  0.3687,  0.0425,  ..., -0.0419,  0.1457, -0.0941],\n",
       "          [ 0.0724,  0.0259,  0.1323,  ..., -0.1561, -0.0612, -0.0711]],\n",
       "\n",
       "         [[-0.0056, -0.0012, -0.0018,  ..., -0.0068,  0.0309, -0.0041],\n",
       "          [ 0.0312,  0.2329, -0.0490,  ..., -0.1148,  0.2454, -0.0285],\n",
       "          [ 0.2395,  0.3765,  0.1420,  ..., -0.1611,  0.5390, -0.0277],\n",
       "          ...,\n",
       "          [ 0.2242, -0.0243, -0.0661,  ..., -0.0591,  0.2580,  0.1806],\n",
       "          [ 0.1617,  0.0351, -0.0842,  ..., -0.0336,  0.3628, -0.0609],\n",
       "          [ 0.0992,  0.2046, -0.1046,  ..., -0.0425,  0.1788, -0.0414]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0030, -0.0119,  0.0091,  ..., -0.0092,  0.0199, -0.0047],\n",
       "          [-0.0375,  0.4086,  0.1100,  ...,  0.2078,  0.0181,  0.1245],\n",
       "          [ 0.1451,  0.3539,  0.1698,  ...,  0.1129, -0.1397,  0.2660],\n",
       "          ...,\n",
       "          [ 0.2311,  0.4293,  0.3480,  ..., -0.1669,  0.0678, -0.1492],\n",
       "          [-0.3029,  0.0463,  0.7179,  ..., -0.6294, -0.3951, -0.1003],\n",
       "          [ 0.0997,  0.2699,  0.1181,  ..., -0.0587,  0.0578, -0.0935]],\n",
       "\n",
       "         [[ 0.0134,  0.0008, -0.0022,  ..., -0.0010, -0.0012, -0.0049],\n",
       "          [ 0.0671,  0.3228,  0.0871,  ...,  0.4840, -0.0548, -0.0776],\n",
       "          [ 0.0117,  0.0836,  0.2338,  ...,  0.0781,  0.0957, -0.0929],\n",
       "          ...,\n",
       "          [-0.1828,  0.0544,  0.4339,  ...,  0.0073,  0.4241,  0.3053],\n",
       "          [ 0.0441,  0.1060,  0.3979,  ...,  0.0834,  0.1563,  0.0840],\n",
       "          [ 0.3031,  0.0012,  0.3888,  ...,  0.0028, -0.1321,  0.0459]],\n",
       "\n",
       "         [[-0.3288,  0.0018, -0.0043,  ..., -0.0064, -0.0015,  0.0062],\n",
       "          [ 0.6705,  0.2359, -0.2838,  ...,  0.0790,  0.2508,  0.0754],\n",
       "          [ 0.7097,  0.2889, -0.5277,  ...,  0.3516, -0.1196, -0.0524],\n",
       "          ...,\n",
       "          [ 0.4079, -0.0899, -0.1789,  ...,  0.2807,  0.1293, -0.1553],\n",
       "          [ 0.3324, -0.0200, -0.1086,  ...,  0.3308, -0.0324, -0.0980],\n",
       "          [ 0.5128, -0.0492, -0.1611,  ...,  0.3042, -0.1249,  0.1640]]]],\n",
       "       device='cuda:0', grad_fn=<TransposeBackward0>)), (tensor([[[[-4.1479e-03,  2.6018e-03, -4.8375e-03,  ..., -1.2629e-02,\n",
       "           -1.2070e+00,  8.2598e-01],\n",
       "          [-1.2209e-01,  1.4067e+00, -5.1542e-01,  ..., -2.2333e+00,\n",
       "            1.6881e+00, -1.0777e+00],\n",
       "          [ 2.1409e-01,  8.9767e-01, -7.6354e-01,  ..., -2.2735e+00,\n",
       "            1.4914e+00, -1.8501e+00],\n",
       "          ...,\n",
       "          [ 1.2251e-01, -3.7254e-01, -3.9970e-01,  ..., -1.3528e+00,\n",
       "            1.7417e-01, -2.1718e+00],\n",
       "          [ 5.0979e-01, -7.7408e-01, -5.0118e-01,  ...,  5.0118e-02,\n",
       "            1.6451e+00, -3.1755e+00],\n",
       "          [ 1.0227e+00, -5.9505e-01, -1.0463e-01,  ...,  3.9174e-01,\n",
       "            1.9883e+00, -2.5455e+00]],\n",
       "\n",
       "         [[-1.0649e-03,  3.6985e-03, -4.7033e-04,  ..., -3.1617e-01,\n",
       "            1.2458e+00, -2.9902e-02],\n",
       "          [ 1.8417e-01, -8.0864e-01, -1.9713e+00,  ...,  1.6258e+00,\n",
       "           -7.3110e-01,  8.7997e-01],\n",
       "          [ 1.7754e+00,  8.2775e-01, -1.1326e-01,  ...,  2.2782e+00,\n",
       "           -1.3348e-01,  1.6581e-01],\n",
       "          ...,\n",
       "          [-1.5489e-01,  8.9879e-01, -5.2141e-01,  ..., -3.3879e-01,\n",
       "           -1.0462e+00,  3.9779e-01],\n",
       "          [ 1.1613e+00,  2.0570e+00, -2.7442e+00,  ...,  1.0159e+00,\n",
       "           -4.7821e-01, -1.4435e+00],\n",
       "          [ 1.7265e+00,  8.5418e-01, -1.3972e+00,  ...,  5.7247e-01,\n",
       "            4.1111e-01, -1.4021e+00]],\n",
       "\n",
       "         [[-1.0327e-02,  6.6609e-03, -3.5683e-04,  ..., -1.1958e+00,\n",
       "           -1.3145e+00,  1.2583e+00],\n",
       "          [-1.8724e+00,  1.7373e+00,  2.2634e+00,  ...,  2.9354e+00,\n",
       "            1.0588e+00,  3.1257e-01],\n",
       "          [-4.6846e+00,  5.1680e-01,  5.4203e+00,  ...,  2.4117e+00,\n",
       "            1.1508e+00, -1.7519e+00],\n",
       "          ...,\n",
       "          [-6.9852e-01, -3.9404e-01, -3.0901e-01,  ...,  4.4635e+00,\n",
       "            1.1828e+00, -3.2489e-01],\n",
       "          [-1.8644e+00, -2.5431e+00,  9.4300e-01,  ...,  2.4580e+00,\n",
       "            1.6207e+00, -9.7401e-01],\n",
       "          [-1.9835e+00, -2.5516e+00,  1.4020e+00,  ...,  2.4023e+00,\n",
       "            1.4053e+00, -1.2924e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.8897e-03, -8.5541e-03, -6.6792e-03,  ...,  7.3067e-01,\n",
       "           -2.6189e-01,  9.3621e-01],\n",
       "          [ 9.1417e-02, -5.6554e-01, -2.4241e-01,  ..., -3.8125e+00,\n",
       "            1.1236e+00, -1.9188e+00],\n",
       "          [ 5.9978e-01,  1.8372e-01, -3.3556e-01,  ..., -2.5278e+00,\n",
       "            3.4617e+00, -2.7418e+00],\n",
       "          ...,\n",
       "          [-6.8268e-03,  3.3824e-01, -5.2479e-01,  ..., -3.6314e+00,\n",
       "            2.2873e+00, -3.8610e+00],\n",
       "          [ 7.0300e-02,  6.9542e-01, -2.4932e-01,  ..., -3.8232e+00,\n",
       "            3.8180e+00, -4.3734e+00],\n",
       "          [-1.1762e-01,  2.4245e-01, -2.2961e-01,  ..., -3.6924e+00,\n",
       "            3.5998e+00, -6.4167e-01]],\n",
       "\n",
       "         [[ 8.9313e-03,  1.9249e-04, -1.3081e-03,  ...,  2.4107e-01,\n",
       "           -5.8914e-01,  4.6706e-01],\n",
       "          [ 8.4847e+00,  3.8659e+00, -1.4906e+00,  ...,  1.1985e+00,\n",
       "           -1.0129e+00,  7.3079e-01],\n",
       "          [ 3.5251e-01,  4.8102e+00, -9.2580e-01,  ...,  6.4300e-01,\n",
       "           -1.3643e+00,  1.7171e+00],\n",
       "          ...,\n",
       "          [ 1.0547e+01,  1.5806e+00, -3.7557e+00,  ...,  5.5973e-01,\n",
       "           -1.1172e+00,  2.4767e-01],\n",
       "          [ 6.0395e+00, -2.7517e+00, -2.8386e+00,  ...,  9.7357e-01,\n",
       "           -1.0798e+00,  1.5438e+00],\n",
       "          [-3.6974e+00, -4.4412e+00, -1.7539e+00,  ...,  8.0639e-01,\n",
       "           -5.3735e-01,  1.7353e+00]],\n",
       "\n",
       "         [[-6.1773e-03, -4.7648e-03,  4.3674e-03,  ..., -2.2127e+00,\n",
       "           -2.8439e-01, -2.1261e+00],\n",
       "          [-1.6006e+00, -2.1974e+00,  1.8704e+00,  ...,  1.6758e+00,\n",
       "           -7.9390e-02,  2.4110e-01],\n",
       "          [ 2.4528e+00, -1.7297e+00,  2.6152e+00,  ...,  2.1451e+00,\n",
       "           -3.4960e+00,  1.2591e+00],\n",
       "          ...,\n",
       "          [-1.0968e+00, -8.7350e-01,  1.4677e+00,  ...,  1.1448e+00,\n",
       "           -3.0650e+00,  4.6271e+00],\n",
       "          [-1.2892e+00,  4.2466e-01,  2.4658e+00,  ...,  2.4421e+00,\n",
       "           -2.7948e+00,  4.6624e+00],\n",
       "          [ 3.4765e+00,  3.1062e+00,  1.4147e+00,  ...,  2.0979e+00,\n",
       "           -4.4659e+00,  3.9163e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.1986e-03, -6.1555e-04,  2.7634e-02,  ..., -3.5271e-04,\n",
       "           -4.4634e-02,  5.3452e-03],\n",
       "          [-6.0881e-01, -5.8325e-01,  3.1055e-02,  ...,  7.4701e-01,\n",
       "           -5.4889e-01, -3.2873e-01],\n",
       "          [-2.2036e-01, -5.4827e-01, -5.6060e-01,  ...,  3.3911e-01,\n",
       "            7.9925e-01, -4.6696e-01],\n",
       "          ...,\n",
       "          [ 5.8245e-01, -8.5974e-01,  1.3895e-01,  ..., -2.2891e-02,\n",
       "           -1.9298e-01,  4.5588e-01],\n",
       "          [ 1.1794e-01, -1.0868e-01, -6.9830e-01,  ...,  2.0577e-01,\n",
       "           -3.5786e-01, -1.3674e-01],\n",
       "          [ 1.1109e-02, -1.5779e-01, -1.0237e+00,  ...,  1.6694e-01,\n",
       "           -1.9032e-01, -9.6651e-02]],\n",
       "\n",
       "         [[-1.3977e-03, -1.9324e-03,  1.5541e-04,  ..., -1.2159e-03,\n",
       "           -2.7175e-03, -2.4352e-03],\n",
       "          [ 4.5875e-02, -8.3041e-02,  7.7391e-02,  ..., -2.1849e-01,\n",
       "           -1.0348e-01,  1.5184e-01],\n",
       "          [ 2.3543e-02,  7.1947e-02,  1.0577e-02,  ..., -3.2470e-01,\n",
       "           -1.8506e-01, -2.3783e-01],\n",
       "          ...,\n",
       "          [ 2.9183e-01,  1.9155e-02,  2.8777e-01,  ..., -1.2707e-01,\n",
       "            6.6898e-02,  4.7408e-02],\n",
       "          [ 1.7751e-01, -1.7755e-01, -1.2796e-01,  ..., -1.8853e-01,\n",
       "            9.7866e-02,  1.3219e-01],\n",
       "          [-3.0405e-02,  1.1970e-01, -1.4007e-01,  ..., -3.5724e-02,\n",
       "            2.0786e-01,  1.7572e-01]],\n",
       "\n",
       "         [[-6.6764e-02,  6.5199e-03,  7.4771e-03,  ...,  1.8510e-04,\n",
       "           -1.2284e-03,  4.5895e-03],\n",
       "          [ 4.7820e-01,  9.1610e-02,  1.4808e-01,  ..., -6.7166e-03,\n",
       "            8.3459e-02,  3.8141e-02],\n",
       "          [-4.5277e-01, -2.3820e-01, -3.6978e-02,  ...,  2.8525e-01,\n",
       "            2.9966e-01,  1.4864e-01],\n",
       "          ...,\n",
       "          [-2.4190e-01,  4.3877e-02,  4.1503e-01,  ..., -3.2355e-02,\n",
       "            1.8130e-01,  6.2558e-02],\n",
       "          [ 5.1659e-01,  6.0556e-02,  1.2100e-01,  ..., -1.9469e-01,\n",
       "           -2.2688e-01,  1.8642e-01],\n",
       "          [ 4.4829e-01,  3.0116e-01,  2.9936e-01,  ..., -6.4570e-01,\n",
       "           -3.1778e-01,  6.5463e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.3710e-03,  2.2200e-02,  3.0882e-03,  ..., -3.8832e-04,\n",
       "           -9.7100e-03,  7.9522e-04],\n",
       "          [-3.2862e-01, -3.6268e-01,  5.0070e-02,  ...,  8.1113e-03,\n",
       "            2.6225e-01,  3.8278e-02],\n",
       "          [-2.1063e-01, -1.1080e-01,  4.3070e-01,  ...,  9.3783e-02,\n",
       "            9.4757e-02,  1.5797e-03],\n",
       "          ...,\n",
       "          [ 3.9111e-01, -3.6245e-01,  3.7688e-02,  ...,  2.1235e-01,\n",
       "            2.5800e-01,  1.8558e-01],\n",
       "          [ 3.2666e-01,  3.7693e-02,  8.1845e-02,  ...,  3.3504e-01,\n",
       "           -1.4554e-01,  1.6861e-02],\n",
       "          [ 1.4035e-01,  3.5824e-01,  4.2171e-01,  ..., -1.9447e-01,\n",
       "           -4.0935e-01, -1.8569e-01]],\n",
       "\n",
       "         [[ 3.6565e-02,  1.1252e-02, -1.2403e-02,  ...,  4.7556e-03,\n",
       "            2.1622e-04, -8.3046e-03],\n",
       "          [-1.1779e-01, -4.3347e-02, -5.2408e-02,  ..., -3.7160e-02,\n",
       "            1.3147e-01,  2.1386e-01],\n",
       "          [-1.5717e-01,  1.4303e-01,  1.3540e-01,  ..., -8.8188e-02,\n",
       "           -2.8767e-01,  2.3716e-01],\n",
       "          ...,\n",
       "          [ 1.9579e-01, -1.5795e-01, -4.5826e-01,  ..., -2.6651e-01,\n",
       "           -3.0792e-01, -1.3897e-01],\n",
       "          [-4.1344e-01,  2.6270e-01,  1.0617e-01,  ..., -1.2814e-01,\n",
       "           -2.8619e-01,  1.6938e-01],\n",
       "          [-1.9172e-01,  4.6324e-02,  6.6009e-02,  ..., -1.9707e-01,\n",
       "           -3.7782e-02, -1.8310e-01]],\n",
       "\n",
       "         [[-1.0527e-02, -2.3308e-02,  4.2465e-03,  ..., -2.5730e-03,\n",
       "           -2.7933e-04, -3.1259e-03],\n",
       "          [ 1.5071e-01,  1.7240e-01,  2.8966e-01,  ...,  3.3822e-02,\n",
       "           -2.2152e-02, -4.9997e-03],\n",
       "          [-1.2489e-01, -1.1842e-01,  1.2845e-01,  ...,  1.4871e-02,\n",
       "            2.0932e-01,  3.2700e-01],\n",
       "          ...,\n",
       "          [ 1.3476e-01, -6.0808e-02, -2.6076e-01,  ...,  3.1230e-01,\n",
       "            1.0025e-01, -2.2514e-02],\n",
       "          [-1.0873e-01,  1.4865e-01,  7.2098e-02,  ...,  1.8193e-02,\n",
       "           -2.9236e-02,  5.7537e-02],\n",
       "          [-2.6432e-01,  2.5725e-03,  7.6470e-02,  ...,  2.3485e-01,\n",
       "           -2.6849e-01,  8.1100e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[-3.8794e-03, -3.1203e-03,  5.1856e-03,  ...,  6.9401e-02,\n",
       "            9.0124e-01, -1.2046e+00],\n",
       "          [-1.2592e-01, -5.4229e-01,  1.1890e+00,  ..., -5.3476e-01,\n",
       "           -1.7403e+00,  1.7805e+00],\n",
       "          [ 1.1032e+00,  1.2134e+00,  1.3930e+00,  ..., -1.4813e-01,\n",
       "           -3.6474e-01,  2.1338e+00],\n",
       "          ...,\n",
       "          [-2.7666e-01,  1.9065e-01, -2.7231e-01,  ..., -9.2503e-01,\n",
       "            1.0413e+00,  1.2234e+00],\n",
       "          [ 6.2393e-01,  1.0906e+00, -4.4125e-02,  ...,  7.1395e-01,\n",
       "            2.6201e-01,  3.4005e+00],\n",
       "          [ 1.2533e+00, -4.9962e-01, -2.3943e-02,  ..., -1.0685e-01,\n",
       "            3.1310e-01,  2.7197e+00]],\n",
       "\n",
       "         [[-1.1785e-02, -4.3223e-03, -4.2970e-03,  ...,  7.0344e-01,\n",
       "            4.9700e-02,  2.2158e-01],\n",
       "          [-1.9663e+00,  3.8710e-01, -2.3805e+00,  ...,  4.3729e-01,\n",
       "           -1.9104e+00,  8.9567e-01],\n",
       "          [ 9.1066e-01, -1.0600e+00, -3.1927e+00,  ...,  4.3163e-01,\n",
       "           -1.1056e+00,  1.4394e+00],\n",
       "          ...,\n",
       "          [-1.4250e+00, -2.0087e+00,  1.3086e+00,  ...,  1.5079e-01,\n",
       "           -6.8533e-01,  1.2094e+00],\n",
       "          [-1.7651e+00, -1.6570e+00, -1.9167e+00,  ...,  4.8681e-01,\n",
       "           -2.2654e+00,  1.4917e+00],\n",
       "          [ 2.8278e+00, -5.7090e-01, -3.2720e+00,  ...,  7.5208e-01,\n",
       "           -2.2524e+00, -2.7833e-01]],\n",
       "\n",
       "         [[-2.4910e-04,  8.0325e-04,  5.0257e-03,  ...,  6.3299e-01,\n",
       "           -4.7348e-01, -9.2368e-01],\n",
       "          [ 9.2740e-01, -6.4531e-01,  1.3229e+00,  ..., -6.8219e-02,\n",
       "           -1.6595e+00,  1.7148e+00],\n",
       "          [ 1.8785e+00, -8.1871e-01,  8.2999e-01,  ..., -3.8360e-01,\n",
       "           -2.5895e-01,  1.1784e+00],\n",
       "          ...,\n",
       "          [ 2.4794e-02, -2.0274e-01,  1.3354e+00,  ..., -1.1163e+00,\n",
       "            3.9240e-01,  1.5782e+00],\n",
       "          [ 1.1329e+00,  1.6296e+00,  1.3888e+00,  ..., -5.1933e-01,\n",
       "            2.4863e-01,  1.7394e+00],\n",
       "          [ 1.0608e+00,  5.5540e-01,  1.7003e+00,  ..., -1.0957e+00,\n",
       "            1.1238e+00,  4.9591e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.2498e-03, -5.2814e-03, -1.2501e-02,  ..., -6.4204e-01,\n",
       "           -2.6914e-02,  1.3408e+00],\n",
       "          [-1.3139e-02, -2.9145e-01, -4.5131e-01,  ...,  1.6504e+00,\n",
       "           -8.3797e-02, -2.1704e+00],\n",
       "          [-5.4113e-01,  6.9693e-02, -1.2535e+00,  ..., -5.1585e-02,\n",
       "           -8.7262e-01, -4.4919e+00],\n",
       "          ...,\n",
       "          [-9.4789e-03,  3.2223e-01, -5.1321e-01,  ...,  9.8710e-01,\n",
       "            1.2947e+00, -4.4129e+00],\n",
       "          [-4.6767e-01,  1.1685e+00, -5.4474e-01,  ...,  1.8401e-01,\n",
       "           -1.7990e+00, -4.5388e+00],\n",
       "          [ 5.4937e-02,  5.9582e-01, -9.1061e-01,  ...,  5.5907e-01,\n",
       "           -6.3478e-01, -3.9993e+00]],\n",
       "\n",
       "         [[ 1.1934e-02, -2.2342e-03, -1.1771e-03,  ..., -7.3529e-02,\n",
       "            2.9520e-01, -2.0425e+00],\n",
       "          [ 1.5413e+00, -9.5974e-01, -1.6695e+00,  ..., -7.4896e-01,\n",
       "            1.4137e+00,  2.6680e+00],\n",
       "          [-1.1687e+00, -2.3902e+00, -2.4301e+00,  ..., -1.4638e+00,\n",
       "            1.5426e-01,  3.8336e+00],\n",
       "          ...,\n",
       "          [ 1.6851e+00, -1.2113e+00,  8.2196e-01,  ..., -2.4623e+00,\n",
       "           -3.1268e+00,  3.2767e+00],\n",
       "          [ 1.1869e+00, -6.8006e-01, -2.7821e-01,  ..., -2.7955e+00,\n",
       "            1.9201e+00,  5.1113e+00],\n",
       "          [-2.1448e+00,  5.6387e-01, -1.3741e+00,  ..., -2.8154e+00,\n",
       "            3.3972e+00,  3.7103e+00]],\n",
       "\n",
       "         [[-4.1185e-03, -3.1462e-03, -3.4451e-03,  ...,  1.6131e-01,\n",
       "            7.0371e-01,  1.0784e+00],\n",
       "          [-5.7492e-02, -1.7536e-01, -4.4413e-02,  ..., -2.4529e+00,\n",
       "            4.5135e-02, -2.5887e+00],\n",
       "          [-1.2697e-01, -2.1372e-01,  5.0015e-01,  ...,  7.3268e-01,\n",
       "           -1.0646e+00, -3.0724e+00],\n",
       "          ...,\n",
       "          [-2.7601e-02, -3.7859e-01,  1.0560e-01,  ..., -1.9268e+00,\n",
       "           -1.8253e+00, -7.3212e-01],\n",
       "          [-1.5349e-01, -1.2367e-02,  1.5089e-01,  ...,  1.6531e+00,\n",
       "           -2.7630e+00, -3.3483e+00],\n",
       "          [ 2.0851e-01, -4.8146e-02,  2.8686e-01,  ...,  6.7316e-01,\n",
       "           -2.5097e+00, -2.8698e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 5.2594e-03,  9.4592e-03,  1.9867e-03,  ..., -3.8706e-03,\n",
       "           -1.7056e-03,  7.7468e-03],\n",
       "          [ 1.9298e-01,  8.1492e-01, -4.8087e-01,  ..., -8.5063e-01,\n",
       "            7.5912e-01,  5.3320e-01],\n",
       "          [-2.8135e-01,  1.5063e-01,  1.1840e-01,  ..., -4.7821e-01,\n",
       "           -3.0643e-01,  2.7797e-01],\n",
       "          ...,\n",
       "          [-3.0402e-01, -4.3815e-01,  4.4622e-01,  ..., -5.9696e-01,\n",
       "            1.3555e-01,  5.0251e-01],\n",
       "          [-4.3297e-01, -2.2490e-01,  1.4321e-01,  ..., -1.2620e-01,\n",
       "           -1.1699e-01,  4.5587e-02],\n",
       "          [ 3.5111e-02, -1.2276e-01,  4.1070e-02,  ...,  1.2854e-01,\n",
       "            1.5057e-02, -1.8823e-01]],\n",
       "\n",
       "         [[ 1.0722e-01,  1.5723e-02, -7.6928e-03,  ..., -4.9784e-02,\n",
       "           -3.6010e-02,  4.9264e-02],\n",
       "          [-3.6844e-02, -1.8172e-01,  1.1652e-01,  ...,  1.2303e-01,\n",
       "            2.5226e-01, -2.0720e-02],\n",
       "          [ 2.7474e-01, -8.7224e-02, -1.9187e-01,  ...,  1.2024e-01,\n",
       "            2.6569e-01, -3.3163e-01],\n",
       "          ...,\n",
       "          [-3.9618e-01, -1.4716e-01, -1.6782e-01,  ..., -4.4367e-02,\n",
       "           -9.7106e-02, -2.2385e-01],\n",
       "          [-2.3633e-01, -4.4070e-01, -2.1435e-01,  ..., -2.3864e-01,\n",
       "           -1.8083e-01, -3.8948e-01],\n",
       "          [-1.1772e-01, -2.5441e-01, -8.1915e-02,  ..., -2.3311e-01,\n",
       "            5.0733e-02, -4.6929e-01]],\n",
       "\n",
       "         [[ 2.5745e-03,  1.3998e-03, -1.4485e-01,  ...,  3.1090e-03,\n",
       "           -7.6176e-03,  2.9843e-03],\n",
       "          [ 2.9424e-01,  2.6524e-01,  4.4290e-01,  ...,  6.8095e-02,\n",
       "            9.3785e-01,  2.4705e-01],\n",
       "          [-2.2151e-01, -2.2390e-01,  1.6890e-01,  ...,  4.1762e-01,\n",
       "           -2.4887e-01, -2.9783e-01],\n",
       "          ...,\n",
       "          [-3.5760e-01,  7.2559e-01, -6.0378e-03,  ...,  5.5287e-01,\n",
       "           -1.1144e+00,  6.3684e-01],\n",
       "          [-4.7756e-01,  1.2510e-01, -3.5053e-01,  ..., -5.3108e-04,\n",
       "           -2.3532e-01, -6.2104e-02],\n",
       "          [-4.0835e-01,  2.7073e-02, -1.7554e-01,  ...,  4.0779e-02,\n",
       "            1.5586e-01,  1.9162e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.5809e-03, -6.7373e-03, -6.5214e-03,  ..., -2.2586e-03,\n",
       "           -4.3845e-03, -2.7203e-04],\n",
       "          [ 1.1266e+00,  3.9672e-01,  5.2206e-01,  ...,  5.9311e-01,\n",
       "           -1.3530e-01,  2.8587e-01],\n",
       "          [-3.4696e-01, -4.8278e-01,  1.3970e+00,  ...,  7.0488e-02,\n",
       "            2.8192e-01,  5.2621e-01],\n",
       "          ...,\n",
       "          [-7.3875e-01,  1.9128e-01, -1.0024e-01,  ..., -6.4737e-01,\n",
       "            3.3761e-01,  6.3950e-02],\n",
       "          [-5.4791e-01, -3.0586e-01, -4.8719e-02,  ..., -7.5464e-01,\n",
       "            2.0926e-01,  1.3394e-01],\n",
       "          [ 2.1649e-01, -1.7136e-01, -4.8676e-02,  ..., -3.5303e-01,\n",
       "            2.0836e-01,  4.7170e-01]],\n",
       "\n",
       "         [[-2.6733e-02, -3.1421e-02, -4.2373e-02,  ..., -4.9862e-02,\n",
       "           -3.1812e-02,  7.0532e-01],\n",
       "          [-2.8368e-01, -1.9462e-01,  1.5569e-01,  ..., -2.1656e-01,\n",
       "            1.3451e-01, -5.8143e-01],\n",
       "          [-8.9365e-03,  8.3859e-02,  6.1132e-01,  ...,  2.0914e-01,\n",
       "            3.6407e-01, -6.2821e-01],\n",
       "          ...,\n",
       "          [ 2.3481e-01, -1.4020e-02,  2.0344e-01,  ..., -3.4711e-01,\n",
       "           -1.7117e-01, -3.6969e-01],\n",
       "          [ 7.8077e-02, -7.7294e-02,  8.0888e-02,  ...,  2.6880e-02,\n",
       "           -2.4258e-01, -7.6956e-01],\n",
       "          [ 7.1500e-02,  1.6142e-01,  1.4644e-01,  ..., -2.3763e-02,\n",
       "           -9.3285e-03, -9.8162e-01]],\n",
       "\n",
       "         [[-1.0708e-02,  9.6130e-03,  2.8059e-03,  ...,  2.5493e-03,\n",
       "           -6.9626e-03,  1.7700e-02],\n",
       "          [ 2.3265e-01,  1.1925e-02,  4.6238e-01,  ..., -4.1554e-02,\n",
       "            5.7132e-01,  1.4859e+00],\n",
       "          [ 1.4220e+00,  2.9448e-01,  6.5626e-01,  ...,  6.3176e-01,\n",
       "           -3.2001e-01, -2.3594e-01],\n",
       "          ...,\n",
       "          [-5.2675e-03, -8.1516e-01, -6.7243e-01,  ..., -5.5626e-01,\n",
       "            5.0817e-01,  3.5889e-01],\n",
       "          [ 3.1207e-01, -1.3228e-01, -6.0033e-01,  ..., -8.2791e-02,\n",
       "           -1.0062e-01,  3.0007e-02],\n",
       "          [-3.2178e-02, -1.8696e-01, -3.5750e-01,  ...,  1.3259e-01,\n",
       "            5.5105e-02,  7.9816e-02]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[ 7.0327e-03,  1.4660e-03,  2.1513e-03,  ...,  4.8881e-01,\n",
       "           -4.8315e-01,  8.0926e-01],\n",
       "          [ 2.0983e+00,  7.3174e-01,  1.8339e+00,  ..., -5.7210e-01,\n",
       "           -1.7062e+00,  4.1039e-01],\n",
       "          [ 2.8778e+00, -3.5760e-01,  1.2291e+00,  ..., -3.2705e+00,\n",
       "            5.1240e-01,  3.6652e-01],\n",
       "          ...,\n",
       "          [-3.6144e-01, -1.3848e+00,  2.3746e+00,  ..., -1.2707e+00,\n",
       "           -1.0779e+00,  1.3108e+00],\n",
       "          [ 1.7084e+00, -2.2807e+00,  1.8253e+00,  ..., -1.6498e+00,\n",
       "           -1.7953e+00,  1.6369e+00],\n",
       "          [ 1.5324e+00, -1.2716e+00,  4.2708e-01,  ..., -1.8769e-01,\n",
       "            6.9996e-01,  1.9809e+00]],\n",
       "\n",
       "         [[-4.9468e-03, -1.1031e-05, -3.9722e-03,  ..., -6.5628e-02,\n",
       "           -1.3539e+00,  2.4680e+00],\n",
       "          [-1.0861e+00, -3.3818e-01, -9.1292e-01,  ...,  7.6190e-01,\n",
       "            2.5443e+00, -5.2727e+00],\n",
       "          [-1.7937e+00, -1.8947e+00, -5.0875e-01,  ...,  3.0845e-01,\n",
       "            3.3158e+00, -6.0765e+00],\n",
       "          ...,\n",
       "          [-7.4530e-01, -7.4544e-01, -7.2845e-01,  ..., -1.8061e+00,\n",
       "            3.4001e+00, -4.4755e+00],\n",
       "          [-1.0624e+00, -1.1227e+00, -3.1310e+00,  ..., -1.0135e+00,\n",
       "            3.4352e+00, -3.3968e+00],\n",
       "          [-4.2296e-01,  2.6193e-01, -1.9792e+00,  ..., -6.9703e-01,\n",
       "            3.0726e+00, -6.5264e+00]],\n",
       "\n",
       "         [[-9.6954e-03, -8.6172e-03, -2.4659e-03,  ...,  1.2285e+00,\n",
       "           -5.0399e-01, -1.1271e+00],\n",
       "          [-2.9079e+00, -2.4856e+00, -1.8951e+00,  ...,  1.2957e+00,\n",
       "            1.1913e+00,  2.3681e-01],\n",
       "          [ 2.4111e+00, -2.1722e+00,  6.0063e-01,  ...,  2.3223e+00,\n",
       "            8.8791e-01,  4.5416e-01],\n",
       "          ...,\n",
       "          [-3.8141e+00,  9.6450e-01, -3.0345e+00,  ...,  1.9113e+00,\n",
       "            1.1420e+00,  5.0736e-01],\n",
       "          [-5.7817e-01,  2.6108e+00, -2.7311e+00,  ...,  7.3053e-01,\n",
       "           -9.2061e-01, -3.7586e-01],\n",
       "          [ 4.0167e+00,  3.4943e+00, -1.8346e+00,  ..., -2.7539e-02,\n",
       "            4.4734e-02, -6.8945e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.0942e-03, -2.1835e-03, -6.7511e-04,  ..., -6.8808e-03,\n",
       "           -9.0186e-01, -4.3874e-01],\n",
       "          [ 2.3339e+00,  3.6252e-01,  4.2947e-01,  ...,  1.9069e+00,\n",
       "            1.1109e-01,  1.0847e+00],\n",
       "          [ 2.3776e+00,  1.7912e+00, -1.4346e+00,  ...,  1.4578e+00,\n",
       "            2.5370e+00, -5.7103e-01],\n",
       "          ...,\n",
       "          [ 1.8229e+00,  1.9123e+00,  2.7203e-01,  ...,  1.8366e+00,\n",
       "            5.9247e-01, -1.6726e+00],\n",
       "          [ 3.2758e+00,  2.9197e+00, -1.2460e+00,  ...,  2.6787e+00,\n",
       "            2.4918e+00, -3.3501e-01],\n",
       "          [ 1.0471e+00,  1.1864e+00, -2.0834e+00,  ...,  1.8434e+00,\n",
       "            3.0575e+00, -1.6023e-01]],\n",
       "\n",
       "         [[-1.0082e-02, -3.9730e-03,  3.2855e-03,  ..., -1.8727e+00,\n",
       "            2.6773e-01, -1.9366e+00],\n",
       "          [-2.0344e+00,  1.2713e+00,  2.9216e+00,  ...,  4.6183e-01,\n",
       "           -8.1242e-01,  1.5041e+00],\n",
       "          [-3.3437e+00, -3.7324e-01,  1.6497e+00,  ...,  2.6711e+00,\n",
       "            3.4798e-01,  2.1894e+00],\n",
       "          ...,\n",
       "          [ 6.4838e-01, -3.0898e+00,  1.1836e+00,  ...,  2.6829e+00,\n",
       "           -8.7267e-01,  3.4150e+00],\n",
       "          [-2.6066e+00, -1.9348e+00,  4.3269e+00,  ...,  6.2830e-01,\n",
       "           -3.5729e-02,  2.2689e+00],\n",
       "          [-2.9916e+00, -6.3251e-01,  3.5973e+00,  ...,  1.1734e+00,\n",
       "           -1.0061e+00,  2.9538e+00]],\n",
       "\n",
       "         [[-5.0430e-03,  3.6427e-04, -3.3835e-03,  ..., -1.0226e+00,\n",
       "            5.2830e-02,  3.4624e-01],\n",
       "          [ 1.0669e-01, -4.2091e-01,  6.3873e-01,  ...,  2.7454e+00,\n",
       "           -1.0217e+00, -1.4204e+00],\n",
       "          [-5.9890e-01, -7.3788e-02,  2.8257e-01,  ...,  1.9577e+00,\n",
       "           -3.1626e+00, -2.4538e+00],\n",
       "          ...,\n",
       "          [ 4.7136e-02,  5.3411e-02,  1.6551e-01,  ...,  3.4904e+00,\n",
       "           -2.6842e+00, -1.8915e+00],\n",
       "          [-3.2201e-01,  5.8181e-01,  1.3053e+00,  ...,  3.2922e+00,\n",
       "           -3.2858e+00, -9.9177e-01],\n",
       "          [-1.2643e+00,  4.8774e-01,  9.4143e-01,  ...,  2.6433e+00,\n",
       "           -2.8445e+00, -1.3496e+00]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[ 1.4253e-02, -1.5911e-02, -7.3517e-03,  ...,  1.2178e-02,\n",
       "            2.6016e-02,  7.7523e-03],\n",
       "          [-2.4423e-02,  2.9294e-01, -2.8238e-01,  ...,  2.2545e-01,\n",
       "            2.1764e-01, -2.2164e-01],\n",
       "          [-7.5877e-01, -5.9638e-02, -1.6602e-01,  ..., -2.2169e-01,\n",
       "           -2.3963e-01, -2.9295e-01],\n",
       "          ...,\n",
       "          [ 7.5836e-02, -1.4294e-01,  1.9233e-01,  ..., -3.7491e-01,\n",
       "            2.6397e-01,  1.4929e-01],\n",
       "          [-8.7805e-02, -3.9182e-01,  2.3827e-01,  ..., -2.1209e-01,\n",
       "            1.9617e-01, -2.7654e-02],\n",
       "          [ 3.5634e-01, -6.3868e-01,  8.4051e-02,  ..., -5.8190e-01,\n",
       "           -2.7527e-03,  1.6700e-01]],\n",
       "\n",
       "         [[ 1.2082e-02, -5.9872e-04,  8.3699e-03,  ..., -1.2029e-02,\n",
       "            5.6987e-03,  1.3760e-02],\n",
       "          [-1.1389e-01, -8.9223e-02, -7.4823e-01,  ...,  4.8467e-01,\n",
       "            3.0723e-01, -1.0544e-01],\n",
       "          [-1.5283e-01,  2.7231e-01, -4.9646e-02,  ..., -2.3676e-01,\n",
       "           -5.0148e-01,  1.5238e-01],\n",
       "          ...,\n",
       "          [-5.4270e-01, -9.1190e-02, -3.6716e-01,  ..., -1.6930e-01,\n",
       "           -5.1548e-01,  4.8189e-02],\n",
       "          [ 3.7237e-01,  1.1107e-02,  5.2343e-02,  ..., -2.7538e-01,\n",
       "           -1.5433e-01,  2.3381e-01],\n",
       "          [ 4.0247e-01,  4.4542e-02,  1.4476e-01,  ...,  1.5323e-02,\n",
       "           -4.4547e-02,  2.1567e-01]],\n",
       "\n",
       "         [[ 1.0414e-01,  2.2541e-01,  7.4422e-03,  ..., -2.8213e-02,\n",
       "            4.5830e-01,  3.2158e-03],\n",
       "          [-1.5206e-01,  9.2096e-02,  4.5182e-01,  ..., -4.0786e-01,\n",
       "           -2.5606e+00,  5.4543e-02],\n",
       "          [ 1.0048e-01, -6.2714e-03,  5.3115e-01,  ..., -8.5924e-02,\n",
       "           -2.6331e+00,  1.7325e-01],\n",
       "          ...,\n",
       "          [ 3.1729e-01, -5.9426e-02,  1.6300e-01,  ..., -3.1388e-01,\n",
       "           -2.7477e+00, -6.6864e-02],\n",
       "          [ 4.7832e-03, -1.2722e-01,  2.9366e-01,  ..., -1.9763e-01,\n",
       "           -2.1621e+00,  1.1235e-01],\n",
       "          [ 1.8549e-02, -2.5964e-01,  1.2386e-01,  ..., -3.1146e-01,\n",
       "           -2.6983e+00, -1.8557e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4819e-02, -2.2122e-02,  1.8370e-02,  ...,  1.3814e-02,\n",
       "            9.0936e-03,  7.7128e-03],\n",
       "          [-1.2729e+00, -1.3179e-01,  5.7708e-01,  ..., -1.4908e-01,\n",
       "           -1.9363e+00,  5.0641e-01],\n",
       "          [-1.6932e+00,  2.3225e-01,  1.1135e-01,  ..., -8.9417e-01,\n",
       "            3.6712e-01,  1.0141e+00],\n",
       "          ...,\n",
       "          [-1.7799e+00, -3.6059e-01, -1.3660e+00,  ...,  2.1226e+00,\n",
       "            8.8817e-01, -9.7610e-01],\n",
       "          [-3.1432e-01, -3.0257e-01,  4.4905e-01,  ...,  3.1469e-01,\n",
       "            3.5898e-02, -8.6918e-01],\n",
       "          [-3.0261e-01, -2.7096e-01,  3.4502e-03,  ...,  1.4119e-01,\n",
       "           -8.5421e-02, -3.2117e-01]],\n",
       "\n",
       "         [[ 2.9545e-02,  9.4759e-02, -5.8445e-02,  ..., -3.6007e-02,\n",
       "            2.4065e-02,  2.3776e-02],\n",
       "          [ 1.7652e-01, -6.1769e-01,  4.2904e-02,  ..., -3.8472e-02,\n",
       "           -6.6789e-02, -9.3961e-02],\n",
       "          [ 3.7490e-01,  4.7670e-01, -1.1498e-01,  ..., -1.0804e-02,\n",
       "           -5.2380e-02,  2.7229e-01],\n",
       "          ...,\n",
       "          [ 4.8751e-01,  2.6375e-01,  3.3268e-02,  ..., -1.6817e-01,\n",
       "           -3.4326e-01, -1.8377e-01],\n",
       "          [-4.3130e-01, -3.3948e-02,  6.4834e-02,  ...,  2.5659e-01,\n",
       "           -3.0970e-01, -1.8172e-02],\n",
       "          [-3.8782e-01,  1.8313e-01, -6.6158e-02,  ...,  1.9715e-01,\n",
       "           -1.8194e-02, -9.5334e-02]],\n",
       "\n",
       "         [[-5.1900e-03,  7.0205e-03, -4.5082e-03,  ...,  1.0111e-04,\n",
       "            7.2874e-03, -3.5747e-03],\n",
       "          [-2.2020e+00,  2.0725e-01, -6.4701e-01,  ..., -8.5136e-01,\n",
       "           -1.1429e+00, -1.2009e+00],\n",
       "          [ 1.5616e+00, -4.4230e-01,  8.1782e-01,  ..., -8.0213e-01,\n",
       "           -5.8112e-01,  5.7025e-01],\n",
       "          ...,\n",
       "          [ 1.0619e+00,  3.2636e-01, -7.3605e-01,  ..., -3.3246e-01,\n",
       "            3.1248e-02,  7.1247e-01],\n",
       "          [ 2.9815e-01, -4.5339e-02,  4.6774e-02,  ..., -2.9805e-01,\n",
       "            5.7812e-01,  2.0000e-01],\n",
       "          [ 4.5741e-02, -7.0499e-02, -2.5740e-01,  ..., -4.8810e-02,\n",
       "            1.6293e-01,  2.4758e-01]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's evaluate the FX graph using a custom backend\n",
    "from typing import List\n",
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    print(\"custom backend called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "    return gm.forward\n",
    "\n",
    "# Reset since we are using a different backend.\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(init_model(), backend=custom_backend)\n",
    "inp_data = generate_data(1)\n",
    "opt_model(input_ids=inp_data[0], attention_mask=inp_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expensive cost of Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::mm        53.11%     469.455ms        53.19%     470.211ms      97.961us          4800  \n",
      "         model_inference        20.03%     177.102ms       100.00%     884.002ms     884.002ms             1  \n",
      "         aten::clamp_min         2.83%      25.031ms         2.83%      25.031ms      41.718us           600  \n",
      "            aten::matmul         1.75%      15.436ms        60.83%     537.777ms      81.481us          6600  \n",
      "              aten::view         1.60%      14.177ms         1.60%      14.177ms       1.447us          9800  \n",
      "               aten::mul         1.41%      12.432ms         1.46%      12.911ms       3.637us          3550  \n",
      "               aten::bmm         1.36%      12.043ms         1.38%      12.192ms       6.773us          1800  \n",
      "               aten::add         1.32%      11.671ms         1.69%      14.898ms       4.318us          3450  \n",
      "      aten::_unsafe_view         1.26%      11.101ms         1.26%      11.101ms       1.682us          6600  \n",
      "         aten::transpose         1.11%       9.773ms         1.64%      14.463ms       1.555us          9300  \n",
      "                 aten::t         1.07%       9.503ms         1.85%      16.327ms       3.401us          4800  \n",
      "               aten::pow         0.98%       8.643ms         1.03%       9.109ms       5.693us          1600  \n",
      "      aten::index_select         0.94%       8.327ms         1.04%       9.234ms      46.169us           200  \n",
      "        aten::as_strided         0.84%       7.390ms         0.84%       7.390ms       0.485us         15250  \n",
      "           aten::reshape         0.79%       7.011ms         2.39%      21.165ms       2.461us          8600  \n",
      "              aten::mean         0.79%       6.967ms         2.65%      23.426ms      14.641us          1600  \n",
      "            aten::expand         0.67%       5.880ms         0.90%       7.969ms       2.154us          3700  \n",
      "             aten::copy_         0.61%       5.387ms         0.61%       5.387ms       1.056us          5100  \n",
      "          aten::_softmax         0.60%       5.297ms         0.60%       5.297ms       5.885us           900  \n",
      "              aten::div_         0.59%       5.177ms         1.31%      11.599ms       7.249us          1600  \n",
      "            aten::linear         0.58%       5.113ms        58.81%     519.840ms     108.300us          4800  \n",
      "          aten::_to_copy         0.53%       4.723ms         1.18%      10.466ms       2.522us          4150  \n",
      "              aten::add_         0.53%       4.705ms         0.53%       4.705ms       4.953us           950  \n",
      "    aten::_reshape_alias         0.48%       4.242ms         0.48%       4.242ms       1.571us          2700  \n",
      "               aten::sum         0.44%       3.915ms         0.55%       4.860ms       3.038us          1600  \n",
      "             aten::rsqrt         0.42%       3.691ms         0.42%       3.691ms       2.307us          1600  \n",
      "     aten::empty_strided         0.34%       2.965ms         0.34%       2.965ms       0.690us          4300  \n",
      "             aten::empty         0.29%       2.597ms         0.29%       2.597ms       1.484us          1750  \n",
      "                aten::to         0.27%       2.428ms         1.46%      12.894ms       1.394us          9250  \n",
      "             aten::clone         0.22%       1.927ms         0.84%       7.388ms       8.209us           900  \n",
      "              aten::relu         0.18%       1.568ms         3.01%      26.598ms      44.331us           600  \n",
      "             aten::slice         0.16%       1.414ms         0.18%       1.569ms       2.092us           750  \n",
      "            aten::arange         0.14%       1.255ms         0.27%       2.373ms       4.746us           500  \n",
      "             aten::fill_         0.13%       1.146ms         0.13%       1.146ms       0.619us          1850  \n",
      "         aten::embedding         0.13%       1.143ms         1.20%      10.607ms      53.034us           200  \n",
      "         aten::unsqueeze         0.11%       1.012ms         0.14%       1.216ms       1.430us           850  \n",
      "        aten::empty_like         0.11%     957.722us         0.37%       3.268ms       3.113us          1050  \n",
      "      aten::resolve_conj         0.10%     905.259us         0.10%     905.259us       0.069us         13200  \n",
      "               aten::sub         0.10%     883.292us         0.16%       1.438ms       5.754us           250  \n",
      "           aten::softmax         0.09%     816.041us         0.69%       6.113ms       6.792us           900  \n",
      "               aten::log         0.08%     729.411us         0.08%     729.411us       7.294us           100  \n",
      "           aten::type_as         0.08%     674.251us         0.09%     786.482us       0.874us           900  \n",
      "            aten::select         0.08%     668.149us         0.09%     798.970us       1.997us           400  \n",
      "        aten::contiguous         0.07%     645.460us         0.91%       8.033ms       8.926us           900  \n",
      "           aten::dropout         0.06%     570.075us         0.06%     570.075us       0.178us          3200  \n",
      "             aten::where         0.06%     511.595us         0.06%     535.935us       5.359us           100  \n",
      "               aten::div         0.05%     475.705us         0.11%     952.451us       4.762us           200  \n",
      "           aten::minimum         0.05%     440.278us         0.05%     440.278us       2.935us           150  \n",
      "                aten::lt         0.04%     389.371us         0.04%     389.371us       3.894us           100  \n",
      "       aten::result_type         0.04%     387.676us         0.04%     387.676us       0.242us          1600  \n",
      "            aten::repeat         0.04%     372.606us         0.11%     994.050us      19.881us            50  \n",
      "              aten::rsub         0.04%     358.845us         0.16%       1.397ms       9.313us           150  \n",
      "              aten::ones         0.04%     353.903us         0.06%     550.946us       3.673us           150  \n",
      "           aten::permute         0.04%     319.412us         0.04%     355.603us       3.556us           100  \n",
      "               aten::abs         0.03%     272.714us         0.05%     404.745us       4.047us           100  \n",
      "                aten::gt         0.03%     257.684us         0.03%     257.684us       5.154us            50  \n",
      "                aten::le         0.03%     225.902us         0.03%     225.902us       4.518us            50  \n",
      "            aten::unfold         0.02%     164.751us         0.03%     249.171us       1.661us           150  \n",
      "               aten::min         0.02%     163.554us         0.07%     603.832us       4.026us           150  \n",
      "               aten::neg         0.02%     162.150us         0.02%     162.150us       3.243us            50  \n",
      "         aten::full_like         0.02%     136.584us         0.04%     316.535us       3.165us           100  \n",
      "           aten::resize_         0.01%     129.300us         0.01%     129.300us       0.517us           250  \n",
      "        aten::zeros_like         0.01%     123.191us         0.03%     248.413us       4.968us            50  \n",
      "             aten::zeros         0.01%     118.910us         0.02%     184.210us       3.684us            50  \n",
      "             aten::alias         0.01%      64.060us         0.01%      64.060us       1.281us            50  \n",
      "             aten::zero_         0.00%      42.901us         0.00%      42.901us       0.429us           100  \n",
      "         aten::expand_as         0.00%      34.760us         0.01%     107.520us       2.150us            50  \n",
      "------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 884.002ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch._inductor.config as config \n",
    "\n",
    "config.cpp.weight_prepack=True\n",
    "config.freezing=True\n",
    "\n",
    "def test_inference(mode, num_iter):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "    input_ids = tokenizer(\n",
    "        \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    "    ).input_ids  # Batch size 1\n",
    "    decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "    if (mode == 'compile'):\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(50):\n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU]) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            for _ in range(num_iter):\n",
    "                outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "\n",
    "def main(mode='eager', num_iter=50) -> None:\n",
    "    test_inference(mode, num_iter)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2169] [4/0] [__guards] GUARDS:\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] \n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] +- RootGuardManager\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125137956418832)                   # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 468362288)                 # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=L['self'].decoder, accessed_by=DictGetItemGuardAccessor(decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder, 125146543897808)           # decoder_outputs = self.decoder(  # transformers/models/t5/modeling_t5.py:1514 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=L['self'].decoder.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.__dict__)   # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.training, 8905664)          # decoder_outputs = self.decoder(  # transformers/models/t5/modeling_t5.py:1514 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.block, accessed_by=DictGetItemGuardAccessor(block)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block, 125138699587216)     # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block.training, 8905664)    # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight, 125138600635536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.weight, 125138600630832)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.weight, 125138600636208)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.weight, 125138600630448)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.weight, 125138600636112)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, 125138600631504)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight, 125138600629680)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.weight, 125138600634576)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.weight, 125138600636304)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.weight, 125138600625456)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.weight, 125138600639856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight, 125138600624304)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.weight, 125138600625360)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight, 125138600629296)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight, 125138853503888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.weight, 125138853504368)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.weight, 125138853510896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.weight, 125138600638320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.weight, 125138600639088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight, 125138853504464)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.weight, 125138853508592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.weight, 125138853507056)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.weight, 125138853508400)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.weight, 125138853507344)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight, 125138853507248)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.weight, 125138600626032)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight, 125138867923536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight, 125138867927472)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.weight, 125138853507536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.weight, 125138867927280)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.weight, 125138867926704)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.weight, 125138867926992)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight, 125138867928720)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.weight, 125138867923920)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.weight, 125138867927184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.weight, 125138867928816)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.weight, 125138867926224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight, 125138750823632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.weight, 125138867927856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight, 125138750831888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3'), accessed_by=DictGetItemGuardAccessor(3)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight, 125138750826704)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.weight, 125138750828048)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.weight, 125138750830256)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.weight, 125138867923728)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.weight, 125138750826896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight, 125138750826800)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.weight, 125138750831312)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.weight, 125138750830160)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.weight, 125138750823536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.weight, 125138750820848)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight, 125138750829776)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.weight, 125138750831024)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight, 125138750822000)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4'), accessed_by=DictGetItemGuardAccessor(4)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight, 125138750830544)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.weight, 125138750829968)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.weight, 125138750825360)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.weight, 125138750826224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.weight, 125138750827568)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight, 125138750830064)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.weight, 125138851307664)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.weight, 125138851305552)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.weight, 125138851303536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.weight, 125138851304208)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight, 125138851307088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.weight, 125138750826320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight, 125138851304784)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5'), accessed_by=DictGetItemGuardAccessor(5)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight, 125138851301712)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.weight, 125138851303920)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.weight, 125138851310352)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.weight, 125138750831504)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.weight, 125138851303152)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight, 125138851304880)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.weight, 125138851309584)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.weight, 125138851305456)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.weight, 125138851314864)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.weight, 125138851308144)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight, 125138851308624)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.weight, 125138851301232)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight, 125138851299984)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0], 125146543897424)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer, 125146543895888)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0], 125146543887312)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].dropout, 125138675931792)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].layer_norm, 125137961899536)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention, 125146543892688)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.k, 125137950235792)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.o, 125138599605968)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.q, 125137950708368)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.v, 125137960731728)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias, 125138580192208)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, 8905664)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.is_decoder, 8906112)  # bidirectional=(not self.is_decoder),  # transformers/models/t5/modeling_t5.py:437 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, 8906112)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets, accessed_by=DictGetItemGuardAccessor(relative_attention_num_buckets)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets == 32  # num_buckets=self.relative_attention_num_buckets,  # transformers/models/t5/modeling_t5.py:438 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_max_distance, accessed_by=DictGetItemGuardAccessor(relative_attention_max_distance)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_max_distance == 128  # max_distance=self.relative_attention_max_distance,  # transformers/models/t5/modeling_t5.py:439 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1], 125138675939216)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].dropout, 125138695387920)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].layer_norm, 125138695382416)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention, 125138698647632)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.k, 125138678508816)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.o, 125138695386448)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.q, 125138677900816)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.v, 125138695383952)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:532 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.has_relative_attention_bias, 8905664)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1], 125138695373072)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].dropout, 125138695383632)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].layer_norm, 125138695386640)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense, 125138695384656)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wi, 125138695384464)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wo, 125138695384592)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.act, 125138695386256)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout, 125138695375120)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1], 125138675931984)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer, 125138695380304)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0], 125138695377488)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].dropout, 125146861621136)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].layer_norm, 125146861624784)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention, 125138695387408)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.k, 125138673072592)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.o, 125137946498064)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.q, 125138695385232)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.v, 125138400002576)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1], 125146861625232)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].dropout, 125137963270864)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].layer_norm, 125137963270608)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention, 125146861624656)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.k, 125138673792528)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.o, 125138690743824)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.q, 125146861625168)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.v, 125138690751696)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1], 125137963270928)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].dropout, 125138738826896)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].layer_norm, 125138738825744)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense, 125137962842768)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wi, 125137962842832)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wo, 125137962842704)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.act, 125137947579728)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout, 125137962842000)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2], accessed_by=GetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2], 125137962235088)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer, 125138673896976)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0], 125137963042192)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].dropout, 125138673316432)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].layer_norm, 125138673318480)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention, 125137963042128)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.k, 125138578403984)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.o, 125137963407760)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.q, 125137963042000)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.v, 125137963407824)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1], 125138673310736)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].dropout, 125138587312400)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].layer_norm, 125138587318544)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention, 125138673312464)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.k, 125137960778064)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.o, 125138857733840)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.q, 125137947380560)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.v, 125138576607568)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1], 125137961097296)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].dropout, 125137946940176)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].layer_norm, 125137947967312)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense, 125137961097872)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wi, 125138683385616)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wo, 125138738129936)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.act, 125138738127888)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout, 125138738130448)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3], accessed_by=GetItemGuardAccessor(3)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3], 125138673304592)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer, 125138601051408)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0], 125138601054928)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].dropout, 125138672743248)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].layer_norm, 125138672742736)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention, 125138601058768)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.k, 125137962372688)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.o, 125137946070864)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.q, 125138601056144)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.v, 125137962372112)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1], 125138672745808)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].dropout, 125138672743824)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].layer_norm, 125138672744976)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention, 125138672745296)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.k, 125138672738960)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.o, 125138672731856)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.q, 125138672731280)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.v, 125138672746064)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1], 125138675445072)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].dropout, 125138700328080)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].layer_norm, 125138681881040)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense, 125137947898896)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wi, 125137947899472)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wo, 125138672680912)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.act, 125138672676688)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout, 125138672675856)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4], accessed_by=GetItemGuardAccessor(4)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4], 125138672736976)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer, 125137962509648)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0], 125137946269456)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].dropout, 125138860636048)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].layer_norm, 125137961052048)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention, 125137946270032)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.k, 125137962872272)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.o, 125138701836304)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.q, 125138400150544)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.v, 125137960914512)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1], 125138701195728)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].dropout, 125138701205072)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].layer_norm, 125138701195152)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention, 125138701197840)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.k, 125138701189392)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.o, 125138701199632)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.q, 125138701195408)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.v, 125138701191184)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1], 125138701202384)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].dropout, 125138701195600)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].layer_norm, 125138701197584)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense, 125138701202000)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wi, 125138701193360)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wo, 125138701202320)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.act, 125138701201168)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout, 125138701195536)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5], accessed_by=GetItemGuardAccessor(5)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5], 125137959501456)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer, 125137959501200)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0], 125138678053392)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].dropout, 125138699588304)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].layer_norm, 125138699591504)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention, 125138678048272)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.k, 125138575404944)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.o, 125138699588112)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.q, 125137963071440)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.v, 125138699586448)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1], 125138699593872)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].dropout, 125138740978384)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].layer_norm, 125138740976336)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention, 125138740973200)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.k, 125138740972368)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.o, 125138740982864)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.q, 125138740973584)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.v, 125138740976208)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1], 125138740977232)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].dropout, 125138674408592)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].layer_norm, 125138698148752)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense, 125138740985680)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wi, 125138740982672)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wo, 125138700184464)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.act, 125138695795536)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout, 125138695795984)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.dropout, 125138674403856)   # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.dropout.training, 8905664)  # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens, accessed_by=DictGetItemGuardAccessor(embed_tokens)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.embed_tokens, 125138739169360)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.embed_tokens.training, 8905664)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm, accessed_by=DictGetItemGuardAccessor(final_layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm, 125138699712912)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.final_layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm.training, 8905664)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm.weight, 125138851303728)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.final_layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].decoder.config, 468362288)         # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/t5/modeling_t5.py:989 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.is_decoder, 8906112)        # if not self.is_decoder:  # transformers/models/t5/modeling_t5.py:1020 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.model_parallel, 8905664)    # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:985 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:1047 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask, accessed_by=GetAttrGuardAccessor(get_extended_attention_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.get_extended_attention_mask.__defaults__[0], 8820832)  # if device is not None:  # transformers/modeling_utils.py:1079 in create_extended_attention_mask_for_decoder\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.get_extended_attention_mask.__defaults__[1], 8820832)  # if dtype is None:  # transformers/modeling_utils.py:1120 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=L['self'].encoder, accessed_by=DictGetItemGuardAccessor(encoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder, 125137949124688)           # encoder_outputs = self.encoder(  # transformers/models/t5/modeling_t5.py:1484 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.__dict__)   # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.training, 8905664)          # encoder_outputs = self.encoder(  # transformers/models/t5/modeling_t5.py:1484 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.block, accessed_by=DictGetItemGuardAccessor(block)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block, 125146543918160)     # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block.training, 8905664)    # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight, 125138740858224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.weight, 125138755268592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.weight, 125138740861296)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.weight, 125138755268688)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.weight, 125138740859568)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, 125138740866480)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight, 125138740865616)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.weight, 125138740860528)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight, 125138740860144)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight, 125138740869072)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.weight, 125138740864080)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.weight, 125138740865520)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.weight, 125138755271952)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.weight, 125138740858320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight, 125138740864656)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.weight, 125138740859856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight, 125138740864368)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight, 125138746861872)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.weight, 125138740860240)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.weight, 125138740856592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.weight, 125138740864176)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.weight, 125138740857648)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight, 125138740865040)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.weight, 125138746862736)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight, 125138746865808)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3'), accessed_by=DictGetItemGuardAccessor(3)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight, 125138746863312)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.weight, 125138740855632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.weight, 125138746852944)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.weight, 125138746859280)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.weight, 125138746866000)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight, 125138746864176)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.weight, 125138746861968)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight, 125138746863984)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4'), accessed_by=DictGetItemGuardAccessor(4)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight, 125138746865040)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.weight, 125138746853520)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.weight, 125138746858512)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.weight, 125138746855632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.weight, 125138746862928)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight, 125138746863888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.weight, 125138746862064)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight, 125138746863600)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5'), accessed_by=DictGetItemGuardAccessor(5)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight, 125138600636976)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.weight, 125138746858416)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.weight, 125138746861776)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.weight, 125138746853904)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.weight, 125138600626896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight, 125138600627088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.weight, 125138600639184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight, 125138600640432)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0], 125138692317520)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer, 125138671703248)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0], 125138697567056)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].dropout, 125138697563792)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].layer_norm, 125138596947536)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention, 125138697556240)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.k, 125138697558608)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.o, 125138697566672)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.q, 125138703409296)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.v, 125138697555664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias, 125138697565392)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, 8905664)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.is_decoder, 8905664)  # bidirectional=(not self.is_decoder),  # transformers/models/t5/modeling_t5.py:437 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, 8906112)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets, accessed_by=DictGetItemGuardAccessor(relative_attention_num_buckets)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets == 32  # num_buckets=self.relative_attention_num_buckets,  # transformers/models/t5/modeling_t5.py:438 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_max_distance, accessed_by=DictGetItemGuardAccessor(relative_attention_max_distance)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_max_distance == 128  # max_distance=self.relative_attention_max_distance,  # transformers/models/t5/modeling_t5.py:439 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1], 125137956932368)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].dropout, 125138740932048)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].layer_norm, 125138740920912)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense, 125148205499536)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wi, 125138739118672)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wo, 125138674729808)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.act, 125138740930192)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout, 125138740923664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1], 125137956932944)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer, 125138685645328)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0], 125138768050512)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].dropout, 125137953273808)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].layer_norm, 125138671021328)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention, 125138690097616)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.k, 125146861565392)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.o, 125138674977296)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.q, 125138694438736)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.v, 125138700916816)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1], 125138675356944)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].dropout, 125138401814480)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].layer_norm, 125138401816720)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense, 125138697981712)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wi, 125138697984400)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wo, 125137963208656)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.act, 125138401817936)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout, 125138401817808)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2], accessed_by=GetItemGuardAccessor(2)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2], 125138675367440)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer, 125138692431056)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0], 125138692426576)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].dropout, 125138692437392)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].layer_norm, 125138692432336)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention, 125138692433104)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.k, 125138692434512)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.o, 125138692430544)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.q, 125138692438032)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.v, 125138692430288)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1], 125138692429520)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].dropout, 125138692430032)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].layer_norm, 125138692430736)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense, 125138692427664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wi, 125138692434960)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wo, 125138692425552)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.act, 125138692426320)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout, 125138692436944)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3], accessed_by=GetItemGuardAccessor(3)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3], 125138692436112)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer, 125138692434640)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0], 125138692437776)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].dropout, 125138676485584)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].layer_norm, 125137960551568)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention, 125138692436496)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.k, 125138680553744)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.o, 125137960398736)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.q, 125138692437264)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.v, 125138680555792)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1], 125138676492176)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].dropout, 125138676492048)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].layer_norm, 125138676485008)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense, 125138676483536)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wi, 125138676483600)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wo, 125138676485840)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.act, 125138676484880)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout, 125138676487952)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4], accessed_by=GetItemGuardAccessor(4)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4], 125137959970384)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer, 125138676488144)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0], 125145565639312)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].dropout, 125137959302032)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].layer_norm, 125137958999056)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention, 125138674492688)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.k, 125138400360592)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.o, 125137961836752)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.q, 125138674495760)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.v, 125138400359824)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1], 125137959302288)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].dropout, 125137959061072)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].layer_norm, 125137959198096)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense, 125137957541328)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wi, 125137958859472)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wo, 125137959364560)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.act, 125137959198160)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout, 125137959198032)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5], accessed_by=GetItemGuardAccessor(5)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5], 125137959060816)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer, 125137959060944)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0], 125137958644240)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].dropout, 125146543920208)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].layer_norm, 125146543928848)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention, 125137958644432)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.k, 125146543929168)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.o, 125146543927888)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.q, 125138698523280)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.v, 125146543927056)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1], 125146543918992)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].dropout, 125138699030544)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].layer_norm, 125146543929616)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense, 125146543929296)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wi, 125146543929040)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wo, 125146543928336)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.act, 125146543920144)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout, 125146543919184)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.dropout, 125146543898192)   # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.dropout.training, 8905664)  # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens, accessed_by=DictGetItemGuardAccessor(embed_tokens)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens, 125138739169360)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens.training, 8905664)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens.weight, 125138755259760)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm, accessed_by=DictGetItemGuardAccessor(final_layer_norm)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm, 125138679911568)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.final_layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm.training, 8905664)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm.weight, 125138746859184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.final_layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].encoder.config, 468362288)         # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:988 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.is_decoder, 8905664)        # if self.is_decoder and encoder_hidden_states is not None:  # transformers/models/t5/modeling_t5.py:1036 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.model_parallel, 8905664)    # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:985 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:1047 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[6], accessed_by=GetItemGuardAccessor(6)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[6], 8820832)  # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[7], accessed_by=GetItemGuardAccessor(7)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[7], 8820832)  # mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length  # transformers/models/t5/modeling_t5.py:1017 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[8], accessed_by=GetItemGuardAccessor(8)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[8], 8820832)  # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:988 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask, accessed_by=GetAttrGuardAccessor(get_extended_attention_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.get_extended_attention_mask.__defaults__[0], 8820832)  # if device is not None:  # transformers/modeling_utils.py:1125 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.get_extended_attention_mask.__defaults__[1], 8820832)  # if dtype is None:  # transformers/modeling_utils.py:1120 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=L['self'].model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].model_parallel, 8905664)            # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:1503 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['head_mask'], accessed_by=DictGetItemGuardAccessor(head_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['head_mask'], 8820832)                      # if head_mask is not None and decoder_head_mask is None:  # transformers/models/t5/modeling_t5.py:1477 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['input_ids'], accessed_by=DictGetItemGuardAccessor(input_ids)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['input_ids'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.int64, device=None, requires_grad=False, size=[1, 15], stride=[15, 1])  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- NO_HASATTR: hasattr(L['input_ids'], '_dynamo_dynamic_indices') == False   # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['decoder_input_ids'])\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['use_cache'], accessed_by=DictGetItemGuardAccessor(use_cache)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['use_cache'], 8820832)                      # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8820832)                    # return_dict = return_dict if return_dict is not None else self.config.use_return_dict  # transformers/models/t5/modeling_t5.py:1474 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['inputs_embeds'], accessed_by=DictGetItemGuardAccessor(inputs_embeds)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['inputs_embeds'], 8820832)                  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['attention_mask'], accessed_by=DictGetItemGuardAccessor(attention_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['attention_mask'], 8820832)                 # if attention_mask is None:  # transformers/models/t5/modeling_t5.py:1027 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['encoder_outputs'], accessed_by=DictGetItemGuardAccessor(encoder_outputs)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['encoder_outputs'], 8820832)                # if encoder_outputs is None:  # transformers/models/t5/modeling_t5.py:1483 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['past_key_values'], accessed_by=DictGetItemGuardAccessor(past_key_values)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['past_key_values'], 8820832)                # mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length  # transformers/models/t5/modeling_t5.py:1017 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['decoder_head_mask'], accessed_by=DictGetItemGuardAccessor(decoder_head_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_head_mask'], 8820832)              # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['decoder_input_ids'], accessed_by=DictGetItemGuardAccessor(decoder_input_ids)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['decoder_input_ids'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.int64, device=None, requires_grad=False, size=[1, 4], stride=[4, 1])  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- NO_HASATTR: hasattr(L['decoder_input_ids'], '_dynamo_dynamic_indices') == False  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['decoder_input_ids'])\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 8820832)              # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/t5/modeling_t5.py:989 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['cross_attn_head_mask'], accessed_by=DictGetItemGuardAccessor(cross_attn_head_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['cross_attn_head_mask'], 8820832)           # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['output_hidden_states'], accessed_by=DictGetItemGuardAccessor(output_hidden_states)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_hidden_states'], 8820832)           # output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states  # transformers/models/t5/modeling_t5.py:991 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['decoder_inputs_embeds'], accessed_by=DictGetItemGuardAccessor(decoder_inputs_embeds)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_inputs_embeds'], 8820832)          # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=L['decoder_attention_mask'], accessed_by=DictGetItemGuardAccessor(decoder_attention_mask)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_attention_mask'], 8820832)         # if attention_mask is None:  # transformers/models/t5/modeling_t5.py:1027 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['nn'], accessed_by=DictGetItemGuardAccessor(nn)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['nn'], 125150694903136)                     # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['nn'].functional, accessed_by=GetAttrGuardAccessor(functional)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional, 125150684383248)          # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['nn'].functional.dropout, accessed_by=GetAttrGuardAccessor(dropout)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional.dropout, 125150682577344)  # attn_weights = nn.functional.dropout(  # transformers/models/t5/modeling_t5.py:556 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['nn'].functional.softmax, accessed_by=GetAttrGuardAccessor(softmax)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional.softmax, 125150682596992)  # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['math'], accessed_by=DictGetItemGuardAccessor(math)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['math'], 125150909974864)                   # / math.log(max_distance / max_exact)  # transformers/models/t5/modeling_t5.py:418 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['math'].log, accessed_by=GetAttrGuardAccessor(log)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['math'].log, 125150908715936)               # / math.log(max_distance / max_exact)  # transformers/models/t5/modeling_t5.py:418 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 125150850121008)                  # attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)  # transformers/models/t5/modeling_t5.py:1028 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].abs, accessed_by=GetAttrGuardAccessor(abs)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].abs, 125150839410016)              # relative_position = torch.abs(relative_position)  # transformers/models/t5/modeling_t5.py:406 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].log, accessed_by=GetAttrGuardAccessor(log)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].log, 125150839481792)              # torch.log(relative_position.float() / max_exact)  # transformers/models/t5/modeling_t5.py:417 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].min, accessed_by=GetAttrGuardAccessor(min)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].min, 125150839483232)              # relative_position_if_large = torch.min(  # transformers/models/t5/modeling_t5.py:421 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].long, accessed_by=GetAttrGuardAccessor(long)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['torch'].long == torch.int64                                # context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]  # transformers/models/t5/modeling_t5.py:432 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].ones, accessed_by=GetAttrGuardAccessor(ones)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].ones, 125150839392032)             # attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)  # transformers/models/t5/modeling_t5.py:1028 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 125150839422576)            # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].where, accessed_by=GetAttrGuardAccessor(where)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].where, 125150839522224)            # relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)  # transformers/models/t5/modeling_t5.py:425 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].zeros, accessed_by=GetAttrGuardAccessor(zeros)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].zeros, 125150839396512)            # position_bias = torch.zeros(  # transformers/models/t5/modeling_t5.py:529 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].Tensor, accessed_by=GetAttrGuardAccessor(Tensor)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].Tensor, 101225952)                 # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].arange, accessed_by=GetAttrGuardAccessor(arange)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].arange, 125150839348320)           # context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]  # transformers/models/t5/modeling_t5.py:432 in compute_bias\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 125150839356640)           # scores = torch.matmul(  # transformers/models/t5/modeling_t5.py:523 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].float16, accessed_by=GetAttrGuardAccessor(float16)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['torch'].float16 == torch.float16                           # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].float32, accessed_by=GetAttrGuardAccessor(float32)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['torch'].float32 == torch.float32                           # variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)  # transformers/models/t5/modeling_t5.py:245 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].bfloat16, accessed_by=GetAttrGuardAccessor(bfloat16)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['torch'].bfloat16 == torch.bfloat16                         # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].full_like, accessed_by=GetAttrGuardAccessor(full_like)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].full_like, 125150839479232)        # relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)  # transformers/models/t5/modeling_t5.py:422 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['torch'].zeros_like, accessed_by=GetAttrGuardAccessor(zeros_like)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].zeros_like, 125150839396752)       # relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))  # transformers/models/t5/modeling_t5.py:408 in _relative_position_bucket\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['Seq2SeqModelOutput'], accessed_by=DictGetItemGuardAccessor(Seq2SeqModelOutput)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['Seq2SeqModelOutput'], 151732880)           # return Seq2SeqModelOutput(  # transformers/models/t5/modeling_t5.py:1532 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['__builtins_dict___55'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___55)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['len'], accessed_by=DictGetItemGuardAccessor(len)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['len'], 125150921102240)  # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['str'], accessed_by=DictGetItemGuardAccessor(str)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['str'], 8799936)    # if isinstance(k, str):  # transformers/utils/generic.py:429 in __getitem__\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['zip'], accessed_by=DictGetItemGuardAccessor(zip)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['zip'], 8789216)    # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['dict'], accessed_by=DictGetItemGuardAccessor(dict)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['dict'], 8835648)   # inner_dict = dict(self.items())  # transformers/utils/generic.py:430 in __getitem__\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['tuple'], accessed_by=DictGetItemGuardAccessor(tuple)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['tuple'], 8810304)  # return tuple(self[k] for k in self.keys())  # transformers/utils/generic.py:458 in to_tuple\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['enumerate'], accessed_by=DictGetItemGuardAccessor(enumerate)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['enumerate'], 8885760)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__builtins_dict___55']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___55']['isinstance'], 125150921101920)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_modeling_utils)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'], 125148096836560)  # return get_parameter_dtype(self)  # transformers/modeling_utils.py:1051 in dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch, 125150850121008)  # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min  # transformers/modeling_utils.py:1154 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch.finfo, accessed_by=GetAttrGuardAccessor(finfo)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch.finfo, 125150511418784)  # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min  # transformers/modeling_utils.py:1154 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch.arange, accessed_by=GetAttrGuardAccessor(arange)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch.arange, 125150839348320)  # seq_ids = torch.arange(seq_length, device=device)  # transformers/modeling_utils.py:1086 in create_extended_attention_mask_for_decoder\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].XLA_USE_BF16, accessed_by=GetAttrGuardAccessor(XLA_USE_BF16)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].XLA_USE_BF16 == '0'  # if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:259 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin, accessed_by=GetAttrGuardAccessor(ModuleUtilsMixin)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin, 156049440)  # extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(  # transformers/modeling_utils.py:1138 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder, accessed_by=GetAttrGuardAccessor(create_extended_attention_mask_for_decoder)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder.__code__, 151700000)  # extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(  # transformers/modeling_utils.py:1138 in get_extended_attention_mask\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].XLA_DOWNCAST_BF16, accessed_by=GetAttrGuardAccessor(XLA_DOWNCAST_BF16)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].XLA_DOWNCAST_BF16 == '0'  # if XLA_DOWNCAST_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:261 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].get_parameter_dtype, accessed_by=GetAttrGuardAccessor(get_parameter_dtype)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].get_parameter_dtype.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].get_parameter_dtype.__code__, 151815504)  # return get_parameter_dtype(self)  # transformers/modeling_utils.py:1051 in dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ENV_VARS_TRUE_VALUES, accessed_by=GetAttrGuardAccessor(ENV_VARS_TRUE_VALUES)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].ENV_VARS_TRUE_VALUES == {'YES', 'ON', '1', 'TRUE'}  # if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:259 in get_parameter_dtype\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['BaseModelOutputWithPastAndCrossAttentions'], accessed_by=DictGetItemGuardAccessor(BaseModelOutputWithPastAndCrossAttentions)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['BaseModelOutputWithPastAndCrossAttentions'], 151709408)  # return BaseModelOutputWithPastAndCrossAttentions(  # transformers/models/t5/modeling_t5.py:1167 in forward\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_utils_dot_generic'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_utils_dot_generic)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_utils_dot_generic'], 125148405357984)  # if isinstance(k, str):  # transformers/utils/generic.py:429 in __getitem__\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 125150694908736)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch, 125150850121008)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, accessed_by=GetAttrGuardAccessor(_C)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, 125150839212848)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, accessed_by=GetAttrGuardAccessor(_get_tracing_state)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, 125150786030384)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:11.806000 125150921582400 torch/_dynamo/guards.py:2148] [4/0] [__guards] \n",
      "V1024 00:57:12.277000 125150921582400 torch/_dynamo/guards.py:2611] [4/1] [__recompiles] Recompiling function forward in /home/gaurav/anaconda3/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1429\n",
      "V1024 00:57:12.277000 125150921582400 torch/_dynamo/guards.py:2611] [4/1] [__recompiles]     triggered by the following guard failure(s):\n",
      "V1024 00:57:12.277000 125150921582400 torch/_dynamo/guards.py:2611] [4/1] [__recompiles]     - GLOBAL_STATE changed: grad_mode \n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2169] [4/1] [__guards] GUARDS:\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] \n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] +- RootGuardManager\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125137956418832)                   # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8905664)                  # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=L['self'].config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(L['self'].config, 468362288)                 # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=L['self'].decoder, accessed_by=DictGetItemGuardAccessor(decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder, 125146543897808)           # decoder_outputs = self.decoder(  # transformers/models/t5/modeling_t5.py:1514 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=L['self'].decoder.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.__dict__)   # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.training, 8905664)          # decoder_outputs = self.decoder(  # transformers/models/t5/modeling_t5.py:1514 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.block, accessed_by=DictGetItemGuardAccessor(block)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block, 125138699587216)     # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block.training, 8905664)    # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '0').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight, 125138600635536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.k.weight, 125138600630832)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.o.weight, 125138600636208)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.q.weight, 125138600630448)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.v.weight, 125138600636112)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, 125138600631504)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight, 125138600629680)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.k.weight, 125138600634576)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.o.weight, 125138600636304)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.q.weight, 125138600625456)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '1').EncDecAttention.v.weight, 125138600639856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight, 125138600624304)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wi.weight, 125138600625360)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight, 125138600629296)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '1').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight, 125138853503888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.k.weight, 125138853504368)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.o.weight, 125138853510896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.q.weight, 125138600638320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '0').SelfAttention.v.weight, 125138600639088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight, 125138853504464)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.k.weight, 125138853508592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.o.weight, 125138853507056)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.q.weight, 125138853508400)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '1').EncDecAttention.v.weight, 125138853507344)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight, 125138853507248)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wi.weight, 125138600626032)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight, 125138867923536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '2').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight, 125138867927472)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.k.weight, 125138853507536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.o.weight, 125138867927280)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.q.weight, 125138867926704)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '0').SelfAttention.v.weight, 125138867926992)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight, 125138867928720)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.k.weight, 125138867923920)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.o.weight, 125138867927184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.q.weight, 125138867928816)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '1').EncDecAttention.v.weight, 125138867926224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight, 125138750823632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wi.weight, 125138867927856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight, 125138750831888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3'), accessed_by=DictGetItemGuardAccessor(3)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '3').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight, 125138750826704)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.k.weight, 125138750828048)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.o.weight, 125138750830256)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.q.weight, 125138867923728)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '0').SelfAttention.v.weight, 125138750826896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight, 125138750826800)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.k.weight, 125138750831312)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.o.weight, 125138750830160)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.q.weight, 125138750823536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '1').EncDecAttention.v.weight, 125138750820848)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight, 125138750829776)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wi.weight, 125138750831024)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight, 125138750822000)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4'), accessed_by=DictGetItemGuardAccessor(4)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '4').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight, 125138750830544)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.k.weight, 125138750829968)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.o.weight, 125138750825360)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.q.weight, 125138750826224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '0').SelfAttention.v.weight, 125138750827568)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight, 125138750830064)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.k.weight, 125138851307664)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.o.weight, 125138851305552)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.q.weight, 125138851303536)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '1').EncDecAttention.v.weight, 125138851304208)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight, 125138851307088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wi.weight, 125138750826320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight, 125138851304784)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5'), accessed_by=DictGetItemGuardAccessor(5)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].decoder.block, '5').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight, 125138851301712)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.k.weight, 125138851303920)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.o.weight, 125138851310352)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.q.weight, 125138750831504)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '0').SelfAttention.v.weight, 125138851303152)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight, 125138851304880)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.k.weight, 125138851309584)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.o.weight, 125138851305456)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.q.weight, 125138851314864)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '1').EncDecAttention.v.weight, 125138851308144)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight, 125138851308624)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wi.weight, 125138851301232)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight, 125138851299984)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0], 125146543897424)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer, 125146543895888)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0], 125146543887312)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].dropout, 125138675931792)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].layer_norm, 125137961899536)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '0').layer_norm.weight is L['self'].decoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention, 125146543892688)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.k, 125137950235792)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.o, 125138599605968)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.q, 125137950708368)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.v, 125137960731728)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias, 125138580192208)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, 8905664)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.is_decoder, 8906112)  # bidirectional=(not self.is_decoder),  # transformers/models/t5/modeling_t5.py:437 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, 8906112)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets, accessed_by=DictGetItemGuardAccessor(relative_attention_num_buckets)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets == 32  # num_buckets=self.relative_attention_num_buckets,  # transformers/models/t5/modeling_t5.py:438 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_max_distance, accessed_by=DictGetItemGuardAccessor(relative_attention_max_distance)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[0].SelfAttention.relative_attention_max_distance == 128  # max_distance=self.relative_attention_max_distance,  # transformers/models/t5/modeling_t5.py:439 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1], 125138675939216)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].dropout, 125138695387920)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].layer_norm, 125138695382416)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '1').layer_norm.weight is L['self'].decoder.block[0].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention, 125138698647632)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.k, 125138678508816)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.o, 125138695386448)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.q, 125138677900816)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.v, 125138695383952)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:532 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1].EncDecAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[1].EncDecAttention.has_relative_attention_bias, 8905664)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1], 125138695373072)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].dropout, 125138695383632)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].layer_norm, 125138695386640)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[0].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').layer_norm.weight is L['self'].decoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense, 125138695384656)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[0].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wi, 125138695384464)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wo, 125138695384592)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '0').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.act, 125138695386256)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout, 125138695375120)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[0].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1], 125138675931984)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer, 125138695380304)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0], 125138695377488)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].dropout, 125146861621136)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].layer_norm, 125146861624784)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '0').layer_norm.weight is L['self'].decoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention, 125138695387408)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.k, 125138673072592)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.o, 125137946498064)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.q, 125138695385232)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.v, 125138400002576)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1], 125146861625232)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].dropout, 125137963270864)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].layer_norm, 125137963270608)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '1').layer_norm.weight is L['self'].decoder.block[1].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention, 125146861624656)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.k, 125138673792528)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.o, 125138690743824)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.q, 125146861625168)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.v, 125138690751696)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1], 125137963270928)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].dropout, 125138738826896)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].layer_norm, 125138738825744)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[1].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').layer_norm.weight is L['self'].decoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense, 125137962842768)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[1].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wi, 125137962842832)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wo, 125137962842704)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '1').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.act, 125137947579728)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout, 125137962842000)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[1].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2], accessed_by=GetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2], 125137962235088)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer, 125138673896976)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0], 125137963042192)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].dropout, 125138673316432)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].layer_norm, 125138673318480)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '0').layer_norm.weight is L['self'].decoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention, 125137963042128)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.k, 125138578403984)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.o, 125137963407760)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.q, 125137963042000)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.v, 125137963407824)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1], 125138673310736)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].dropout, 125138587312400)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].layer_norm, 125138587318544)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '1').layer_norm.weight is L['self'].decoder.block[2].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention, 125138673312464)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.k, 125137960778064)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.o, 125138857733840)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.q, 125137947380560)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.v, 125138576607568)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1], 125137961097296)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].dropout, 125137946940176)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].layer_norm, 125137947967312)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[2].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').layer_norm.weight is L['self'].decoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense, 125137961097872)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[2].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wi, 125138683385616)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wo, 125138738129936)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '2').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.act, 125138738127888)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout, 125138738130448)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[2].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[2]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3], accessed_by=GetItemGuardAccessor(3)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3], 125138673304592)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer, 125138601051408)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0], 125138601054928)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].dropout, 125138672743248)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].layer_norm, 125138672742736)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '0').layer_norm.weight is L['self'].decoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention, 125138601058768)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.k, 125137962372688)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.o, 125137946070864)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.q, 125138601056144)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.v, 125137962372112)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1], 125138672745808)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].dropout, 125138672743824)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].layer_norm, 125138672744976)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '1').layer_norm.weight is L['self'].decoder.block[3].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention, 125138672745296)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.k, 125138672738960)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.o, 125138672731856)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.q, 125138672731280)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.v, 125138672746064)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1], 125138675445072)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].dropout, 125138700328080)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].layer_norm, 125138681881040)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[3].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').layer_norm.weight is L['self'].decoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense, 125137947898896)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[3].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wi, 125137947899472)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wo, 125138672680912)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '3').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.act, 125138672676688)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout, 125138672675856)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[3].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[3]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4], accessed_by=GetItemGuardAccessor(4)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4], 125138672736976)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer, 125137962509648)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0], 125137946269456)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].dropout, 125138860636048)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].layer_norm, 125137961052048)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '0').layer_norm.weight is L['self'].decoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention, 125137946270032)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.k, 125137962872272)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.o, 125138701836304)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.q, 125138400150544)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.v, 125137960914512)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1], 125138701195728)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].dropout, 125138701205072)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].layer_norm, 125138701195152)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '1').layer_norm.weight is L['self'].decoder.block[4].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention, 125138701197840)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.k, 125138701189392)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.o, 125138701199632)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.q, 125138701195408)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.v, 125138701191184)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1], 125138701202384)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].dropout, 125138701195600)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].layer_norm, 125138701197584)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[4].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').layer_norm.weight is L['self'].decoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense, 125138701202000)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[4].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wi, 125138701193360)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wo, 125138701202320)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '4').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.act, 125138701201168)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout, 125138701195536)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[4].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[4]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5], accessed_by=GetItemGuardAccessor(5)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5], 125137959501456)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer, 125137959501200)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0], 125138678053392)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].dropout, 125138699588304)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].layer_norm, 125138699591504)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '0').layer_norm.weight is L['self'].decoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention, 125138678048272)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.k, 125138575404944)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.o, 125138699588112)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.q, 125137963071440)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.v, 125138699586448)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1], 125138699593872)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].training, 8905664)  # cross_attention_outputs = self.layer[1](  # transformers/models/t5/modeling_t5.py:716 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].dropout, 125138740978384)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].dropout.training, 8905664)  # layer_output = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:638 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].layer_norm, 125138740976336)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:626 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '1').layer_norm.weight is L['self'].decoder.block[5].layer[1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention, accessed_by=DictGetItemGuardAccessor(EncDecAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention, 125138740973200)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[1].EncDecAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.training, 8905664)  # attention_output = self.EncDecAttention(  # transformers/models/t5/modeling_t5.py:627 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.k, 125138740972368)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.o, 125138740982864)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.q, 125138740973584)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.v, 125138740976208)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[1].EncDecAttention.is_decoder, 8906112)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[1].EncDecAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1].EncDecAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1], 125138740977232)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].dropout, 125138674408592)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].layer_norm, 125138698148752)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.block[5].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').layer_norm.weight is L['self'].decoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense, 125138740985680)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.block[5].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wi, 125138740982672)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wo, 125138700184464)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].decoder.block, '5').layer, '2').DenseReluDense.wo.weight is L['self'].decoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.act, 125138695795536)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout, 125138695795984)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.block[5].is_decoder, 8906112)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.block[5]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.dropout, 125138674403856)   # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.dropout.training, 8905664)  # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens, accessed_by=DictGetItemGuardAccessor(embed_tokens)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.embed_tokens, 125138739169360)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.embed_tokens.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.embed_tokens.training, 8905664)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm, accessed_by=DictGetItemGuardAccessor(final_layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm, 125138699712912)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].decoder.final_layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm.training, 8905664)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.final_layer_norm.weight, 125138851303728)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].decoder.final_layer_norm.weight is L['self'].decoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- EQUALS_MATCH: L['self'].decoder.final_layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].decoder.final_layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].decoder.config, 468362288)         # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/t5/modeling_t5.py:989 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.is_decoder, 8906112)        # if not self.is_decoder:  # transformers/models/t5/modeling_t5.py:1020 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.model_parallel, 8905664)    # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:985 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:1047 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask, accessed_by=GetAttrGuardAccessor(get_extended_attention_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.get_extended_attention_mask.__defaults__[0], 8820832)  # if device is not None:  # transformers/modeling_utils.py:1079 in create_extended_attention_mask_for_decoder\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].decoder.get_extended_attention_mask.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].decoder.get_extended_attention_mask.__defaults__[1], 8820832)  # if dtype is None:  # transformers/modeling_utils.py:1120 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=L['self'].encoder, accessed_by=DictGetItemGuardAccessor(encoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder, 125137949124688)           # encoder_outputs = self.encoder(  # transformers/models/t5/modeling_t5.py:1484 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.__dict__)   # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.training, 8905664)          # encoder_outputs = self.encoder(  # transformers/models/t5/modeling_t5.py:1484 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.block, accessed_by=DictGetItemGuardAccessor(block)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block, 125146543918160)     # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block.training, 8905664)    # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '0').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight, 125138740858224)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.k.weight, 125138755268592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.o.weight, 125138740861296)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.q.weight, 125138755268688)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.v.weight, 125138740859568)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '0').SelfAttention.relative_attention_bias.weight, 125138740866480)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight, 125138740865616)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wi.weight, 125138740860528)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight, 125138740860144)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '1').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight, 125138740869072)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.k.weight, 125138740864080)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.o.weight, 125138740865520)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.q.weight, 125138755271952)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '0').SelfAttention.v.weight, 125138740858320)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight, 125138740864656)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wi.weight, 125138740859856)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight, 125138740864368)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2'), accessed_by=DictGetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '2').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight, 125138746861872)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.k.weight, 125138740860240)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.o.weight, 125138740856592)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.q.weight, 125138740864176)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '0').SelfAttention.v.weight, 125138740857648)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight, 125138740865040)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wi.weight, 125138746862736)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight, 125138746865808)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3'), accessed_by=DictGetItemGuardAccessor(3)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '3').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight, 125138746863312)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.k.weight, 125138740855632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.o.weight, 125138746852944)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.q.weight, 125138746859280)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '0').SelfAttention.v.weight, 125138746866000)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight, 125138746864176)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wi.weight, 125138746861968)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight, 125138746863984)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4'), accessed_by=DictGetItemGuardAccessor(4)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '4').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight, 125138746865040)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.k.weight, 125138746853520)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.o.weight, 125138746858512)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.q.weight, 125138746855632)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '0').SelfAttention.v.weight, 125138746862928)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight, 125138746863888)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wi.weight, 125138746862064)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight, 125138746863600)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5'), accessed_by=DictGetItemGuardAccessor(5)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].encoder.block, '5').layer._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight, 125138600636976)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.k.weight, 125138746858416)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.o.weight, 125138746861776)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.q.weight, 125138746853904)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '0').SelfAttention.v.weight, 125138600626896)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight, 125138600627088)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wi.weight, 125138600639184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight, 125138600640432)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0], 125138692317520)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer, 125138671703248)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0], 125138697567056)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].dropout, 125138697563792)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].layer_norm, 125138596947536)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '0').layer_norm.weight is L['self'].encoder.block[0].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention, 125138697556240)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.k, 125138697558608)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.o, 125138697566672)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.q, 125138703409296)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.v, 125138697555664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias, accessed_by=DictGetItemGuardAccessor(relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias, 125138697565392)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_bias.training, 8905664)  # values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)  # transformers/models/t5/modeling_t5.py:441 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.is_decoder, 8905664)  # bidirectional=(not self.is_decoder),  # transformers/models/t5/modeling_t5.py:437 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, accessed_by=DictGetItemGuardAccessor(has_relative_attention_bias)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.has_relative_attention_bias, 8906112)  # if not self.has_relative_attention_bias:  # transformers/models/t5/modeling_t5.py:528 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets, accessed_by=DictGetItemGuardAccessor(relative_attention_num_buckets)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_num_buckets == 32  # num_buckets=self.relative_attention_num_buckets,  # transformers/models/t5/modeling_t5.py:438 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_max_distance, accessed_by=DictGetItemGuardAccessor(relative_attention_max_distance)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[0].SelfAttention.relative_attention_max_distance == 128  # max_distance=self.relative_attention_max_distance,  # transformers/models/t5/modeling_t5.py:439 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1], 125137956932368)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].dropout, 125138740932048)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].layer_norm, 125138740920912)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[0].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').layer_norm.weight is L['self'].encoder.block[0].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense, 125148205499536)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[0].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wi, 125138739118672)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wo, 125138674729808)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '0').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[0].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.act, 125138740930192)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout, 125138740923664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[0].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1], 125137956932944)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer, 125138685645328)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0], 125138768050512)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].dropout, 125137953273808)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].layer_norm, 125138671021328)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '0').layer_norm.weight is L['self'].encoder.block[1].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention, 125138690097616)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.k, 125146861565392)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.o, 125138674977296)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.q, 125138694438736)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.v, 125138700916816)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1], 125138675356944)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].dropout, 125138401814480)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].layer_norm, 125138401816720)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[1].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').layer_norm.weight is L['self'].encoder.block[1].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense, 125138697981712)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[1].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wi, 125138697984400)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wo, 125137963208656)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '1').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[1].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.act, 125138401817936)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout, 125138401817808)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[1].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2], accessed_by=GetItemGuardAccessor(2)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2], 125138675367440)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer, 125138692431056)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0], 125138692426576)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].dropout, 125138692437392)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].layer_norm, 125138692432336)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '0').layer_norm.weight is L['self'].encoder.block[2].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention, 125138692433104)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.k, 125138692434512)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.o, 125138692430544)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.q, 125138692438032)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.v, 125138692430288)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1], 125138692429520)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].dropout, 125138692430032)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].layer_norm, 125138692430736)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[2].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').layer_norm.weight is L['self'].encoder.block[2].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense, 125138692427664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[2].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wi, 125138692434960)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wo, 125138692425552)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '2').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[2].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.act, 125138692426320)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout, 125138692436944)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[2].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[2]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3], accessed_by=GetItemGuardAccessor(3)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3], 125138692436112)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer, 125138692434640)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0], 125138692437776)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].dropout, 125138676485584)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].layer_norm, 125137960551568)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '0').layer_norm.weight is L['self'].encoder.block[3].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention, 125138692436496)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.k, 125138680553744)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.o, 125137960398736)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.q, 125138692437264)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.v, 125138680555792)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1], 125138676492176)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].dropout, 125138676492048)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].layer_norm, 125138676485008)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[3].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').layer_norm.weight is L['self'].encoder.block[3].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense, 125138676483536)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[3].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wi, 125138676483600)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wo, 125138676485840)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '3').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[3].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.act, 125138676484880)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout, 125138676487952)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[3].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[3]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4], accessed_by=GetItemGuardAccessor(4)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4], 125137959970384)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer, 125138676488144)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0], 125145565639312)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].dropout, 125137959302032)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].layer_norm, 125137958999056)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '0').layer_norm.weight is L['self'].encoder.block[4].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention, 125138674492688)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.k, 125138400360592)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.o, 125137961836752)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.q, 125138674495760)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.v, 125138400359824)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1], 125137959302288)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].dropout, 125137959061072)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].layer_norm, 125137959198096)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[4].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').layer_norm.weight is L['self'].encoder.block[4].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense, 125137957541328)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[4].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wi, 125137958859472)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wo, 125137959364560)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '4').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[4].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.act, 125137959198160)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout, 125137959198032)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[4].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[4]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5], accessed_by=GetItemGuardAccessor(5)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5], 125137959060816)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].training, 8905664)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer, accessed_by=DictGetItemGuardAccessor(layer)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer, 125137959060944)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer.training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0], 125137958644240)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].training, 8905664)  # self_attention_outputs = self.layer[0](  # transformers/models/t5/modeling_t5.py:686 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].dropout, 125146543920208)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(attention_output[0])  # transformers/models/t5/modeling_t5.py:602 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].layer_norm, 125146543928848)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].layer_norm.training, 8905664)  # normed_hidden_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:592 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '0').layer_norm.weight is L['self'].encoder.block[5].layer[0].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention, accessed_by=DictGetItemGuardAccessor(SelfAttention)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention, 125137958644432)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[0].SelfAttention.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.training, 8905664)  # attention_output = self.SelfAttention(  # transformers/models/t5/modeling_t5.py:593 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k, accessed_by=DictGetItemGuardAccessor(k)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.k, 125146543929168)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.k.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.k.training, 8905664)  # hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:516 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o, accessed_by=DictGetItemGuardAccessor(o)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.o, 125146543927888)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.o.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.o.training, 8905664)  # attn_output = self.o(attn_output)  # transformers/models/t5/modeling_t5.py:565 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q, accessed_by=DictGetItemGuardAccessor(q)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.q, 125138698523280)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.q.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.q.training, 8905664)  # query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)  # transformers/models/t5/modeling_t5.py:512 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v, accessed_by=DictGetItemGuardAccessor(v)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.v, 125146543927056)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.v.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.v.training, 8905664)  # hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None  # transformers/models/t5/modeling_t5.py:519 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.dropout == 0.1  # attn_weights, p=self.dropout, training=self.training  # transformers/models/t5/modeling_t5.py:557 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.n_heads, accessed_by=DictGetItemGuardAccessor(n_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.n_heads == 8  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.inner_dim, accessed_by=DictGetItemGuardAccessor(inner_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.inner_dim == 512  # return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)  # transformers/models/t5/modeling_t5.py:482 in unshape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.is_decoder, 8905664)  # present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None  # transformers/models/t5/modeling_t5.py:567 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.pruned_heads, accessed_by=DictGetItemGuardAccessor(pruned_heads)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.pruned_heads == set()  # if self.pruned_heads:  # transformers/models/t5/modeling_t5.py:545 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.key_value_proj_dim, accessed_by=DictGetItemGuardAccessor(key_value_proj_dim)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[0].SelfAttention.key_value_proj_dim == 64  # return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)  # transformers/models/t5/modeling_t5.py:478 in shape\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[0].SelfAttention.forward.__defaults__[1], 8820832)  # key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]  # transformers/models/t5/modeling_t5.py:474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[0]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1], accessed_by=GetItemGuardAccessor(-1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1], 125146543918992)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].training, 8905664)  # hidden_states = self.layer[-1](hidden_states)  # transformers/models/t5/modeling_t5.py:746 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].dropout, 125138699030544)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].dropout.training, 8905664)  # hidden_states = hidden_states + self.dropout(forwarded_states)  # transformers/models/t5/modeling_t5.py:336 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm, accessed_by=DictGetItemGuardAccessor(layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].layer_norm, 125146543929616)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].layer_norm.training, 8905664)  # forwarded_states = self.layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:334 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.block[5].layer[-1].layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').layer_norm.weight is L['self'].encoder.block[5].layer[-1].layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense, accessed_by=DictGetItemGuardAccessor(DenseReluDense)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense, 125146543929296)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.block[5].layer[-1].DenseReluDense.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.training, 8905664)  # forwarded_states = self.DenseReluDense(forwarded_states)  # transformers/models/t5/modeling_t5.py:335 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi, accessed_by=DictGetItemGuardAccessor(wi)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wi, 125146543929040)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wi.training, 8905664)  # hidden_states = self.wi(hidden_states)  # transformers/models/t5/modeling_t5.py:280 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo, accessed_by=DictGetItemGuardAccessor(wo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wo, 125146543928336)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.training, 8905664)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # and hidden_states.dtype != self.wo.weight.dtype  # transformers/models/t5/modeling_t5.py:285 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | | +- TENSOR_ALIASING: getattr(getattr(L['self'].encoder.block, '5').layer, '1').DenseReluDense.wo.weight is L['self'].encoder.block[5].layer[-1].DenseReluDense.wo.weight  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act, accessed_by=DictGetItemGuardAccessor(act)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.act, 125146543920144)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.act.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.act.training, 8905664)  # hidden_states = self.act(hidden_states)  # transformers/models/t5/modeling_t5.py:281 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout, 125146543919184)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].layer[-1].DenseReluDense.dropout.training, 8905664)  # hidden_states = self.dropout(hidden_states)  # transformers/models/t5/modeling_t5.py:282 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1].DenseReluDense._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].layer[-1]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5].is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.block[5].is_decoder, 8905664)  # do_cross_attention = self.is_decoder and encoder_hidden_states is not None  # transformers/models/t5/modeling_t5.py:707 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.block[5]._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.dropout, accessed_by=DictGetItemGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.dropout, 125146543898192)   # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.dropout.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.dropout.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.dropout.training, 8905664)  # hidden_states = self.dropout(inputs_embeds)  # transformers/models/t5/modeling_t5.py:1064 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens, accessed_by=DictGetItemGuardAccessor(embed_tokens)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens, 125138739169360)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens.training, 8905664)  # if self.embed_tokens is None:  # transformers/models/t5/modeling_t5.py:1010 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.embed_tokens.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.embed_tokens.weight, 125138755259760)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm, accessed_by=DictGetItemGuardAccessor(final_layer_norm)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm, 125138679911568)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self'].encoder.final_layer_norm.__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm.training, 8905664)  # hidden_states = self.final_layer_norm(hidden_states)  # transformers/models/t5/modeling_t5.py:1148 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._parameters, accessed_by=DictGetItemGuardAccessor(_parameters)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.weight, accessed_by=DictGetItemGuardAccessor(weight)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.final_layer_norm.weight, 125138746859184)  # for t in parameter.parameters():  # transformers/modeling_utils.py:252 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # return self.weight * hidden_states  # transformers/models/t5/modeling_t5.py:252 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | | +- TENSOR_ALIASING: L['self'].encoder.final_layer_norm.weight is L['self'].encoder.final_layer_norm.weight  # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm.variance_epsilon, accessed_by=DictGetItemGuardAccessor(variance_epsilon)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | | +- EQUALS_MATCH: L['self'].encoder.final_layer_norm.variance_epsilon == 1e-06  # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | | +- GuardManager: source=L['self'].encoder.final_layer_norm._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.config, accessed_by=DictGetItemGuardAccessor(config)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- TYPE_MATCH: ___check_type_id(L['self'].encoder.config, 468362288)         # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:988 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.is_decoder, accessed_by=DictGetItemGuardAccessor(is_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.is_decoder, 8905664)        # if self.is_decoder and encoder_hidden_states is not None:  # transformers/models/t5/modeling_t5.py:1036 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.model_parallel, 8905664)    # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:985 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.gradient_checkpointing, accessed_by=DictGetItemGuardAccessor(gradient_checkpointing)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.gradient_checkpointing, 8905664)  # if self.gradient_checkpointing and self.training:  # transformers/models/t5/modeling_t5.py:1047 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.forward, accessed_by=GetAttrGuardAccessor(forward)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.forward, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[6], accessed_by=GetItemGuardAccessor(6)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[6], 8820832)  # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[7], accessed_by=GetItemGuardAccessor(7)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[7], 8820832)  # mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length  # transformers/models/t5/modeling_t5.py:1017 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.forward.__defaults__[8], accessed_by=GetItemGuardAccessor(8)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.forward.__defaults__[8], 8820832)  # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:988 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask, accessed_by=GetAttrGuardAccessor(get_extended_attention_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask, accessed_by=FuncDefaultsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask.__defaults__[0], accessed_by=GetItemGuardAccessor(0)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.get_extended_attention_mask.__defaults__[0], 8820832)  # if device is not None:  # transformers/modeling_utils.py:1125 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | +- GuardManager: source=L['self'].encoder.get_extended_attention_mask.__defaults__[1], accessed_by=GetItemGuardAccessor(1)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].encoder.get_extended_attention_mask.__defaults__[1], 8820832)  # if dtype is None:  # transformers/modeling_utils.py:1120 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=L['self'].model_parallel, accessed_by=DictGetItemGuardAccessor(model_parallel)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].model_parallel, 8905664)            # if self.model_parallel:  # transformers/models/t5/modeling_t5.py:1503 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['head_mask'], accessed_by=DictGetItemGuardAccessor(head_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['head_mask'], 8820832)                      # if head_mask is not None and decoder_head_mask is None:  # transformers/models/t5/modeling_t5.py:1477 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['input_ids'], accessed_by=DictGetItemGuardAccessor(input_ids)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['input_ids'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.int64, device=None, requires_grad=False, size=[1, 15], stride=[15, 1])  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- NO_HASATTR: hasattr(L['input_ids'], '_dynamo_dynamic_indices') == False   # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['decoder_input_ids'])\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['use_cache'], accessed_by=DictGetItemGuardAccessor(use_cache)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['use_cache'], 8820832)                      # use_cache = use_cache if use_cache is not None else self.config.use_cache  # transformers/models/t5/modeling_t5.py:1473 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['return_dict'], accessed_by=DictGetItemGuardAccessor(return_dict)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['return_dict'], 8820832)                    # return_dict = return_dict if return_dict is not None else self.config.use_return_dict  # transformers/models/t5/modeling_t5.py:1474 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['inputs_embeds'], accessed_by=DictGetItemGuardAccessor(inputs_embeds)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['inputs_embeds'], 8820832)                  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['attention_mask'], accessed_by=DictGetItemGuardAccessor(attention_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['attention_mask'], 8820832)                 # if attention_mask is None:  # transformers/models/t5/modeling_t5.py:1027 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['encoder_outputs'], accessed_by=DictGetItemGuardAccessor(encoder_outputs)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['encoder_outputs'], 8820832)                # if encoder_outputs is None:  # transformers/models/t5/modeling_t5.py:1483 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['past_key_values'], accessed_by=DictGetItemGuardAccessor(past_key_values)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['past_key_values'], 8820832)                # mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length  # transformers/models/t5/modeling_t5.py:1017 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['decoder_head_mask'], accessed_by=DictGetItemGuardAccessor(decoder_head_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_head_mask'], 8820832)              # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['decoder_input_ids'], accessed_by=DictGetItemGuardAccessor(decoder_input_ids)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['decoder_input_ids'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.int64, device=None, requires_grad=False, size=[1, 4], stride=[4, 1])  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- NO_HASATTR: hasattr(L['decoder_input_ids'], '_dynamo_dynamic_indices') == False  # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- NO_TENSOR_ALIASING: check_no_aliasing(L['input_ids'], L['decoder_input_ids'])\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['output_attentions'], accessed_by=DictGetItemGuardAccessor(output_attentions)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_attentions'], 8820832)              # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions  # transformers/models/t5/modeling_t5.py:989 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['cross_attn_head_mask'], accessed_by=DictGetItemGuardAccessor(cross_attn_head_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['cross_attn_head_mask'], 8820832)           # if head_mask is not None:  # transformers/modeling_utils.py:1175 in get_head_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['output_hidden_states'], accessed_by=DictGetItemGuardAccessor(output_hidden_states)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['output_hidden_states'], 8820832)           # output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states  # transformers/models/t5/modeling_t5.py:991 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['decoder_inputs_embeds'], accessed_by=DictGetItemGuardAccessor(decoder_inputs_embeds)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_inputs_embeds'], 8820832)          # if input_ids is not None and inputs_embeds is not None:  # transformers/models/t5/modeling_t5.py:995 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=L['decoder_attention_mask'], accessed_by=DictGetItemGuardAccessor(decoder_attention_mask)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- ID_MATCH: ___check_obj_id(L['decoder_attention_mask'], 8820832)         # if attention_mask is None:  # transformers/models/t5/modeling_t5.py:1027 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['nn'], accessed_by=DictGetItemGuardAccessor(nn)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['nn'], 125150694903136)                     # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['nn'].functional, accessed_by=GetAttrGuardAccessor(functional)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional, 125150684383248)          # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['nn'].functional.dropout, accessed_by=GetAttrGuardAccessor(dropout)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional.dropout, 125150682577344)  # attn_weights = nn.functional.dropout(  # transformers/models/t5/modeling_t5.py:556 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['nn'].functional.softmax, accessed_by=GetAttrGuardAccessor(softmax)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['nn'].functional.softmax, 125150682596992)  # attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(  # transformers/models/t5/modeling_t5.py:553 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['math'], accessed_by=DictGetItemGuardAccessor(math)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['math'], 125150909974864)                   # / math.log(max_distance / max_exact)  # transformers/models/t5/modeling_t5.py:418 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['math'].log, accessed_by=GetAttrGuardAccessor(log)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['math'].log, 125150908715936)               # / math.log(max_distance / max_exact)  # transformers/models/t5/modeling_t5.py:418 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 125150850121008)                  # attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)  # transformers/models/t5/modeling_t5.py:1028 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].abs, accessed_by=GetAttrGuardAccessor(abs)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].abs, 125150839410016)              # relative_position = torch.abs(relative_position)  # transformers/models/t5/modeling_t5.py:406 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].log, accessed_by=GetAttrGuardAccessor(log)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].log, 125150839481792)              # torch.log(relative_position.float() / max_exact)  # transformers/models/t5/modeling_t5.py:417 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].min, accessed_by=GetAttrGuardAccessor(min)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].min, 125150839483232)              # relative_position_if_large = torch.min(  # transformers/models/t5/modeling_t5.py:421 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].long, accessed_by=GetAttrGuardAccessor(long)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['torch'].long == torch.int64                                # context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]  # transformers/models/t5/modeling_t5.py:432 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].ones, accessed_by=GetAttrGuardAccessor(ones)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].ones, 125150839392032)             # attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)  # transformers/models/t5/modeling_t5.py:1028 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].rsqrt, accessed_by=GetAttrGuardAccessor(rsqrt)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].rsqrt, 125150839422576)            # hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)  # transformers/models/t5/modeling_t5.py:246 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].where, accessed_by=GetAttrGuardAccessor(where)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].where, 125150839522224)            # relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)  # transformers/models/t5/modeling_t5.py:425 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].zeros, accessed_by=GetAttrGuardAccessor(zeros)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].zeros, 125150839396512)            # position_bias = torch.zeros(  # transformers/models/t5/modeling_t5.py:529 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].Tensor, accessed_by=GetAttrGuardAccessor(Tensor)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].Tensor, 101225952)                 # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].arange, accessed_by=GetAttrGuardAccessor(arange)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].arange, 125150839348320)           # context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]  # transformers/models/t5/modeling_t5.py:432 in compute_bias\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 125150839356640)           # scores = torch.matmul(  # transformers/models/t5/modeling_t5.py:523 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].float16, accessed_by=GetAttrGuardAccessor(float16)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['torch'].float16 == torch.float16                           # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].float32, accessed_by=GetAttrGuardAccessor(float32)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['torch'].float32 == torch.float32                           # variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)  # transformers/models/t5/modeling_t5.py:245 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].bfloat16, accessed_by=GetAttrGuardAccessor(bfloat16)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['torch'].bfloat16 == torch.bfloat16                         # if self.weight.dtype in [torch.float16, torch.bfloat16]:  # transformers/models/t5/modeling_t5.py:249 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].full_like, accessed_by=GetAttrGuardAccessor(full_like)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].full_like, 125150839479232)        # relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)  # transformers/models/t5/modeling_t5.py:422 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['torch'].zeros_like, accessed_by=GetAttrGuardAccessor(zeros_like)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].zeros_like, 125150839396752)       # relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))  # transformers/models/t5/modeling_t5.py:408 in _relative_position_bucket\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['Seq2SeqModelOutput'], accessed_by=DictGetItemGuardAccessor(Seq2SeqModelOutput)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['Seq2SeqModelOutput'], 151732880)           # return Seq2SeqModelOutput(  # transformers/models/t5/modeling_t5.py:1532 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['__builtins_dict___57'], accessed_by=DictGetItemGuardAccessor(__builtins_dict___57)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['len'], accessed_by=DictGetItemGuardAccessor(len)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['len'], 125150921102240)  # past_key_values = [None] * len(self.block)  # transformers/models/t5/modeling_t5.py:1025 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['str'], accessed_by=DictGetItemGuardAccessor(str)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['str'], 8799936)    # if isinstance(k, str):  # transformers/utils/generic.py:429 in __getitem__\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['zip'], accessed_by=DictGetItemGuardAccessor(zip)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['zip'], 8789216)    # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['dict'], accessed_by=DictGetItemGuardAccessor(dict)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['dict'], 8835648)   # inner_dict = dict(self.items())  # transformers/utils/generic.py:430 in __getitem__\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['tuple'], accessed_by=DictGetItemGuardAccessor(tuple)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['tuple'], 8810304)  # return tuple(self[k] for k in self.keys())  # transformers/utils/generic.py:458 in to_tuple\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['enumerate'], accessed_by=DictGetItemGuardAccessor(enumerate)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['enumerate'], 8885760)  # for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):  # transformers/models/t5/modeling_t5.py:1066 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__builtins_dict___57']['isinstance'], accessed_by=DictGetItemGuardAccessor(isinstance)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__builtins_dict___57']['isinstance'], 125150921101920)  # isinstance(self.wo.weight, torch.Tensor)  # transformers/models/t5/modeling_t5.py:284 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_modeling_utils)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'], 125148096836560)  # return get_parameter_dtype(self)  # transformers/modeling_utils.py:1051 in dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch, 125150850121008)  # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min  # transformers/modeling_utils.py:1154 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch.finfo, accessed_by=GetAttrGuardAccessor(finfo)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch.finfo, 125150511418784)  # extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min  # transformers/modeling_utils.py:1154 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].torch.arange, accessed_by=GetAttrGuardAccessor(arange)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].torch.arange, 125150839348320)  # seq_ids = torch.arange(seq_length, device=device)  # transformers/modeling_utils.py:1086 in create_extended_attention_mask_for_decoder\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].XLA_USE_BF16, accessed_by=GetAttrGuardAccessor(XLA_USE_BF16)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].XLA_USE_BF16 == '0'  # if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:259 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin, accessed_by=GetAttrGuardAccessor(ModuleUtilsMixin)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin, 156049440)  # extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(  # transformers/modeling_utils.py:1138 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder, accessed_by=GetAttrGuardAccessor(create_extended_attention_mask_for_decoder)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].ModuleUtilsMixin.create_extended_attention_mask_for_decoder.__code__, 151700000)  # extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(  # transformers/modeling_utils.py:1138 in get_extended_attention_mask\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].XLA_DOWNCAST_BF16, accessed_by=GetAttrGuardAccessor(XLA_DOWNCAST_BF16)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].XLA_DOWNCAST_BF16 == '0'  # if XLA_DOWNCAST_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:261 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].get_parameter_dtype, accessed_by=GetAttrGuardAccessor(get_parameter_dtype)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].get_parameter_dtype.__code__, accessed_by=GetAttrGuardAccessor(__code__)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_modeling_utils'].get_parameter_dtype.__code__, 151815504)  # return get_parameter_dtype(self)  # transformers/modeling_utils.py:1051 in dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_transformers_dot_modeling_utils'].ENV_VARS_TRUE_VALUES, accessed_by=GetAttrGuardAccessor(ENV_VARS_TRUE_VALUES)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- EQUALS_MATCH: G['__import_transformers_dot_modeling_utils'].ENV_VARS_TRUE_VALUES == {'YES', 'ON', '1', 'TRUE'}  # if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():  # transformers/modeling_utils.py:259 in get_parameter_dtype\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['BaseModelOutputWithPastAndCrossAttentions'], accessed_by=DictGetItemGuardAccessor(BaseModelOutputWithPastAndCrossAttentions)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['BaseModelOutputWithPastAndCrossAttentions'], 151709408)  # return BaseModelOutputWithPastAndCrossAttentions(  # transformers/models/t5/modeling_t5.py:1167 in forward\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['__import_transformers_dot_utils_dot_generic'], accessed_by=DictGetItemGuardAccessor(__import_transformers_dot_utils_dot_generic)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_transformers_dot_utils_dot_generic'], 125148405357984)  # if isinstance(k, str):  # transformers/utils/generic.py:429 in __getitem__\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 125150694908736)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch, 125150850121008)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, accessed_by=GetAttrGuardAccessor(_C)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, 125150839212848)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, accessed_by=GetAttrGuardAccessor(_get_tracing_state)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, 125150786030384)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 00:57:25.670000 125150921582400 torch/_dynamo/guards.py:2148] [4/1] [__guards] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         _compile.<locals>.compile_inner (dynamo_timed)        30.90%        4.296s        96.45%       13.410s       13.410s             1  \n",
      "         GraphLowering.compile_to_module (dynamo_timed)        15.29%        2.126s        30.20%        4.199s        4.199s             1  \n",
      "                       Scheduler.codegen (dynamo_timed)         9.20%        1.279s         9.21%        1.281s        1.281s             1  \n",
      "          create_aot_dispatcher_function (dynamo_timed)         9.10%        1.265s        63.90%        8.884s        8.884s             1  \n",
      "                       GraphLowering.run (dynamo_timed)         5.26%     731.084ms         5.33%     740.756ms     740.756ms             1  \n",
      "                      Scheduler.__init__ (dynamo_timed)         5.06%     703.923ms         5.07%     704.442ms     704.442ms             1  \n",
      "                                               aten::mm         3.49%     485.856ms         3.90%     542.179ms      95.387us          5684  \n",
      "                                             aten::view         3.28%     456.003ms         5.75%     799.780ms     181.028us          4418  \n",
      "                                                 detach         1.84%     256.059ms         2.79%     387.892ms     128.314us          3023  \n",
      "                                           aten::detach         1.63%     226.910ms         4.65%     646.165ms      26.145us         24715  \n",
      "                                          aten::permute         1.11%     154.864ms         1.86%     258.771ms     139.800us          1851  \n",
      "                                              aten::mul         0.86%     119.956ms         1.20%     166.436ms     178.197us           934  \n",
      "                                                aten::t         0.81%     112.126ms         2.74%     380.594ms     304.963us          1248  \n",
      "                        compile_fx_inner (dynamo_timed)         0.75%     103.789ms        38.67%        5.377s        5.377s             1  \n",
      "                                    aten::empty_strided         0.67%      93.454ms         0.69%      96.358ms       5.216us         18473  \n",
      "                                       CompiledFunction         0.65%      90.273ms         3.17%     441.034ms       8.821ms            50  \n",
      "                                        aten::transpose         0.64%      88.349ms         2.44%     338.959ms     269.015us          1260  \n",
      "          OutputGraph.call_user_compiler (dynamo_timed)         0.64%      88.340ms        64.54%        8.973s        8.973s             1  \n",
      "                 WrapperCodeGen.generate (dynamo_timed)         0.63%      87.300ms         0.63%      87.300ms      87.300ms             1  \n",
      "                                           aten::matmul         0.62%      86.761ms         3.79%     527.225ms       1.331ms           396  \n",
      "                                              aten::add         0.62%      86.605ms         0.84%     116.423ms     166.081us           701  \n",
      "                                            aten::clone         0.53%      73.417ms         0.73%     101.794ms     152.386us           668  \n",
      "                                           aten::expand         0.50%      69.635ms         0.87%     120.655ms     189.411us           637  \n",
      "                                             aten::set_         0.49%      67.984ms         0.54%      75.239ms       4.828us         15584  \n",
      "                                          aten::reshape         0.40%      55.263ms         2.77%     385.674ms     300.603us          1283  \n",
      "                                              aten::bmm         0.39%      54.221ms         0.55%      76.391ms      35.764us          2136  \n",
      "                                              aten::pow         0.27%      37.678ms         0.36%      50.475ms     171.683us           294  \n",
      "                                            aten::alias         0.27%      36.965ms         0.49%      68.020ms     125.499us           542  \n",
      "                                               aten::to         0.26%      35.637ms         0.29%      39.643ms      32.152us          1233  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.23%      31.814ms         1.72%     239.609ms     644.109us           372  \n",
      "                                            aten::rsqrt         0.19%      27.083ms         0.42%      58.250ms     257.742us           226  \n",
      "                                              aten::sum         0.18%      25.309ms         0.27%      37.223ms     165.435us           225  \n",
      "                                             aten::mean         0.17%      24.022ms         0.27%      37.005ms     163.741us           226  \n",
      "    compile_fx.<locals>.fw_compiler_base (dynamo_timed)         0.16%      22.488ms        38.83%        5.399s        5.399s             1  \n",
      "                               TorchDynamo Cache Lookup         0.15%      20.452ms         0.15%      20.452ms     409.036us            50  \n",
      "                                             aten::add_         0.13%      18.420ms         0.35%      48.754ms     497.493us            98  \n",
      "                                  Torch-Compiled Region         0.13%      18.360ms         3.31%     459.752ms       9.195ms            50  \n",
      "       autograd::engine::evaluate_function: MmBackward0         0.11%      15.335ms         1.67%     232.850ms       2.426ms            96  \n",
      "                                              aten::div         0.11%      14.832ms         0.15%      21.339ms     145.160us           147  \n",
      "                                           aten::linear         0.10%      13.641ms         2.78%     386.484ms       1.342ms           288  \n",
      "                                         aten::_softmax         0.10%      13.488ms         0.49%      68.121ms     756.896us            90  \n",
      "      autograd::engine::evaluate_function: MulBackward0         0.10%      13.461ms         0.75%     104.068ms       1.626ms            64  \n",
      "                                        model_inference         0.09%      12.675ms       100.00%       13.903s       13.903s             1  \n",
      "                                        aten::unsqueeze         0.08%      11.023ms         0.15%      21.251ms     147.578us           144  \n",
      "                                             aten::relu         0.08%      10.648ms         0.17%      23.713ms     275.736us            86  \n",
      "                                     aten::_unsafe_view         0.06%       8.913ms         0.15%      20.263ms      16.236us          1248  \n",
      "                                            aten::fill_         0.06%       8.489ms         0.06%       8.489ms      95.383us            89  \n",
      "                                            aten::slice         0.06%       8.418ms         0.11%      15.238ms     169.306us            90  \n",
      "                                        aten::sym_numel         0.06%       8.329ms         0.06%       8.329ms       2.941us          2832  \n",
      "      autograd::engine::evaluate_function: BmmBackward0         0.06%       7.787ms         0.54%      74.626ms       2.073ms            36  \n",
      "autograd::engine::evaluate_function: TransposeBackwa...         0.06%       7.692ms         0.37%      51.952ms     577.246us            90  \n",
      "        autograd::engine::evaluate_function: TBackward0         0.05%       7.602ms         0.36%      49.922ms     520.022us            96  \n",
      "                                aten::new_empty_strided         0.05%       6.472ms         0.07%       9.059ms     232.289us            39  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.05%       6.329ms         0.05%       6.398ms      99.969us            64  \n",
      "                               aten::sym_storage_offset         0.04%       6.235ms         0.04%       6.235ms       2.367us          2634  \n",
      "                                             aten::copy         0.04%       5.986ms         0.08%      11.479ms     147.164us            78  \n",
      "                                              aten::sub         0.04%       5.936ms         0.07%       9.352ms     114.054us            82  \n",
      "                                            aten::copy_         0.04%       5.817ms         0.16%      22.392ms     164.650us           136  \n",
      "                                               aten::eq         0.04%       5.747ms         0.06%       8.109ms      75.084us           108  \n",
      "                                         RsqrtBackward0         0.04%       5.680ms         0.39%      53.847ms       1.683ms            32  \n",
      "                                            aten::zeros         0.04%       5.568ms         0.07%      10.349ms      12.439us           832  \n",
      "    autograd::engine::evaluate_function: CloneBackward0         0.04%       5.519ms         0.04%       5.734ms      69.928us            82  \n",
      "                                            aten::where         0.04%       5.414ms         0.14%      19.871ms     292.218us            68  \n",
      "autograd::engine::evaluate_function: ExpandBackward0...         0.03%       4.667ms         0.10%      13.235ms     183.825us            72  \n",
      "                                             aten::amax         0.03%       4.214ms         0.05%       6.755ms     112.577us            60  \n",
      "                                        ExpandBackward0         0.03%       4.001ms         0.03%       4.001ms      55.568us            72  \n",
      "                           aten::_softmax_backward_data         0.03%       3.887ms         0.17%      23.520ms       1.307ms            18  \n",
      "                                            aten::empty         0.03%       3.786ms         0.03%       3.786ms       3.817us           992  \n",
      "                                        aten::embedding         0.03%       3.663ms         0.04%       5.847ms     292.349us            20  \n",
      "      autograd::engine::evaluate_function: PowBackward0         0.03%       3.565ms         0.34%      47.259ms       1.477ms            32  \n",
      "                                            MmBackward0         0.02%       3.427ms         1.56%     217.515ms       2.266ms            96  \n",
      "                                               aten::le         0.02%       3.372ms         0.04%       5.914ms     128.558us            46  \n",
      "                                              aten::exp         0.02%       3.256ms         0.04%       5.023ms      88.129us            57  \n",
      "    autograd::engine::evaluate_function: RsqrtBackward0         0.02%       3.172ms         0.41%      57.018ms       1.782ms            32  \n",
      "                                              aten::neg         0.02%       3.136ms         0.04%       5.906ms     131.255us            45  \n",
      "                                    aten::promote_types         0.02%       3.040ms         0.02%       3.040ms       2.946us          1032  \n",
      "                                             prims::fma         0.02%       2.994ms         0.06%       8.918ms     247.722us            36  \n",
      "                                          aten::squeeze         0.02%       2.941ms         0.04%       5.438ms     543.841us            10  \n",
      "                                       SoftmaxBackward0         0.02%       2.867ms         0.25%      35.341ms       1.963ms            18  \n",
      "     autograd::engine::evaluate_function: MeanBackward1         0.02%       2.749ms         0.21%      28.592ms     893.503us            32  \n",
      "                                              aten::all         0.02%       2.687ms         0.02%       3.230ms      34.366us            94  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.02%       2.601ms         0.39%      53.883ms       2.993ms            18  \n",
      "                                         aten::_to_copy         0.02%       2.482ms         0.03%       4.007ms     121.415us            33  \n",
      "                                    aten::scalar_tensor         0.02%       2.451ms         0.03%       4.244ms      84.877us            50  \n",
      "                                             aten::full         0.02%       2.437ms         0.09%      12.253ms     314.174us            39  \n",
      "                               aten::threshold_backward         0.02%       2.216ms         0.09%      11.999ms     999.955us            12  \n",
      "autograd::engine::evaluate_function: SoftmaxBackward...         0.01%       1.945ms         0.27%      37.286ms       2.071ms            18  \n",
      "                                           aten::arange         0.01%       1.926ms         0.02%       3.234ms     129.369us            25  \n",
      "                                          ReluBackward0         0.01%       1.828ms         0.14%      19.625ms       1.635ms            12  \n",
      "                                       aten::lift_fresh         0.01%       1.785ms         0.01%       2.052ms      78.916us            26  \n",
      "                                          aten::minimum         0.01%       1.606ms         0.01%       1.986ms     110.306us            18  \n",
      "                                          aten::dropout         0.01%       1.482ms         0.30%      41.702ms     217.199us           192  \n",
      "                                       aten::as_strided         0.01%       1.455ms         0.01%       1.455ms       2.970us           490  \n",
      "                                       aten::contiguous         0.01%       1.391ms         0.10%      14.162ms     177.020us            80  \n",
      "                                            aten::zero_         0.01%       1.385ms         0.01%       1.385ms       1.671us           829  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.01%       1.309ms         0.15%      20.934ms       1.745ms            12  \n",
      "                                           BmmBackward0         0.01%       1.215ms         0.48%      66.839ms       1.857ms            36  \n",
      "                            prims::convert_element_type         0.01%       1.155ms         0.02%       2.827ms     104.708us            27  \n",
      "                                             aten::rsub         0.01%       1.141ms         0.01%       1.826ms     202.929us             9  \n",
      "                                aten::_unsafe_index_put         0.01%       1.116ms         0.02%       2.414ms     201.155us            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.903s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main('compile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's profile this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2169] [5/0] [__guards] GUARDS:\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] \n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] +- RootGuardManager\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=True, size=[2, 2], stride=[2, 1])  # return x.sin().relu()  # mp/ipykernel_4051904/1820356168.py:15 in fn\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x.sin().relu()  # mp/ipykernel_4051904/1820356168.py:15 in fn\n",
      "V1024 02:34:32.005000 125150921582400 torch/_dynamo/guards.py:2148] [5/0] [__guards] \n",
      "V1024 02:34:37.575000 125150921582400 torch/_dynamo/guards.py:2169] [6/0] [__guards] GUARDS:\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] \n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] TREE_GUARD_MANAGER:\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] +- RootGuardManager\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:460 in init_ambient_guards\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float32, device=0, requires_grad=False, size=[5, 3, 224, 224], stride=[150528, 50176, 224, 1])  # x = self.conv1(x)  # torchvision/models/resnet.py:268 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = self.conv1(x)  # torchvision/models/resnet.py:268 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor(self)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- ID_MATCH: ___check_obj_id(L['self'], 125138400404880)                   # return self._forward_impl(x)  # torchvision/models/resnet.py:285 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=L['self'].training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(L['self'].training, 8906112)                  # return self._forward_impl(x)  # torchvision/models/resnet.py:285 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].fc, accessed_by=DictGetItemGuardAccessor(fc)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].fc, 125138703752208)                # x = self.fc(x)  # torchvision/models/resnet.py:280 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].fc.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].fc.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].fc.training, 8906112)               # x = self.fc(x)  # torchvision/models/resnet.py:280 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].bn1, 125138738942544)               # x = self.bn1(x)  # torchvision/models/resnet.py:269 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].bn1.training, 8906112)              # x = self.bn1(x)  # torchvision/models/resnet.py:269 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].relu, 125138400134032)              # x = self.relu(x)  # torchvision/models/resnet.py:270 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].relu.training, 8906112)             # x = self.relu(x)  # torchvision/models/resnet.py:270 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].conv1, 125138702501136)             # x = self.conv1(x)  # torchvision/models/resnet.py:268 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].conv1.training, 8906112)            # x = self.conv1(x)  # torchvision/models/resnet.py:268 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].layer1, accessed_by=DictGetItemGuardAccessor(layer1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer1, 125137942056144)            # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].layer1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer1.training, 8906112)           # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer1._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0'), 125138675160080)  # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer1, '0').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').training, 8906112)  # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').bn1, 125138846025680)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').bn2, 125137942053072)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').relu, 125137460398160)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').conv1, 125138401340176)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').conv2, 125138689731600)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '0').downsample, 8820832)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '0')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1'), 125138683518992)  # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer1, '1').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').training, 8906112)  # x = self.layer1(x)  # torchvision/models/resnet.py:273 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').bn1, 125137502193168)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').bn2, 125137502198928)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').relu, 125137502193744)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').conv1, 125137942060112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').conv2, 125137502192976)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer1, '1').downsample, 8820832)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer1, '1')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].layer2, accessed_by=DictGetItemGuardAccessor(layer2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer2, 125137502204496)            # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].layer2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer2.training, 8906112)           # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer2._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0'), 125137502191824)  # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer2, '0').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').training, 8906112)  # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').bn1, 125137502205648)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').bn2, 125137502199504)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').relu, 125137502205456)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').conv1, 125137502205264)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').conv2, 125137502194896)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').downsample, 125137502195472)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').downsample.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').downsample.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '0').downsample.training, 8906112)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0').downsample._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer2, '0').downsample, '0'), 125137942055696)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer2, '0').downsample, '0').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer2, '0').downsample, '1'), 125137942055312)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer2, '0').downsample, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer2, '0').downsample, '1').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '0')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1'), 125137502196816)  # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer2, '1').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').training, 8906112)  # x = self.layer2(x)  # torchvision/models/resnet.py:274 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').bn1, 125137502195280)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').bn2, 125137502197584)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').relu, 125137502202192)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').conv1, 125137502193552)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').conv2, 125137502194512)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer2, '1').downsample, 8820832)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer2, '1')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].layer3, accessed_by=DictGetItemGuardAccessor(layer3)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer3, 125137502204112)            # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].layer3.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer3.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer3.training, 8906112)           # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer3._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0'), 125137502199312)  # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer3, '0').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').training, 8906112)  # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').bn1, 125137502207632)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').bn2, 125137502206992)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').relu, 125137502207440)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').conv1, 125137502199120)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').conv2, 125137502206928)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').downsample, 125137502202768)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').downsample.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').downsample.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '0').downsample.training, 8906112)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0').downsample._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer3, '0').downsample, '0'), 125138598170000)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer3, '0').downsample, '0').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer3, '0').downsample, '1'), 125137502193936)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer3, '0').downsample, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer3, '0').downsample, '1').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '0')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1'), 125138332646608)  # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer3, '1').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').training, 8906112)  # x = self.layer3(x)  # torchvision/models/resnet.py:275 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').bn1, 125137502205776)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').bn2, 125137502205136)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').relu, 125138859950992)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').conv1, 125137502206544)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').conv2, 125137502205712)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer3, '1').downsample, 8820832)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer3, '1')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].layer4, accessed_by=DictGetItemGuardAccessor(layer4)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer4, 125137502201488)            # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].layer4.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer4.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].layer4.training, 8906112)           # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].layer4._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0'), 125137502203216)  # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer4, '0').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').training, 8906112)  # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').bn1, 125137502202448)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').bn2, 125137502201872)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').relu, 125137502202896)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').conv1, 125137502203280)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').conv2, 125137502202128)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').downsample, 125137502203600)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').downsample.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').downsample.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '0').downsample.training, 8906112)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0').downsample._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '0'), accessed_by=DictGetItemGuardAccessor(0)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer4, '0').downsample, '0'), 125137502204624)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '0').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '0').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer4, '0').downsample, '0').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer4, '0').downsample, '1'), 125137502204816)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | +- GuardManager: source=getattr(getattr(L['self'].layer4, '0').downsample, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(getattr(L['self'].layer4, '0').downsample, '1').training, 8906112)  # identity = self.downsample(x)  # torchvision/models/resnet.py:100 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '0')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1'), accessed_by=DictGetItemGuardAccessor(1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1'), 125137502203024)  # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', getattr(L['self'].layer4, '1').__dict__)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').training, 8906112)  # x = self.layer4(x)  # torchvision/models/resnet.py:276 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1')._modules, accessed_by=DictGetItemGuardAccessor(_modules)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn1, accessed_by=DictGetItemGuardAccessor(bn1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').bn1, 125137502200528)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').bn1.training, 8906112)  # out = self.bn1(out)  # torchvision/models/resnet.py:93 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn2, accessed_by=DictGetItemGuardAccessor(bn2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').bn2, 125137502199952)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').bn2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').bn2.training, 8906112)  # out = self.bn2(out)  # torchvision/models/resnet.py:97 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').relu, accessed_by=DictGetItemGuardAccessor(relu)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').relu, 125137502200976)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').relu.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').relu.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').relu.training, 8906112)  # out = self.relu(out)  # torchvision/models/resnet.py:94 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv1, accessed_by=DictGetItemGuardAccessor(conv1)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').conv1, 125137502201104)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv1.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv1.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').conv1.training, 8906112)  # out = self.conv1(x)  # torchvision/models/resnet.py:92 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv2, accessed_by=DictGetItemGuardAccessor(conv2)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').conv2, 125137502200208)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv2.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').conv2.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').conv2.training, 8906112)  # out = self.conv2(out)  # torchvision/models/resnet.py:96 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1').downsample, accessed_by=DictGetItemGuardAccessor(downsample)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | | +- ID_MATCH: ___check_obj_id(getattr(L['self'].layer4, '1').downsample, 8820832)  # if self.downsample is not None:  # torchvision/models/resnet.py:99 in forward\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1')._forward_hooks, accessed_by=DictGetItemGuardAccessor(_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1')._backward_hooks, accessed_by=DictGetItemGuardAccessor(_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1')._forward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | | | +- GuardManager: source=getattr(L['self'].layer4, '1')._backward_pre_hooks, accessed_by=DictGetItemGuardAccessor(_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].avgpool, accessed_by=DictGetItemGuardAccessor(avgpool)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].avgpool, 125137439056336)           # x = self.avgpool(x)  # torchvision/models/resnet.py:278 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].avgpool.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].avgpool.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].avgpool.training, 8906112)          # x = self.avgpool(x)  # torchvision/models/resnet.py:278 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=L['self'].maxpool, accessed_by=DictGetItemGuardAccessor(maxpool)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(L['self'].maxpool, 125138580192592)           # x = self.maxpool(x)  # torchvision/models/resnet.py:271 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=L['self'].maxpool.__dict__, accessed_by=GetGenericDictGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- GuardManager: source=L['self'].maxpool.training, accessed_by=DictGetItemGuardAccessor(training)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | | +- ID_MATCH: ___check_obj_id(L['self'].maxpool.training, 8906112)          # x = self.maxpool(x)  # torchvision/models/resnet.py:271 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor(torch)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 125150850121008)                  # x = torch.flatten(x, 1)  # torchvision/models/resnet.py:279 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['torch'].flatten, accessed_by=GetAttrGuardAccessor(flatten)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].flatten, 125150839415856)          # x = torch.flatten(x, 1)  # torchvision/models/resnet.py:279 in _forward_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor(__import_torch_dot_nn_dot_modules_dot_module)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 125150694908736)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch, accessed_by=GetAttrGuardAccessor(torch)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch, 125150850121008)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, accessed_by=GetAttrGuardAccessor(_C)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C, 125150839212848)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, accessed_by=GetAttrGuardAccessor(_get_tracing_state)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'].torch._C._get_tracing_state, 125150786030384)  # forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)  # nn/modules/module.py:1556 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, 8829024)  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # or _global_forward_hooks or _global_forward_pre_hooks):  # nn/modules/module.py:1561 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- TYPE_MATCH: ___check_type_id(G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, 8829024)  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # or _global_backward_pre_hooks or _global_backward_hooks  # nn/modules/module.py:1560 in _call_impl\n",
      "V1024 02:34:37.576000 125150921582400 torch/_dynamo/guards.py:2148] [6/0] [__guards] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18().cuda()\n",
    "inputs = [torch.randn((5, 3, 224, 224), device='cuda') for _ in range(10)]\n",
    "\n",
    "model_c = torch.compile(model)\n",
    "\n",
    "def fwd_bwd(inp):\n",
    "    out = model_c(inp)\n",
    "    out.sum().backward()\n",
    "\n",
    "def warmup_compile():\n",
    "    def fn(x):\n",
    "        return x.sin().relu()\n",
    "\n",
    "    x = torch.rand((2, 2), device='cuda', requires_grad=True)\n",
    "    fn_c = torch.compile(fn)\n",
    "    out = fn_c(x)\n",
    "    out.sum().backward()\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    with torch.profiler.record_function(\"warmup compile\"):\n",
    "        warmup_compile()\n",
    "\n",
    "    with torch.profiler.record_function(\"resnet18 compile\"):\n",
    "        fwd_bwd(inputs[0])\n",
    "\n",
    "prof.export_chrome_trace(\"trace_compile.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
